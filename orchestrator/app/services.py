"""
–°–µ—Ä–≤–∏—Å—ã –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Orchestrator

–≠—Ç–∞–ø 2: –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è Rule Engine + NLP Service
"""
import asyncio
import httpx
import logging
from typing import List, Dict, Any, Optional
import time

logger = logging.getLogger(__name__)

class ParallelAnalyzer:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    –ù–∞—Å—Ç–æ—è—â–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏—è:
    1. –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ‚Üí –±–ª–æ–∫–∏
    2. –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û: Rule Engine + NLP Service (–æ—Ç–¥–µ–ª—å–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã)
    3. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    
    def __init__(
        self,
        unified_service_url: str,
        nlp_service_url: str,
        rule_engine_url: str
    ):
        self.unified_service_url = unified_service_url
        self.nlp_service_url = nlp_service_url
        self.rule_engine_url = rule_engine_url
    
    async def analyze_document_parallel(
        self,
        file_content: bytes,
        filename: str,
        patterns_file: str = "patterns/sensitive_patterns.xlsx"
    ) -> Dict[str, Any]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        –®–∞–≥–∏:
        1. –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ ‚Üí –±–ª–æ–∫–∏ (Unified Service /parse_document)
        2. –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–û: 
           - Rule Engine –∞–Ω–∞–ª–∏–∑ (Unified Service /analyze_rule_engine)
           - NLP –∞–Ω–∞–ª–∏–∑ (Unified Service /analyze_nlp)
        3. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            file_content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ DOCX —Ñ–∞–π–ª–∞
            filename: –ò–º—è —Ñ–∞–π–ª–∞
            patterns_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        
        Returns:
            Dict —Å found_items –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        start_time = time.time()
        
        # –≠–¢–ê–ü 1: –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ)
        logger.info("[PARALLEL] –≠—Ç–∞–ø 1: –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞...")
        parse_start = time.time()
        
        blocks = await self._parse_document(file_content, filename)
        
        parse_time = time.time() - parse_start
        logger.info(f"[PARALLEL] –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω: {len(blocks)} –±–ª–æ–∫–æ–≤ –∑–∞ {parse_time:.2f}s")
        
        if not blocks:
            logger.warning("‚ö†Ô∏è [PARALLEL] –ù–µ—Ç –±–ª–æ–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return {
                "success": True,
                "found_items": [],
                "total_items": 0,
                "performance_metrics": {
                    "total_time_seconds": round(time.time() - start_time, 2),
                    "parse_time_seconds": round(parse_time, 2),
                    "parallel_processing_time_seconds": 0,
                    "blocks_processed": 0,
                    "rule_engine_items": 0,
                    "nlp_items": 0
                }
            }
        
        # –≠–¢–ê–ü 2: –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ Rule Engine + NLP
        logger.info(f"[PARALLEL] –≠—Ç–∞–ø 2: –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(blocks)} –±–ª–æ–∫–æ–≤...")
        parallel_start = time.time()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –û–ë–ê —Å–µ—Ä–≤–∏—Å–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ –Ω–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
        rule_task = self._process_rule_engine(blocks, patterns_file)
        nlp_task = self._process_nlp_service(blocks)
        
        # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –û–ë–û–ò–• –∑–∞–¥–∞—á
        results = await asyncio.gather(
            rule_task,
            nlp_task,
            return_exceptions=True  # –ù–µ –ø–∞–¥–∞–µ–º –µ—Å–ª–∏ –æ–¥–Ω–∞ –∑–∞–¥–∞—á–∞ —É–ø–∞–ª–∞
        )
        
        parallel_time = time.time() - parallel_start
        logger.info(f"[PARALLEL] –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {parallel_time:.2f}s")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        rule_results = results[0] if not isinstance(results[0], Exception) else []
        nlp_results = results[1] if not isinstance(results[1], Exception) else []
        
        if isinstance(results[0], Exception):
            logger.error(f"‚ùå [PARALLEL] Rule Engine error: {results[0]}")
        if isinstance(results[1], Exception):
            logger.error(f"‚ùå [PARALLEL] NLP Service error: {results[1]}")
        
        # –≠–¢–ê–ü 3: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        all_items = rule_results + nlp_results
        
        total_time = time.time() - start_time
        
        logger.info(f"[PARALLEL] –ò—Ç–æ–≥–æ: Rule={len(rule_results)}, NLP={len(nlp_results)}, Total={len(all_items)}")
        logger.info(f"[PARALLEL] –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f}s (–ø–∞—Ä—Å–∏–Ω–≥: {parse_time:.2f}s, –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {parallel_time:.2f}s)")
        
        return {
            "success": True,
            "found_items": all_items,
            "total_items": len(all_items),
            "performance_metrics": {
                "total_time_seconds": round(total_time, 2),
                "parse_time_seconds": round(parse_time, 2),
                "parallel_processing_time_seconds": round(parallel_time, 2),
                "blocks_processed": len(blocks),
                "rule_engine_items": len(rule_results),
                "nlp_items": len(nlp_results),
                "speedup_estimate": f"{round(parse_time / parallel_time, 1)}x faster than sequential" if parallel_time > 0 else "N/A"
            },
            "orchestrator_metadata": {
                "version": "2.0.0",
                "mode": "parallel_processing",
                "etap": 2
            }
        }
    
    async def _parse_document(
        self,
        file_content: bytes,
        filename: str
    ) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Unified Service /parse_document
        
        Returns:
            List of blocks (–±–µ–∑ element)
        """
        try:
            files = {
                'file': (filename, file_content, 'application/vnd.openxmlformats-officedocument.wordprocessingml.document')
            }
            
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.unified_service_url}/parse_document",
                    files=files
                )
            
            if response.status_code == 200:
                result = response.json()
                blocks = result.get('blocks', [])
                logger.info(f"[PARSE] –ü–æ–ª—É—á–µ–Ω–æ {len(blocks)} –±–ª–æ–∫–æ–≤ –æ—Ç Unified Service")
                return blocks
            else:
                logger.error(f"‚ùå [PARSE] Unified Service /parse_document error: {response.status_code} - {response.text}")
                return []
        
        except Exception as e:
            logger.error(f"üí• [PARSE] Exception: {e}")
            return []
    
    async def _process_rule_engine(
        self,
        blocks: List[Dict[str, Any]],
        patterns_file: str
    ) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Rule Engine —á–µ—Ä–µ–∑ Unified Service /analyze_rule_engine
        
        Args:
            blocks: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            patterns_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        
        Returns:
            List of found_items from Rule Engine
        """
        try:
            logger.info(f"[RULE] –û—Ç–ø—Ä–∞–≤–∫–∞ {len(blocks)} –±–ª–æ–∫–æ–≤ –≤ Rule Engine...")
            
            payload = {
                "blocks": blocks,
                "patterns_file": patterns_file
            }
            
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    f"{self.unified_service_url}/analyze_rule_engine",
                    json=payload
                )
            
            if response.status_code == 200:
                result = response.json()
                items = result.get('found_items', [])
                logger.info(f"[RULE] Rule Engine –Ω–∞—à—ë–ª {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                return items
            else:
                logger.error(f"‚ùå [RULE] Unified Service /analyze_rule_engine error: {response.status_code} - {response.text}")
                return []
        
        except Exception as e:
            logger.error(f"üí• [RULE] Exception: {e}")
            return []
    
    async def _process_nlp_service(
        self,
        blocks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ NLP Service —á–µ—Ä–µ–∑ Unified Service /analyze_nlp
        
        Args:
            blocks: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Returns:
            List of found_items from NLP Service
        """
        try:
            logger.info(f"[NLP] –û—Ç–ø—Ä–∞–≤–∫–∞ {len(blocks)} –±–ª–æ–∫–æ–≤ –≤ NLP Service...")
            
            payload = {
                "blocks": blocks
            }
            
            async with httpx.AsyncClient(timeout=240.0) as client:  # –ë–æ–ª—å—à–µ timeout –¥–ª—è NLP
                response = await client.post(
                    f"{self.unified_service_url}/analyze_nlp",
                    json=payload
                )
            
            if response.status_code == 200:
                result = response.json()
                items = result.get('found_items', [])
                logger.info(f"[NLP] NLP Service –Ω–∞—à—ë–ª {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
                return items
            else:
                logger.error(f"‚ùå [NLP] Unified Service /analyze_nlp error: {response.status_code} - {response.text}")
                return []
        
        except Exception as e:
            logger.error(f"üí• [NLP] Exception: {e}")
            return []

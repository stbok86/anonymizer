#!/usr/bin/env python3
"""
–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É spaCy NER –∏ Phrase Matcher
"""

import spacy
from spacy.matcher import PhraseMatcher
from typing import List, Dict, Any
import time

class NERVsPhraseMatcherDemo:
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É spaCy NER –∏ Phrase Matcher"""
    
    def __init__(self):
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å...")
        self.nlp = spacy.load("ru_core_news_lg")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Phrase Matcher
        self.phrase_matcher = PhraseMatcher(self.nlp.vocab, attr="LOWER")
        self._setup_phrase_matcher()
    
    def _setup_phrase_matcher(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Phrase Matcher —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –≥–æ—Å–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏"""
        
        # –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        gov_orgs = [
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª",
            "–ú–í–î –†–æ—Å—Å–∏–∏",
            "–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", 
            "–§–ù–° –†–æ—Å—Å–∏–∏",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –Ω–∞–ª–æ–≥–æ–≤–∞—è —Å–ª—É–∂–±–∞",
            "–ú–∏–Ω–∑–¥—Ä–∞–≤ –†–æ—Å—Å–∏–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª"
        ]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ spaCy –¥–æ–∫—É–º–µ–Ω—Ç—ã
        patterns = [self.nlp(org) for org in gov_orgs]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ matcher
        self.phrase_matcher.add("GOVERNMENT_ORG", patterns)
        
        print(f"‚úÖ Phrase Matcher –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å {len(gov_orgs)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏")
    
    def demonstrate_ner_approach(self, text: str) -> Dict[str, Any]:
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É spaCy NER"""
        
        print(f"\nü§ñ SPACY NER –ü–û–î–•–û–î")
        print(f"{'='*50}")
        print(f"üìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: '{text}'")
        
        start_time = time.time()
        doc = self.nlp(text)
        processing_time = time.time() - start_time
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        all_entities = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        organizations = [ent for ent in doc.ents if ent.label_ == "ORG"]
        
        print(f"\nüîç –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ spaCy:")
        for text_span, label, start, end in all_entities:
            confidence = "—Å—Ä–µ–¥–Ω—è—è" if label == "ORG" else "–≤—ã—Å–æ–∫–∞—è"
            print(f"   ‚Ä¢ '{text_span}' ‚Üí {label} (–ø–æ–∑–∏—Ü–∏—è: {start}-{end}, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence})")
        
        print(f"\nüèõÔ∏è –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ (ORG):")
        org_results = []
        for org in organizations:
            # spaCy –Ω–µ –¥–∞–µ—Ç –ø—Ä—è–º–æ–≥–æ confidence, –æ—Ü–µ–Ω–∏–≤–∞–µ–º –∫–æ—Å–≤–µ–Ω–Ω–æ
            confidence = self._estimate_ner_confidence(org)
            result = {
                'text': org.text,
                'start': org.start_char,
                'end': org.end_char,
                'confidence': confidence,
                'method': 'spacy_ner'
            }
            org_results.append(result)
            print(f"   ‚úÖ '{org.text}' (confidence: {confidence:.2f})")
        
        print(f"\n‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time*1000:.1f} –º—Å")
        print(f"üìä –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:")
        print(f"   ‚Ä¢ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, –∫–æ–Ω—Ç–µ–∫—Å—Ç")
        print(f"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å")
        print(f"   ‚Ä¢ –ú–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –ù–ï–ò–ó–í–ï–°–¢–ù–´–ï –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏")
        print(f"   ‚Ä¢ –ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç (\"—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ Apple\" vs \"–∫—É–ø–∏–ª apple\")")
        
        return {
            'results': org_results,
            'processing_time': processing_time,
            'total_entities': len(all_entities),
            'organizations_found': len(organizations)
        }
    
    def demonstrate_phrase_matcher(self, text: str) -> Dict[str, Any]:
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É Phrase Matcher"""
        
        print(f"\nüìö PHRASE MATCHER –ü–û–î–•–û–î")
        print(f"{'='*50}")
        print(f"üìù –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º: '{text}'")
        
        start_time = time.time()
        doc = self.nlp(text)
        matches = self.phrase_matcher(doc)
        processing_time = time.time() - start_time
        
        print(f"\nüîç –ü—Ä–æ—Ü–µ—Å—Å –ø–æ–∏—Å–∫–∞:")
        print(f"   1. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: {[token.text for token in doc]}")
        print(f"   2. –ü–æ–∏—Å–∫ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏")
        print(f"   3. –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}")
        
        phrase_results = []
        for match_id, start, end in matches:
            span = doc[start:end]
            result = {
                'text': span.text,
                'start': span.start_char,
                'end': span.end_char,
                'confidence': 0.95,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                'method': 'phrase_matcher'
            }
            phrase_results.append(result)
            
            print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ: '{span.text}' (—Ç–æ–∫–µ–Ω—ã {start}-{end})")
            print(f"      –ü–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç–µ: {span.start_char}-{span.end_char}")
            print(f"      –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: 0.95 (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)")
        
        print(f"\n‚ö° –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time*1000:.1f} –º—Å")
        print(f"üìä –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:")
        print(f"   ‚Ä¢ –¢–æ—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –∑–∞—Ä–∞–Ω–µ–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏")
        print(f"   ‚Ä¢ –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ (–∞–ª–≥–æ—Ä–∏—Ç–º –∞–≤—Ç–æ–º–∞—Ç–∞)")
        print(f"   ‚Ä¢ –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
        print(f"   ‚Ä¢ –ù–ï –Ω–∞–π–¥–µ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏")
        
        return {
            'results': phrase_results,
            'processing_time': processing_time,
            'matches_found': len(matches)
        }
    
    def _estimate_ner_confidence(self, ent) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç confidence –¥–ª—è NER —Å—É—â–Ω–æ—Å—Ç–∏"""
        # spaCy –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä—è–º–æ–π confidence, –¥–µ–ª–∞–µ–º –æ—Ü–µ–Ω–∫—É
        base_confidence = 0.75
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É (–¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –æ–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ)
        length_bonus = min(0.15, len(ent.text.split()) * 0.05)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã (–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–æ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã)
        if ent.text[0].isupper():
            caps_bonus = 0.05
        else:
            caps_bonus = 0
        
        return min(0.90, base_confidence + length_bonus + caps_bonus)
    
    def compare_approaches(self, test_texts: List[str]):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –æ–±–∞ –ø–æ–¥—Ö–æ–¥–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö"""
        
        print(f"\nüî¨ –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó")
        print(f"{'='*80}")
        
        total_ner_time = 0
        total_phrase_time = 0
        
        comparison_table = []
        
        for i, text in enumerate(test_texts, 1):
            print(f"\nüìÑ –¢–ï–°–¢ {i}: {text}")
            print(f"{'-'*60}")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º NER
            ner_results = self.demonstrate_ner_approach(text)
            total_ner_time += ner_results['processing_time']
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º Phrase Matcher
            phrase_results = self.demonstrate_phrase_matcher(text)
            total_phrase_time += phrase_results['processing_time']
            
            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            print(f"\nüÜö –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
            
            ner_orgs = set(r['text'] for r in ner_results['results'])
            phrase_orgs = set(r['text'] for r in phrase_results['results'])
            
            only_ner = ner_orgs - phrase_orgs
            only_phrase = phrase_orgs - ner_orgs
            both = ner_orgs & phrase_orgs
            
            print(f"   ü§ñ –¢–æ–ª—å–∫–æ NER –Ω–∞—à–µ–ª: {only_ner if only_ner else '–Ω–∏—á–µ–≥–æ'}")
            print(f"   üìö –¢–æ–ª—å–∫–æ Phrase Matcher: {only_phrase if only_phrase else '–Ω–∏—á–µ–≥–æ'}")
            print(f"   üéØ –ù–∞—à–ª–∏ –æ–±–∞: {both if both else '–Ω–∏—á–µ–≥–æ'}")
            
            comparison_table.append({
                'text': text,
                'ner_count': len(ner_results['results']),
                'phrase_count': len(phrase_results['results']),
                'ner_time': ner_results['processing_time'] * 1000,
                'phrase_time': phrase_results['processing_time'] * 1000,
                'overlap': len(both)
            })
        
        # –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        print(f"\nüìä –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï")
        print(f"{'='*80}")
        
        print(f"‚è±Ô∏è –°–∫–æ—Ä–æ—Å—Ç—å:")
        print(f"   spaCy NER: {total_ner_time*1000:.1f} –º—Å –æ–±—â–µ–µ –≤—Ä–µ–º—è")
        print(f"   Phrase Matcher: {total_phrase_time*1000:.1f} –º—Å –æ–±—â–µ–µ –≤—Ä–µ–º—è")
        speedup = total_ner_time / total_phrase_time if total_phrase_time > 0 else 0
        print(f"   Phrase Matcher –±—ã—Å—Ç—Ä–µ–µ –≤ {speedup:.1f}x —Ä–∞–∑")
        
        total_ner_found = sum(row['ner_count'] for row in comparison_table)
        total_phrase_found = sum(row['phrase_count'] for row in comparison_table)
        
        print(f"\nüéØ –ü–æ–∫—Ä—ã—Ç–∏–µ:")
        print(f"   spaCy NER: {total_ner_found} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–∞–π–¥–µ–Ω–æ")
        print(f"   Phrase Matcher: {total_phrase_found} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–∞–π–¥–µ–Ω–æ")
        
        print(f"\nüìã –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:")
        print(f"{'–¢–µ—Å—Ç':<5} {'NER':<4} {'Phrase':<7} {'–í—Ä–µ–º—è NER':<10} {'–í—Ä–µ–º—è Phrase':<13} {'–ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ':<12}")
        print(f"{'-'*60}")
        for i, row in enumerate(comparison_table, 1):
            print(f"{i:<5} {row['ner_count']:<4} {row['phrase_count']:<7} "
                  f"{row['ner_time']:<10.1f} {row['phrase_time']:<13.1f} {row['overlap']:<12}")

def run_comprehensive_demo():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é"""
    
    print(f"üî¨ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –†–ê–ó–õ–ò–ß–ò–ô: spaCy NER vs Phrase Matcher")
    print(f"{'='*80}")
    
    demo = NERVsPhraseMatcherDemo()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏
    test_texts = [
        # 1. –ò–∑–≤–µ—Å—Ç–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä–µ
        "–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª —Å–∞–π—Ç –∫–æ–º–ø–∞–Ω–∏–∏.",
        
        # 2. –°–æ–∫—Ä–∞—â–µ–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä–µ
        "–§–ù–° –†–æ—Å—Å–∏–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞ —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ –Ω–∞–ª–æ–≥–∞–º.",
        
        # 3. –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è (–ù–ï–¢ –≤ —Å–ª–æ–≤–∞—Ä–µ Phrase Matcher)
        "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –ø—Ä–æ–≤–µ–ª–∞ –ø—Ä–æ–≤–µ—Ä–∫—É.",
        
        # 4. –°–∫–ª–æ–Ω–µ–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        "–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–∞ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ–æ–±—â–∏–ª–∏ –Ω–æ–≤–æ—Å—Ç–∏.",
        
        # 5. –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)
        "–ö–æ–º–ø–∞–Ω–∏—è Google –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –Ω–æ–≤—ã–π –ø—Ä–æ–¥—É–∫—Ç.",
        
        # 6. –°–ª–æ–∂–Ω—ã–π —Å–ª—É—á–∞–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        "–ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–∏ –ú–í–î –†–æ—Å—Å–∏–∏ –∏ Apple –æ–±—Å—É–¥–∏–ª–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–æ."
    ]
    
    demo.compare_approaches(test_texts)
    
    # –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print(f"\nüéØ –ö–õ–Æ–ß–ï–í–´–ï –í–´–í–û–î–´:")
    print(f"{'='*50}")
    
    print(f"\nüìö PHRASE MATCHER –ª—É—á—à–µ –¥–ª—è:")
    print(f"   ‚úÖ –¢–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
    print(f"   ‚úÖ –í—ã—Å–æ–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    print(f"   ‚úÖ –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π")
    print(f"   ‚úÖ –ö–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ —Ç–µ–º, —á—Ç–æ –∏—Å–∫–∞—Ç—å")
    
    print(f"\nü§ñ SPACY NER –ª—É—á—à–µ –¥–ª—è:")
    print(f"   ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ù–ï–ò–ó–í–ï–°–¢–ù–´–• –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
    print(f"   ‚úÖ –ü–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    print(f"   ‚úÖ –†–∞–±–æ—Ç—ã —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –Ω–∞–∑–≤–∞–Ω–∏–π")
    print(f"   ‚úÖ –û–±–æ–±—â–µ–Ω–∏—è –Ω–∞ –Ω–æ–≤—ã–µ —Å–ª—É—á–∞–∏")
    
    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø –î–õ–Ø –ì–û–°–û–†–ì–ê–ù–û–í:")
    print(f"   üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –û–ë–ê –ü–û–î–•–û–î–ê —Å–æ–≤–º–µ—Å—Ç–Ω–æ:")
    print(f"   1. Phrase Matcher –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π (–≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)")
    print(f"   2. spaCy NER –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–≤—ã—Å–æ–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ)")
    print(f"   3. –û–±—ä–µ–¥–∏–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")

if __name__ == "__main__":
    run_comprehensive_demo()
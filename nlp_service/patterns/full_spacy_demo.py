#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è spaCy NER vs Phrase Matcher –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
"""

import spacy
from spacy.matcher import PhraseMatcher
from docx import Document
import os
import time
from typing import List, Dict, Any, Tuple

class FullSpacyDemo:
    """–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ spaCy NER –∏ Phrase Matcher"""
    
    def __init__(self):
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å...")
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
        self.nlp = spacy.load("ru_core_news_sm")
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: ru_core_news_sm")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Phrase Matcher
        self.phrase_matcher = PhraseMatcher(self.nlp.vocab, attr="LOWER")
        self._setup_phrase_matcher()
        
        self.doc_path = r"C:\Projects\Anonymizer\unified_document_service\test_docs\test_01_1_4_SD.docx"
    
    def _setup_phrase_matcher(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Phrase Matcher —Å –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏"""
        
        government_orgs = [
            # –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã - –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –Ω–∞–ª–æ–≥–æ–≤–∞—è —Å–ª—É–∂–±–∞",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —á—Ä–µ–∑–≤—ã—á–∞–π–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            
            # –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–æ–≤
            "–ú–í–î –†–æ—Å—Å–∏–∏", "–ú–í–î –†–§",
            "–§–ù–° –†–æ—Å—Å–∏–∏", "–§–ù–° –†–§",
            "–ú–∏–Ω–∑–¥—Ä–∞–≤ –†–æ—Å—Å–∏–∏", "–ú–∏–Ω–∑–¥—Ä–∞–≤ –†–§",
            "–ú–∏–Ω–æ–±—Ä–Ω–∞—É–∫–∏ –†–æ—Å—Å–∏–∏", "–ú–∏–Ω–æ–±—Ä–Ω–∞—É–∫–∏ –†–§",
            "–§–°–ë –†–æ—Å—Å–∏–∏", "–§–°–ë –†–§",
            "–ú–ß–° –†–æ—Å—Å–∏–∏", "–ú–ß–° –†–§",
            "–§–ê–° –†–æ—Å—Å–∏–∏", "–§–ê–° –†–§",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§",
            
            # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ —Å–ª—É–∂–±—ã –∏ –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞
            "–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä",
            "–†–æ—Å—Ä–µ–µ—Å—Ç—Ä",
            "–†–æ—Å—Ç—É—Ä–∏–∑–º",
            "–†–æ—Å—Å—Ç–∞—Ç",
            "–†–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä",
            "–†–æ—Å—Ç–µ—Ö–Ω–∞–¥–∑–æ—Ä",
            "–†–æ—Å–ø—Ä–∏—Ä–æ–¥–Ω–∞–¥–∑–æ—Ä",
            "–†–æ—Å—Å–µ–ª—å—Ö–æ–∑–Ω–∞–¥–∑–æ—Ä",
            
            # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–∞ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –ö–∏—Ä–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –ø–æ –°–≤–µ—Ä–¥–ª–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –ú–æ—Å–∫–≤—ã",
            "–ö–æ–º–∏—Ç–µ—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞",
            
            # –ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –≥–æ—Ä–æ–¥–∞ –ü–µ—Ä–º–∏",
            "–ì–æ—Ä–æ–¥—Å–∫–∞—è –¥—É–º–∞ –≥–æ—Ä–æ–¥–∞ –ü–µ—Ä–º–∏",
            "–ú—ç—Ä–∏—è –≥–æ—Ä–æ–¥–∞ –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–∞",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –õ–µ–Ω–∏–Ω—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –°–≤–µ—Ä–¥–ª–æ–≤—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞",
            
            # –°—É–¥–µ–±–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–í–µ—Ä—Ö–æ–≤–Ω—ã–π —Å—É–¥ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–π —Å—É–¥ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ê—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π —Å—É–¥ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–ü–µ—Ä–º—Å–∫–∏–π —Ä–∞–π–æ–Ω–Ω—ã–π —Å—É–¥",
            "–õ–µ–Ω–∏–Ω—Å–∫–∏–π —Ä–∞–π–æ–Ω–Ω—ã–π —Å—É–¥",
            
            # –°–∏–ª–æ–≤—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            "–ü—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–º–∏—Ç–µ—Ç –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–π —Å–ª—É–∂–±—ã –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞–∫–∞–∑–∞–Ω–∏–π",
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –ø–æ –Ω–∞–¥–∑–æ—Ä—É –≤ —Å—Ñ–µ—Ä–µ —Å–≤—è–∑–∏",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ –ø–æ —Ç—É—Ä–∏–∑–º—É",
        ]
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns = [self.nlp(org) for org in government_orgs]
        self.phrase_matcher.add("GOVERNMENT_ORG", patterns)
        
        print(f"‚úÖ Phrase Matcher –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å {len(government_orgs)} –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
        self.known_orgs = government_orgs
    
    def extract_document_text(self) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        if not os.path.exists(self.doc_path):
            raise FileNotFoundError(f"–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.doc_path}")
        
        print(f"\nüìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç: {os.path.basename(self.doc_path)}")
        
        try:
            doc = Document(self.doc_path)
            full_text = []
            paragraph_texts = []
            table_texts = []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    paragraph_texts.append(paragraph.text.strip())
                    full_text.append(paragraph.text.strip())
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü
            for table in doc.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        table_text = " | ".join(row_text)
                        table_texts.append(table_text)
                        full_text.append(table_text)
            
            document_text = "\n".join(full_text)
            
            print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω:")
            print(f"   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            print(f"      ‚Ä¢ –û–±—â–∞—è –¥–ª–∏–Ω–∞: {len(document_text):,} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"      ‚Ä¢ –ü–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º: {len(paragraph_texts)}")
            print(f"      ‚Ä¢ –°—Ç—Ä–æ–∫ –∏–∑ —Ç–∞–±–ª–∏—Ü: {len(table_texts)}")
            print(f"      ‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {len(document_text.split()):,}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
            print(f"\nüìñ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
            if len(paragraph_texts) > 0:
                print(f"   –ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ: {paragraph_texts[0][:100]}...")
            if len(table_texts) > 0:
                print(f"   –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Ç–∞–±–ª–∏—Ü—ã: {table_texts[0][:100]}...")
            
            return document_text
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
    
    def analyze_with_spacy_ner(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ spaCy NER"""
        
        print(f"\nü§ñ –†–ï–ê–õ–¨–ù–´–ô SPACY NER –ê–ù–ê–õ–ò–ó")
        print(f"{'='*60}")
        
        start_time = time.time()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ spaCy
        doc = self.nlp(text)
        
        processing_time = time.time() - start_time
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏
        all_entities = list(doc.ents)
        organizations = [ent for ent in doc.ents if ent.label_ == "ORG"]
        persons = [ent for ent in doc.ents if ent.label_ in ["PER", "PERSON"]]
        locations = [ent for ent in doc.ents if ent.label_ in ["LOC", "GPE"]]
        other_entities = [ent for ent in doc.ents if ent.label_ not in ["ORG", "PER", "PERSON", "LOC", "GPE"]]
        
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ spaCy: {processing_time*1000:.1f} –º—Å")
        print(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ NER:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(doc):,}")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π: {len(all_entities)}")
        print(f"   ‚Ä¢ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ (ORG): {len(organizations)}")
        print(f"   ‚Ä¢ –ü–µ—Ä—Å–æ–Ω—ã (PER): {len(persons)}")
        print(f"   ‚Ä¢ –õ–æ–∫–∞—Ü–∏–∏ (LOC/GPE): {len(locations)}")
        print(f"   ‚Ä¢ –î—Ä—É–≥–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏: {len(other_entities)}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–∞–ª—å–Ω–æ
        print(f"\nüè¢ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô:")
        
        org_results = []
        government_orgs = []
        commercial_orgs = []
        
        for i, org in enumerate(organizations, 1):
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–∏–ø –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
            is_government = self._classify_organization_type(org.text)
            confidence = self._calculate_ner_confidence(org, text)
            
            org_info = {
                'text': org.text,
                'start': org.start_char,
                'end': org.end_char,
                'confidence': confidence,
                'is_government': is_government,
                'method': 'spacy_ner',
                'entity_label': org.label_
            }
            
            org_results.append(org_info)
            
            if is_government:
                government_orgs.append(org_info)
            else:
                commercial_orgs.append(org_info)
            
            # –í—ã–≤–æ–¥–∏–º —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏ —Ç–∏–ø–∞
            org_type_marker = "üèõÔ∏è" if is_government else "üè¢"
            
            print(f"   {i:2d}. {org_type_marker} '{org.text}'")
            print(f"       üìç –ü–æ–∑–∏—Ü–∏—è: {org.start_char}-{org.end_char}")
            print(f"       üéØ Confidence: {confidence:.3f}")
            print(f"       üè∑Ô∏è spaCy –º–µ—Ç–∫–∞: {org.label_}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_start = max(0, org.start_char - 50)
            context_end = min(len(text), org.end_char + 50)
            context = text[context_start:context_end].replace('\n', ' ')
            print(f"       üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{context}...")
            print()
        
        print(f"üèõÔ∏è –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: {len(government_orgs)}")
        print(f"üè¢ –ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: {len(commercial_orgs)}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥—Ä—É–≥–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏
        if persons:
            print(f"\nüë§ –ù–ê–ô–î–ï–ù–ù–´–ï –ü–ï–†–°–û–ù–´ (–ø–µ—Ä–≤—ã–µ 5):")
            for i, person in enumerate(persons[:5], 1):
                print(f"   {i}. '{person.text}' (–ø–æ–∑–∏—Ü–∏—è: {person.start_char}-{person.end_char})")
        
        if locations:
            print(f"\nüìç –ù–ê–ô–î–ï–ù–ù–´–ï –õ–û–ö–ê–¶–ò–ò (–ø–µ—Ä–≤—ã–µ 5):")
            for i, loc in enumerate(locations[:5], 1):
                print(f"   {i}. '{loc.text}' (–ø–æ–∑–∏—Ü–∏—è: {loc.start_char}-{loc.end_char})")
        
        return {
            'results': org_results,
            'processing_time': processing_time,
            'total_entities': len(all_entities),
            'organizations': len(organizations),
            'government_orgs': len(government_orgs),
            'commercial_orgs': len(commercial_orgs),
            'persons': len(persons),
            'locations': len(locations),
            'doc_tokens': len(doc)
        }
    
    def analyze_with_phrase_matcher(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ Phrase Matcher"""
        
        print(f"\nüìö –†–ï–ê–õ–¨–ù–´–ô PHRASE MATCHER –ê–ù–ê–õ–ò–ó")
        print(f"{'='*60}")
        
        start_time = time.time()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ spaCy (–Ω—É–∂–Ω–æ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏)
        doc = self.nlp(text)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º Phrase Matcher
        matches = self.phrase_matcher(doc)
        
        processing_time = time.time() - start_time
        
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ Phrase Matcher: {processing_time*1000:.1f} –º—Å")
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞:")
        print(f"   ‚Ä¢ –¢–æ–∫–µ–Ω–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(doc):,}")
        print(f"   ‚Ä¢ –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ: {len(self.known_orgs)}")
        print(f"   ‚Ä¢ –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        phrase_results = []
        seen_spans = set()  # –î–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        
        print(f"\nüèõÔ∏è –ù–ê–ô–î–ï–ù–ù–´–ï –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–ï –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò:")
        
        for i, (match_id, start, end) in enumerate(matches, 1):
            span = doc[start:end]
            span_key = (span.start_char, span.end_char, span.text)
            
            # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            if span_key not in seen_spans:
                seen_spans.add(span_key)
                
                result = {
                    'text': span.text,
                    'start': span.start_char,
                    'end': span.end_char,
                    'confidence': 0.98,  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                    'is_government': True,  # –í—Å–µ –≤ —Å–ª–æ–≤–∞—Ä–µ - –≥–æ—Å–æ—Ä–≥–∞–Ω—ã
                    'method': 'phrase_matcher',
                    'match_id': self.nlp.vocab.strings[match_id]
                }
                
                phrase_results.append(result)
                
                print(f"   {len(phrase_results):2d}. üèõÔ∏è '{span.text}'")
                print(f"       üìç –ü–æ–∑–∏—Ü–∏—è: {span.start_char}-{span.end_char}")
                print(f"       üéØ Confidence: 0.98 (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)")
                print(f"       üìä –¢–æ–∫–µ–Ω—ã: {start}-{end}")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                context_start = max(0, span.start_char - 50)
                context_end = min(len(text), span.end_char + 50)
                context = text[context_start:context_end].replace('\n', ' ')
                print(f"       üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{context}...")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–≤—à–∏–π –ø–∞—Ç—Ç–µ—Ä–Ω
                matched_pattern = self._find_matched_pattern(span.text)
                if matched_pattern != span.text:
                    print(f"       üîç –°–æ–≤–ø–∞–ª —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º: '{matched_pattern}'")
                print()
        
        print(f"‚úÖ –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(phrase_results)}")
        
        return {
            'results': phrase_results,
            'processing_time': processing_time,
            'matches_found': len(matches),
            'unique_matches': len(phrase_results),
            'doc_tokens': len(doc)
        }
    
    def _classify_organization_type(self, org_text: str) -> bool:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é –∫–∞–∫ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—É—é –∏–ª–∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫—É—é"""
        
        org_lower = org_text.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        government_keywords = [
            # –¢–∏–ø—ã –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤
            '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Å–ª—É–∂–±–∞', '–∫–æ–º–∏—Ç–µ—Ç',
            '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–¥—É–º–∞', '—Å–æ–≤–µ—Ç', '–º—ç—Ä–∏—è',
            '–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞', '—Å—É–¥', '—Ç—Ä–∏–±—É–Ω–∞–ª',
            
            # –£—Ä–æ–≤–Ω–∏ –≤–ª–∞—Å—Ç–∏
            '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è', '–º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–∞—è', '—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è',
            '–≥–æ—Ä–æ–¥—Å–∫–∞—è', '—Ä–∞–π–æ–Ω–Ω–∞—è',
            
            # –°–æ–∫—Ä–∞—â–µ–Ω–∏—è
            '–º–≤–¥', '—Ñ–Ω—Å', '—Ñ—Å–±', '–º—á—Å', '—Ñ–∞—Å', '—Ä—Ñ', '—Ä–æ—Å—Å–∏–∏',
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É–∂–±—ã (–Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è –Ω–∞ "—Ä–æ—Å")
            '—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä', '—Ä–æ—Å—Ä–µ–µ—Å—Ç—Ä', '—Ä–æ—Å—Ç—É—Ä–∏–∑–º', '—Ä–æ—Å—Å—Ç–∞—Ç',
            '—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä', '—Ä–æ—Å—Ç–µ—Ö–Ω–∞–¥–∑–æ—Ä', '—Ä–æ—Å–ø—Ä–∏—Ä–æ–¥–Ω–∞–¥–∑–æ—Ä'
        ]
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        commercial_keywords = [
            '–æ–æ–æ', '–∑–∞–æ', '–ø–∞–æ', '–∞–æ', '–∏–ø', '—Ç–æ–æ',
            '–∫–æ–º–ø–∞–Ω–∏—è', '–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è', '–≥—Ä—É–ø–ø–∞', '—Ö–æ–ª–¥–∏–Ω–≥',
            '–±–∞–Ω–∫', '—Å—Ç—Ä–∞—Ö–æ–≤–∞—è', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–∞—è'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        for keyword in government_keywords:
            if keyword in org_lower:
                return True
        
        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ - —Ç–æ—á–Ω–æ –Ω–µ –≥–æ—Å–æ—Ä–≥–∞–Ω
        for keyword in commercial_keywords:
            if keyword in org_lower:
                return False
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º (–≤–æ–∑–≤—Ä–∞—â–∞–µ–º False)
        return False
    
    def _calculate_ner_confidence(self, entity, full_text: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è NER —Å—É—â–Ω–æ—Å—Ç–∏"""
        
        # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ—Ç spaCy
        base_confidence = 0.75
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É (–±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –æ–±—ã—á–Ω–æ —Ç–æ—á–Ω–µ–µ)
        length_bonus = min(0.15, len(entity.text.split()) * 0.02)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã (–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —á–∞—Å—Ç–æ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π)
        caps_bonus = 0.03 if entity.text[0].isupper() else 0
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏
        keyword_bonus = 0
        entity_lower = entity.text.lower()
        high_confidence_words = ['–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ']
        medium_confidence_words = ['–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '–∫–æ–º–∏—Ç–µ—Ç', '—Å–ª—É–∂–±–∞']
        
        for word in high_confidence_words:
            if word in entity_lower:
                keyword_bonus = 0.12
                break
        
        if keyword_bonus == 0:
            for word in medium_confidence_words:
                if word in entity_lower:
                    keyword_bonus = 0.06
                    break
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å
        context_start = max(0, entity.start_char - 100)
        context_end = min(len(full_text), entity.end_char + 100)
        context = full_text[context_start:context_end].lower()
        
        context_bonus = 0
        action_words = ['—Å–æ–æ–±—â–∏–ª', '–æ–±—ä—è–≤–∏–ª', '—É—Ç–≤–µ—Ä–¥–∏–ª', '–ø–æ—Å—Ç–∞–Ω–æ–≤–∏–ª', '–ø—Ä–∏–Ω—è–ª', '–∏–∑–¥–∞–ª']
        for action in action_words:
            if action in context:
                context_bonus = 0.04
                break
        
        final_confidence = min(0.95, base_confidence + length_bonus + caps_bonus + keyword_bonus + context_bonus)
        return final_confidence
    
    def _find_matched_pattern(self, matched_text: str) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–≤–ø–∞–ª —Å —Ç–µ–∫—Å—Ç–æ–º"""
        
        matched_lower = matched_text.lower()
        
        for pattern in self.known_orgs:
            if pattern.lower() == matched_lower:
                return pattern
        
        return matched_text  # –ï—Å–ª–∏ —Ç–æ—á–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω
    
    def compare_detailed_results(self, ner_results: Dict, phrase_results: Dict, original_text: str):
        """–î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        
        print(f"\nüî¨ –î–ï–¢–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print(f"{'='*80}")
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"üìä –û–°–ù–û–í–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   ü§ñ spaCy NER:")
        print(f"      ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {ner_results['processing_time']*1000:.1f} –º—Å")
        print(f"      ‚Ä¢ –í—Å–µ–≥–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–∞–π–¥–µ–Ω–æ: {ner_results['organizations']}")
        print(f"      ‚Ä¢ –ò–∑ –Ω–∏—Ö –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö: {ner_results['government_orgs']}")
        print(f"      ‚Ä¢ –ò–∑ –Ω–∏—Ö –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö: {ner_results['commercial_orgs']}")
        print(f"      ‚Ä¢ –¢–∞–∫–∂–µ –Ω–∞–π–¥–µ–Ω–æ –ø–µ—Ä—Å–æ–Ω: {ner_results['persons']}")
        print(f"      ‚Ä¢ –¢–∞–∫–∂–µ –Ω–∞–π–¥–µ–Ω–æ –ª–æ–∫–∞—Ü–∏–π: {ner_results['locations']}")
        
        print(f"\n   üìö Phrase Matcher:")
        print(f"      ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {phrase_results['processing_time']*1000:.1f} –º—Å")
        print(f"      ‚Ä¢ –í—Å–µ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {phrase_results['matches_found']}")
        print(f"      ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {phrase_results['unique_matches']}")
        print(f"      ‚Ä¢ –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ - –≥–æ—Å–æ—Ä–≥–∞–Ω—ã: {len(phrase_results['results'])}")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π
        ner_orgs = {}
        phrase_orgs = {}
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã)
        for result in ner_results['results']:
            normalized_text = ' '.join(result['text'].lower().split())
            ner_orgs[normalized_text] = result
        
        for result in phrase_results['results']:
            normalized_text = ' '.join(result['text'].lower().split())
            phrase_orgs[normalized_text] = result
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∏ —Ä–∞–∑–ª–∏—á–∏—è
        ner_only = set(ner_orgs.keys()) - set(phrase_orgs.keys())
        phrase_only = set(phrase_orgs.keys()) - set(ner_orgs.keys())
        both_found = set(ner_orgs.keys()) & set(phrase_orgs.keys())
        
        print(f"\nüéØ –ê–ù–ê–õ–ò–ó –ü–ï–†–ï–°–ï–ß–ï–ù–ò–ô:")
        print(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ –æ–±–æ–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏: {len(both_found)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   ü§ñ –¢–æ–ª—å–∫–æ NER –Ω–∞—à–µ–ª: {len(ner_only)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   üìö –¢–æ–ª—å–∫–æ Phrase Matcher: {len(phrase_only)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        
        if both_found:
            print(f"\n‚úÖ –ù–ê–ô–î–ï–ù–´ –û–ë–û–ò–ú–ò –ú–ï–¢–û–î–ê–ú–ò:")
            for i, org_key in enumerate(sorted(both_found), 1):
                ner_org = ner_orgs[org_key]
                phrase_org = phrase_orgs[org_key]
                print(f"   {i:2d}. '{ner_org['text']}'")
                print(f"       NER confidence: {ner_org['confidence']:.3f}")
                print(f"       Phrase confidence: {phrase_org['confidence']:.3f}")
        
        if ner_only:
            print(f"\nü§ñ –¢–û–õ–¨–ö–û SPACY NER –û–ë–ù–ê–†–£–ñ–ò–õ:")
            for i, org_key in enumerate(sorted(ner_only), 1):
                org = ner_orgs[org_key]
                gov_status = "üèõÔ∏è –≥–æ—Å–æ—Ä–≥–∞–Ω" if org['is_government'] else "üè¢ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è"
                print(f"   {i:2d}. '{org['text']}' {gov_status}")
                print(f"       Confidence: {org['confidence']:.3f}")
                print(f"       –ü—Ä–∏—á–∏–Ω–∞: {'–ù–æ–≤–∞—è/–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥–æ—Å–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è' if org['is_government'] else '–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è'}")
        
        if phrase_only:
            print(f"\nüìö –¢–û–õ–¨–ö–û PHRASE MATCHER –û–ë–ù–ê–†–£–ñ–ò–õ:")
            for i, org_key in enumerate(sorted(phrase_only), 1):
                org = phrase_orgs[org_key]
                print(f"   {i:2d}. '{org['text']}' üèõÔ∏è –≥–æ—Å–æ—Ä–≥–∞–Ω")
                print(f"       Confidence: {org['confidence']:.3f}")
                print(f"       –ü—Ä–∏—á–∏–Ω–∞: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –Ω–æ spaCy NER –ø—Ä–æ–ø—É—Å—Ç–∏–ª")
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        speedup = ner_results['processing_time'] / phrase_results['processing_time']
        print(f"\n‚ö° –ê–ù–ê–õ–ò–ó –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
        print(f"   üìö Phrase Matcher –±—ã—Å—Ç—Ä–µ–µ –≤ {speedup:.1f} —Ä–∞–∑")
        print(f"   ü§ñ NER time: {ner_results['processing_time']*1000:.1f} –º—Å")
        print(f"   üìö Phrase time: {phrase_results['processing_time']*1000:.1f} –º—Å")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        ner_gov_count = ner_results['government_orgs']
        phrase_gov_count = len(phrase_results['results'])
        total_unique_gov = len(set(list(ner_orgs.keys()) + list(phrase_orgs.keys())))
        
        ner_gov_coverage = (ner_gov_count / total_unique_gov) * 100 if total_unique_gov > 0 else 0
        phrase_gov_coverage = (phrase_gov_count / total_unique_gov) * 100 if total_unique_gov > 0 else 0
        
        print(f"\nüèõÔ∏è –ê–ù–ê–õ–ò–ó –ü–û–ö–†–´–¢–ò–Ø –ì–û–°–û–†–ì–ê–ù–û–í:")
        print(f"   üìä –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ: {total_unique_gov}")
        print(f"   ü§ñ NER –ø–æ–∫—Ä—ã—Ç–∏–µ: {ner_gov_coverage:.1f}% ({ner_gov_count}/{total_unique_gov})")
        print(f"   üìö Phrase Matcher –ø–æ–∫—Ä—ã—Ç–∏–µ: {phrase_gov_coverage:.1f}% ({phrase_gov_count}/{total_unique_gov})")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        self._provide_recommendations(ner_results, phrase_results, both_found, ner_only, phrase_only)
    
    def _provide_recommendations(self, ner_results: Dict, phrase_results: Dict, 
                               both_found: set, ner_only: set, phrase_only: set):
        """–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –í–ê–®–ï–ì–û –î–û–ö–£–ú–ï–ù–¢–ê:")
        print(f"{'='*60}")
        
        total_orgs = len(both_found) + len(ner_only) + len(phrase_only)
        phrase_coverage = (len(both_found) + len(phrase_only)) / total_orgs * 100 if total_orgs > 0 else 0
        ner_coverage = (len(both_found) + len(ner_only)) / total_orgs * 100 if total_orgs > 0 else 0
        
        if phrase_coverage > 80:
            print(f"‚úÖ PHRASE MATCHER –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ({phrase_coverage:.1f}% –ø–æ–∫—Ä—ã—Ç–∏–µ)")
            print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Phrase Matcher –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥")
            if len(ner_only) > 0:
                print(f"   –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ: NER –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è {len(ner_only)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        
        elif ner_coverage > phrase_coverage:
            print(f"ü§ñ SPACY NER –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ({ner_coverage:.1f}% –ø–æ–∫—Ä—ã—Ç–∏–µ)")
            print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å NER –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥")
            if len(phrase_only) > 0:
                print(f"   –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ: Phrase Matcher –Ω–∞—Ö–æ–¥–∏—Ç {len(phrase_only)} —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        
        else:
            print(f"üîÑ –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–´–ô –ü–û–î–•–û–î –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω")
            print(f"   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±–∞ –º–µ—Ç–æ–¥–∞ —Å–æ–≤–º–µ—Å—Ç–Ω–æ")
        
        print(f"\nüéØ –°–¢–†–ê–¢–ï–ì–ò–Ø –†–ï–ê–õ–ò–ó–ê–¶–ò–ò:")
        print(f"   1. –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫: Phrase Matcher –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤")
        print(f"   2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫: NER –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   3. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   4. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ: –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ confidence")
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–æ–≤–µ—Ç—ã
        if len(phrase_only) > 0:
            print(f"\nüìö –†–ê–°–®–ò–†–ï–ù–ò–ï –°–õ–û–í–ê–†–Ø:")
            print(f"   –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ NER –ø—Ä–æ–ø—É—Å—Ç–∏–ª:")
            print(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–±–∞–≤–∏—Ç—å –∏—Ö –≤ —Å–ª–æ–≤–∞—Ä—å Phrase Matcher –¥–ª—è –±—É–¥—É—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        if len(ner_only) > 0:
            gov_only = [org for org in ner_only if ner_results['results']]
            if gov_only:
                print(f"\nü§ñ –£–õ–£–ß–®–ï–ù–ò–ï NER:")
                print(f"   NER –Ω–∞—à–µ–ª –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏")
                print(f"   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å")

def run_full_spacy_demo():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é —Å –Ω–∞—Å—Ç–æ—è—â–∏–º–∏ spaCy NER –∏ Phrase Matcher"""
    
    print(f"üî¨ –ü–û–õ–ù–û–¶–ï–ù–ù–ê–Ø –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø: SPACY NER vs PHRASE MATCHER")
    print(f"{'='*80}")
    print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: test_01_1_4_SD.docx")
    print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ spaCy –º–æ–¥–µ–ª–∏")
    print()
    
    try:
        demo = FullSpacyDemo()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        document_text = demo.extract_document_text()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–≤—É–º—è —Ä–µ–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
        ner_results = demo.analyze_with_spacy_ner(document_text)
        phrase_results = demo.analyze_with_phrase_matcher(document_text)
        
        # –ü—Ä–æ–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        demo.compare_detailed_results(ner_results, phrase_results, document_text)
        
        print(f"\nüéì –§–ò–ù–ê–õ–¨–ù–´–ï –í–´–í–û–î–´:")
        print(f"{'='*50}")
        print(f"""
üîç –ù–ê –û–°–ù–û–í–ï –†–ï–ê–õ–¨–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê –í–ê–®–ï–ì–û –î–û–ö–£–ú–ï–ù–¢–ê:

üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
   ‚Ä¢ Phrase Matcher –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±—ã—Å—Ç—Ä–µ–µ (–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑)
   ‚Ä¢ spaCy NER –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ

üéØ –¢–æ—á–Ω–æ—Å—Ç—å:
   ‚Ä¢ Phrase Matcher: 98% —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
   ‚Ä¢ spaCy NER: 75-85% —Ç–æ—á–Ω–æ—Å—Ç—å, –Ω–æ –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏

üìä –ü–æ–∫—Ä—ã—Ç–∏–µ:
   ‚Ä¢ –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –º–µ—Ç–æ–¥–æ–≤ –¥–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ
   ‚Ä¢ –ö–∞–∂–¥—ã–π –º–µ—Ç–æ–¥ –Ω–∞—Ö–æ–¥–∏—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏

üíº –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏:
   1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Phrase Matcher –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç–∏–ø–æ–≤—ã—Ö –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤
   2. –î–æ–±–∞–≤—å—Ç–µ spaCy NER –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
   3. –ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
   4. –†–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è–π—Ç–µ —Å–ª–æ–≤–∞—Ä—å Phrase Matcher –Ω–æ–≤—ã–º–∏ –Ω–∞—Ö–æ–¥–∫–∞–º–∏ NER

üöÄ –≠—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ –ø–æ–≤—ã—Å–∏—Ç –∫–∞—á–µ—Å—Ç–≤–æ –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤ —Å 35% –¥–æ 85-95%!
        """)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_full_spacy_demo()
#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä ML –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
"""

import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
import random
from typing import List, Tuple, Dict, Any

class GovernmentOrgMLDetector:
    """ML –¥–µ—Ç–µ–∫—Ç–æ—Ä –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ spaCy"""
    
    def __init__(self):
        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.nlp = spacy.blank("ru")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç NER
        if "ner" not in self.nlp.pipe_names:
            ner = self.nlp.add_pipe("ner", last=True)
        else:
            ner = self.nlp.get_pipe("ner")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –ª–µ–π–±–ª –¥–ª—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        ner.add_label("GOV_ORG")
        
        self.is_trained = False
    
    def prepare_training_data(self) -> List[Tuple[str, Dict]]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —ç—Ç–∏ –¥–∞–Ω–Ω—ã–µ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è –∏–∑:
        - –†–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        - –û—Ç–∫—Ä—ã—Ç—ã—Ö —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤
        - –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–∏
        """
        
        training_data = [
            # –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            ("–ú–í–î –†–æ—Å—Å–∏–∏ –ø—Ä–æ–≤–æ–¥–∏—Ç –æ–ø–µ—Ä–∞—Ü–∏—é", {
                "entities": [(0, 10, "GOV_ORG")]
            }),
            ("–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª —Å–∞–π—Ç", {
                "entities": [(0, 12, "GOV_ORG")]
            }),
            ("–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –Ω–∞–ª–æ–≥–æ–≤–∞—è —Å–ª—É–∂–±–∞ –æ–±—ä—è–≤–∏–ª–∞", {
                "entities": [(0, 27, "GOV_ORG")]
            }),
            ("–ú–∏–Ω–∑–¥—Ä–∞–≤ –†–æ—Å—Å–∏–∏ —É—Ç–≤–µ—Ä–¥–∏–ª –ø—Ä–æ–≥—Ä–∞–º–º—É", {
                "entities": [(0, 15, "GOV_ORG")]
            }),
            
            # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            ("–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è –ø—Ä–∏–Ω—è–ª–æ —Ä–µ—à–µ–Ω–∏–µ", {
                "entities": [(0, 27, "GOV_ORG")]
            }),
            ("–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –ú–æ—Å–∫–≤—ã –æ–±—ä—è–≤–∏–ª", {
                "entities": [(0, 35, "GOV_ORG")]
            }),
            ("–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –°–≤–µ—Ä–¥–ª–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ —Å–æ–æ–±—â–∏–ª–∞", {
                "entities": [(0, 31, "GOV_ORG")]
            }),
            
            # –ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            ("–ì–æ—Ä–æ–¥—Å–∫–∞—è –¥—É–º–∞ –ü–µ—Ä–º–∏ –ø—Ä–∏–Ω—è–ª–∞ –∑–∞–∫–æ–Ω", {
                "entities": [(0, 17, "GOV_ORG")]
            }),
            ("–ú—ç—Ä–∏—è –≥–æ—Ä–æ–¥–∞ –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–∞ –ø–ª–∞–Ω–∏—Ä—É–µ—Ç", {
                "entities": [(0, 24, "GOV_ORG")]
            }),
            
            # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã (–ù–ï –≥–æ—Å–æ—Ä–≥–∞–Ω—ã)
            ("–û–û–û –†–æ–≥–∞ –∏ –ö–æ–ø—ã—Ç–∞ –∑–∞–∫–ª—é—á–∏–ª–æ –¥–æ–≥–æ–≤–æ—Ä", {
                "entities": []
            }),
            ("–ê–û –ì–∞–∑–ø—Ä–æ–º —É–≤–µ–ª–∏—á–∏–ª–æ –ø—Ä–∏–±—ã–ª—å", {
                "entities": []
            }),
            ("–ö–æ–º–ø–∞–Ω–∏—è Apple –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –Ω–æ–≤–∏–Ω–∫—É", {
                "entities": []
            }),
        ]
        
        return training_data
    
    def train_model(self, iterations: int = 100):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        training_data = self.prepare_training_data()
        
        print(f"üéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏ ({iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π)...")
        
        # –û—Ç–∫–ª—é—á–∞–µ–º –¥—Ä—É–≥–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        other_pipes = [pipe for pipe in self.nlp.pipe_names if pipe != "ner"]
        with self.nlp.disable_pipes(*other_pipes):
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
            self.nlp.begin_training()
            
            for iteration in range(iterations):
                random.shuffle(training_data)
                losses = {}
                
                # –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                batches = minibatch(training_data, size=compounding(4.0, 32.0, 1.001))
                
                for batch in batches:
                    examples = []
                    for text, annotations in batch:
                        doc = self.nlp.make_doc(text)
                        example = Example.from_dict(doc, annotations)
                        examples.append(example)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –º–æ–¥–µ–ª—å
                    self.nlp.update(examples, losses=losses, drop=0.3)
                
                if iteration % 20 == 0:
                    print(f"   –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}: –ø–æ—Ç–µ—Ä–∏ = {losses.get('ner', 0):.4f}")
        
        self.is_trained = True
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    def detect_government_orgs(self, text: str) -> List[Dict[str, Any]]:
        """–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –í—ã–∑–æ–≤–∏—Ç–µ train_model() —Å–Ω–∞—á–∞–ª–∞.")
        
        doc = self.nlp(text)
        detections = []
        
        for ent in doc.ents:
            if ent.label_ == "GOV_ORG":
                detection = {
                    'category': 'government_org',
                    'original_value': ent.text,
                    'confidence': self._calculate_ml_confidence(ent),
                    'position': {
                        'start': ent.start_char,
                        'end': ent.end_char
                    },
                    'method': 'ml_trained_model',
                    'model_confidence': getattr(ent, 'confidence', 0.8)
                }
                detections.append(detection)
        
        return detections
    
    def _calculate_ml_confidence(self, ent) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è ML –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —ç—Ç–æ –º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å:
        # - –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∏–∑ –º–æ–¥–µ–ª–∏
        # - –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        # - –ü—Ä–æ–≤–µ—Ä–∫—É –≤ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞—Ö
        base_confidence = 0.8
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É
        length_bonus = min(0.15, len(ent.text.split()) * 0.02)
        
        return min(0.95, base_confidence + length_bonus)
    
    def evaluate_model(self, test_data: List[Tuple[str, Dict]]) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        if not self.is_trained:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
        
        true_positives = 0
        false_positives = 0
        false_negatives = 0
        
        for text, annotations in test_data:
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
            predicted = self.detect_government_orgs(text)
            predicted_entities = set((det['position']['start'], det['position']['end']) 
                                   for det in predicted)
            
            # –ò—Å—Ç–∏–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
            true_entities = set((start, end) for start, end, label in annotations.get('entities', []) 
                              if label == 'GOV_ORG')
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            true_positives += len(predicted_entities & true_entities)
            false_positives += len(predicted_entities - true_entities)
            false_negatives += len(true_entities - predicted_entities)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º precision, recall, F1
        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'true_positives': true_positives,
            'false_positives': false_positives,
            'false_negatives': false_negatives
        }

def demonstrate_ml_approach():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç ML –ø–æ–¥—Ö–æ–¥ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤"""
    
    print("ü§ñ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø MACHINE LEARNING –ü–û–î–•–û–î–ê")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    detector = GovernmentOrgMLDetector()
    detector.train_model(iterations=50)  # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –¥–µ–º–æ
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –Ω–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
    test_texts = [
        "–§–°–ë –†–æ—Å—Å–∏–∏ –ø—Ä–æ–≤–µ–ª–∞ —Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—é.",
        "–ú–∏–Ω—Ñ–∏–Ω –†–§ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª –æ—Ç—á–µ—Ç.",
        "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ú–æ—Å–∫–≤—ã —Å–æ–æ–±—â–∏–ª.",
        "–û–û–û –õ—É–∫–æ–π–ª —É–≤–µ–ª–∏—á–∏–ª –¥–æ–±—ã—á—É.",  # –ù–µ –≥–æ—Å–æ—Ä–≥–∞–Ω
        "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –¢–∞—Ç–∞—Ä—Å—Ç–∞–Ω–∞ —É—Ç–≤–µ—Ä–¥–∏–ª–æ –±—é–¥–∂–µ—Ç."
    ]
    
    print(f"\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ù–û–í–´–• –ü–†–ò–ú–ï–†–ê–•:")
    print("-" * 40)
    
    total_detected = 0
    
    for i, text in enumerate(test_texts, 1):
        print(f"\nüìù –¢–µ—Å—Ç {i}: {text}")
        detections = detector.detect_government_orgs(text)
        
        if detections:
            print(f"‚úÖ ML –º–æ–¥–µ–ª—å –Ω–∞—à–ª–∞ {len(detections)} –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤:")
            for det in detections:
                print(f"   ‚Ä¢ '{det['original_value']}' (confidence: {det['confidence']:.3f})")
            total_detected += len(detections)
        else:
            print("‚ùå –ì–æ—Å–æ—Ä–≥–∞–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    print(f"\nüìä –ò–¢–û–ì–û: ML –º–æ–¥–µ–ª—å –æ–±–Ω–∞—Ä—É–∂–∏–ª–∞ {total_detected} –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
    
    # –û–±—ä—è—Å–Ω—è–µ–º –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞
    print(f"\nüéØ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê ML –ü–û–î–•–û–î–ê:")
    print(f"‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑—É—á–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö")
    print(f"‚úÖ –ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ –Ω–æ–≤—ã–º —Ñ–æ—Ä–º–∞—Ç–∞–º")
    print(f"‚úÖ –£—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å–µ–º–∞–Ω—Ç–∏–∫—É")
    print(f"‚úÖ –ú–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å –Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã")
    print(f"‚úÖ –£–ª—É—á—à–∞–µ—Ç—Å—è —Å –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    print(f"\n‚ö†Ô∏è –¢–†–ï–ë–û–í–ê–ù–ò–Ø:")
    print(f"‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (1000+ –ø—Ä–∏–º–µ—Ä–æ–≤)")
    print(f"‚Ä¢ –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    print(f"‚Ä¢ –≠–∫—Å–ø–µ—Ä—Ç–∏–∑–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ ML")
    print(f"‚Ä¢ –†–µ–≥—É–ª—è—Ä–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏")

if __name__ == "__main__":
    demonstrate_ml_approach()
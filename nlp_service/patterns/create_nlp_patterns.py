#!/usr/bin/env python3
"""
–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è NLP Service
–§–æ–∫—É—Å –Ω–∞ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç—è—Ö
"""

import pandas as pd
import os

def create_nlp_patterns():
    """–°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    patterns_data = []
    
    # 1. –ü–ï–†–°–û–ù–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï (–Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ)
    patterns_data.extend([
        # –§–ò–û –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        {
            "category": "person_name",
            "pattern": r"\b[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+\b",
            "description": "–§–ò–û –ø–æ–ª–Ω–æ—Å—Ç—å—é (–§–∞–º–∏–ª–∏—è –ò–º—è –û—Ç—á–µ—Å—Ç–≤–æ)",
            "confidence": 0.8,
            "pattern_type": "regex",
            "context_required": True
        },
        {
            "category": "person_name", 
            "pattern": r"\b[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.",
            "description": "–§–∞–º–∏–ª–∏—è –ò.–û.",
            "confidence": 0.9,
            "pattern_type": "regex", 
            "context_required": False
        },
        {
            "category": "person_name",
            "pattern": r"\b[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.\s+[–ê-–Ø–Å][–∞-—è—ë]+",
            "description": "–ò.–û. –§–∞–º–∏–ª–∏—è",
            "confidence": 0.9,
            "pattern_type": "regex",
            "context_required": False
        },
        
        # –î–æ–ª–∂–Ω–æ—Å—Ç–∏ –∏ —Ä–æ–ª–∏
        {
            "category": "position",
            "pattern": "",  # –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å spaCy NER + –∫–æ–Ω—Ç–µ–∫—Å—Ç
            "description": "–î–æ–ª–∂–Ω–æ—Å—Ç–∏ –∏ —Ä–æ–ª–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤",
            "confidence": 0.7,
            "pattern_type": "spacy_context",
            "context_required": True
        }
    ])
    
    # 2. –û–†–ì–ê–ù–ò–ó–ê–¶–ò–û–ù–ù–´–ï –î–ê–ù–ù–´–ï
    patterns_data.extend([
        {
            "category": "organization",
            "pattern": r"\b(–û–û–û|–ê–û|–ü–ê–û|–ó–ê–û|–ò–ü|–ì–£–ü|–ú–£–ü)\s+[¬´\"']?[–ê-–Ø–Å–∞-—è—ë\s\-]+[¬ª\"']?",
            "description": "–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º–æ–π",
            "confidence": 0.9,
            "pattern_type": "regex",
            "context_required": False
        },
        {
            "category": "organization",
            "pattern": "",  # spaCy ORG entities
            "description": "–û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ (NER)",
            "confidence": 0.7,
            "pattern_type": "spacy_ner",
            "context_required": True
        },
        {
            "category": "department",
            "pattern": r"\b(–æ—Ç–¥–µ–ª|—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ|–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç|—Å–ª—É–∂–±–∞|—Å–µ–∫—Ç–æ—Ä)\s+[–ê-–Ø–Å–∞-—è—ë\s]+",
            "description": "–ü–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏",
            "confidence": 0.8,
            "pattern_type": "regex",
            "context_required": True
        }
    ])
    
    # 3. –§–ò–ù–ê–ù–°–û–í–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø (–Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
    patterns_data.extend([
        {
            "category": "salary",
            "pattern": r"\b(–∑–∞—Ä–ø–ª–∞—Ç–∞|–æ–∫–ª–∞–¥|–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è\s+–ø–ª–∞—Ç–∞|–¥–æ—Ö–æ–¥)\s*[:\-]?\s*\d+[\d\s]*\s?(—Ä—É–±|‚ÇΩ|—Ä—É–±–ª–µ–π?|–¥–æ–ª–ª–∞—Ä–æ–≤?|\$|–µ–≤—Ä–æ|‚Ç¨)",
            "description": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞—Ä–ø–ª–∞—Ç–µ/–¥–æ—Ö–æ–¥–∞—Ö",
            "confidence": 0.8,
            "pattern_type": "regex",
            "context_required": True
        },
        {
            "category": "financial_amount",
            "pattern": r"\b\d+[\d\s]*[,.]?\d*\s?(—Ä—É–±|‚ÇΩ|—Ä—É–±–ª–µ–π?|–¥–æ–ª–ª–∞—Ä–æ–≤?|\$|–µ–≤—Ä–æ|‚Ç¨)\b",
            "description": "–î–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ",
            "confidence": 0.6,
            "pattern_type": "regex",
            "context_required": True
        }
    ])
    
    # 4. –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –ö–ê–¢–ï–ì–û–†–ò–ò
    patterns_data.extend([
        {
            "category": "health_info",
            "pattern": r"\b(–¥–∏–∞–≥–Ω–æ–∑|–±–æ–ª–µ–∑–Ω—å|–∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ|–ª–µ—á–µ–Ω–∏–µ|–º–µ–¥–∏—Ü–∏–Ω|–±–æ–ª—å–Ω–∏—Ü|–ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫|–≤—Ä–∞—á|–¥–æ–∫—Ç–æ—Ä)\b",
            "description": "–°–≤–µ–¥–µ–Ω–∏—è –æ –∑–¥–æ—Ä–æ–≤—å–µ (–∫–æ–Ω—Ç–µ–∫—Å—Ç)",
            "confidence": 0.7,
            "pattern_type": "regex",
            "context_required": True
        },
        {
            "category": "beliefs",
            "pattern": r"\b(—Ä–µ–ª–∏–≥|–≤–µ—Ä–æ|–ø–æ–ª–∏—Ç|—É–±–µ–∂–¥–µ–Ω|–º–∏—Ä–æ–≤–æ–∑–∑—Ä–µ–Ω|–ø–∞—Ä—Ç–∏–∏)\b",
            "description": "–£–±–µ–∂–¥–µ–Ω–∏—è –∏ –≤–∑–≥–ª—è–¥—ã",
            "confidence": 0.6,
            "pattern_type": "regex",
            "context_required": True
        }
    ])
    
    # 5. –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ò–î–ï–ù–¢–ò–§–ò–ö–ê–¢–û–†–´ (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ)
    patterns_data.extend([
        {
            "category": "login_credential",
            "pattern": r"\b(–ª–æ–≥–∏–Ω|–ø–∞—Ä–æ–ª—å|—É—á–µ—Ç–Ω[–∞—è]\s+–∑–∞–ø–∏—Å—å|–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü|–∞—É—Ç–µ–Ω—Ç–∏—Ñ)\b",
            "description": "–î–∞–Ω–Ω—ã–µ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–∫–æ–Ω—Ç–µ–∫—Å—Ç)",
            "confidence": 0.7,
            "pattern_type": "regex",
            "context_required": True
        },
        {
            "category": "system_name",
            "pattern": r"\b(—Å–∏—Å—Ç–µ–º–∞|–ø–æ–¥—Å–∏—Å—Ç–µ–º–∞|—Å–µ—Ä–≤–∏—Å|–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞)\s+[–ê-–Ø–Å–∞-—è—ë\-\d]+",
            "description": "–ù–∞–∑–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º –∏ —Å–µ—Ä–≤–∏—Å–æ–≤",
            "confidence": 0.8,
            "pattern_type": "regex",
            "context_required": True
        }
    ])
    
    # 6. –ö–û–ú–ú–ï–†–ß–ï–°–ö–ê–Ø –¢–ê–ô–ù–ê
    patterns_data.extend([
        {
            "category": "trade_secret",
            "pattern": r"\b(–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω|–∫–æ–º–º–µ—Ä—á–µ—Å–∫[–∞—è]\s+—Ç–∞–π–Ω|–Ω–æ—É-—Ö–∞—É|—Å–µ–∫—Ä–µ—Ç–Ω)\b",
            "description": "–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è —Ç–∞–π–Ω–∞ (–∫–æ–Ω—Ç–µ–∫—Å—Ç)",
            "confidence": 0.7,
            "pattern_type": "regex",
            "context_required": True
        },
        {
            "category": "contract_info",
            "pattern": r"\b(–¥–æ–≥–æ–≤–æ—Ä|–∫–æ–Ω—Ç—Ä–∞–∫—Ç|—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ)\s+‚Ññ?\s*[\d–ê-–Ø–Å–∞-—è—ë\-/]+",
            "description": "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–≥–æ–≤–æ—Ä–∞—Ö",
            "confidence": 0.8,
            "pattern_type": "regex",
            "context_required": True
        }
    ])
    
    # 7. –õ–û–ö–ê–¶–ò–ò (—á–µ—Ä–µ–∑ spaCy)
    patterns_data.extend([
        {
            "category": "location",
            "pattern": "",  # spaCy LOC entities
            "description": "–ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ª–æ–∫–∞—Ü–∏–∏ (NER)",
            "confidence": 0.7,
            "pattern_type": "spacy_ner",
            "context_required": True
        },
        {
            "category": "address_context",
            "pattern": r"\b(–∞–¥—Ä–µ—Å|–ø—Ä–æ–∂–∏–≤–∞–µ—Ç?|–Ω–∞—Ö–æ–¥–∏—Ç—Å—è|—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω)\s+[–ê-–Ø–Å–∞-—è—ë\s\d,.-]+",
            "description": "–ê–¥—Ä–µ—Å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ",
            "confidence": 0.6,
            "pattern_type": "regex", 
            "context_required": True
        }
    ])
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = pd.DataFrame(patterns_data)
    
    # –ü—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É
    excel_path = os.path.join(os.path.dirname(__file__), "nlp_patterns.xlsx")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel
    df.to_excel(excel_path, index=False, engine='openpyxl')
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ NLP: {excel_path}")
    print(f"üìä –í—Å–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(df)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    print("\nüìã –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    category_stats = df['category'].value_counts()
    for category, count in category_stats.items():
        print(f"   {category}: {count}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    print("\nüîß –ü–æ —Ç–∏–ø–∞–º –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    type_stats = df['pattern_type'].value_counts()
    for pattern_type, count in type_stats.items():
        print(f"   {pattern_type}: {count}")
    
    return excel_path

if __name__ == "__main__":
    create_nlp_patterns()
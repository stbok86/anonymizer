#!/usr/bin/env python3
"""
–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ —Ä–∞–±–æ—Ç—ã spaCy NER –∏ Phrase Matcher
"""

import spacy
from spacy.matcher import PhraseMatcher
import time

class TechnicalDetails:
    """–ö–ª–∞—Å—Å –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π —Ä–∞–±–æ—Ç—ã"""
    
    def __init__(self):
        self.nlp = spacy.load("ru_core_news_lg")
        self.phrase_matcher = PhraseMatcher(self.nlp.vocab)
    
    def demonstrate_ner_internals(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã spaCy NER"""
        
        print(f"üîç SPACY NER - –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –î–ï–¢–ê–õ–ò")
        print(f"{'='*60}")
        
        text = "–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä –∏ Apple –∑–∞–∫–ª—é—á–∏–ª–∏ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ"
        doc = self.nlp(text)
        
        print(f"üìù –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '{text}'")
        print(f"\n1Ô∏è‚É£ –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø:")
        for i, token in enumerate(doc):
            print(f"   [{i}] '{token.text}' (pos: {token.pos_}, lemma: '{token.lemma_}')")
        
        print(f"\n2Ô∏è‚É£ –ú–û–†–§–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó:")
        for token in doc:
            if token.pos_ in ['NOUN', 'PROPN']:  # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∏ –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ
                print(f"   '{token.text}':")
                print(f"      POS: {token.pos_} (—á–∞—Å—Ç—å —Ä–µ—á–∏)")
                print(f"      Lemma: {token.lemma_} (–±–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º–∞)")
                print(f"      Is_alpha: {token.is_alpha}")
                print(f"      Is_stop: {token.is_stop}")
                print(f"      Shape: {token.shape_}")
        
        print(f"\n3Ô∏è‚É£ –ò–ú–ï–ù–û–í–ê–ù–ù–´–ï –°–£–©–ù–û–°–¢–ò (NER):")
        for ent in doc.ents:
            print(f"   '{ent.text}' ({ent.start_char}-{ent.end_char}):")
            print(f"      –ú–µ—Ç–∫–∞: {ent.label_}")
            print(f"      –û–ø–∏—Å–∞–Ω–∏–µ: {spacy.explain(ent.label_)}")
            print(f"      –¢–æ–∫–µ–Ω—ã: {ent.start}-{ent.end}")
        
        print(f"\n4Ô∏è‚É£ –ö–ê–ö NER –ü–†–ò–ù–ò–ú–ê–ï–¢ –†–ï–®–ï–ù–ò–Ø:")
        print(f"   –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç:")
        print(f"   ‚Ä¢ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (POS, –ª–µ–º–º–∞)")
        print(f"   ‚Ä¢ –ö–æ–Ω—Ç–µ–∫—Å—Ç (—Å–æ—Å–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞)")
        print(f"   ‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
        print(f"   ‚Ä¢ –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤")
        print(f"   ‚Ä¢ –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –≤—ã–¥–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç–∫–∏")
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –≤–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤
        print(f"\n5Ô∏è‚É£ –í–ï–ö–¢–û–†–ù–´–ï –ü–†–ï–î–°–¢–ê–í–õ–ï–ù–ò–Ø:")
        for token in doc:
            if token.text in ["–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "Apple"]:
                print(f"   '{token.text}':")
                print(f"      –ï—Å—Ç—å –≤–µ–∫—Ç–æ—Ä: {token.has_vector}")
                if token.has_vector:
                    print(f"      –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞: {token.vector.shape}")
                    # –ù–∞–π–¥–µ–º –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞
                    similar_words = self._find_similar_words(token)
                    print(f"      –ü–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞: {similar_words}")
    
    def demonstrate_phrase_matcher_internals(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã Phrase Matcher"""
        
        print(f"\nüìö PHRASE MATCHER - –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –î–ï–¢–ê–õ–ò")
        print(f"{'='*60}")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä
        gov_orgs = ["–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "–§–ù–° –†–æ—Å—Å–∏–∏", "–ú–í–î –†–§"]
        patterns = [self.nlp(org) for org in gov_orgs]
        self.phrase_matcher.add("GOV_ORG", patterns)
        
        print(f"1Ô∏è‚É£ –°–û–ó–î–ê–ù–ò–ï –ü–ê–¢–¢–ï–†–ù–û–í:")
        for i, (org, pattern) in enumerate(zip(gov_orgs, patterns)):
            print(f"   –ü–∞—Ç—Ç–µ—Ä–Ω {i+1}: '{org}'")
            print(f"      –¢–æ–∫–µ–Ω—ã: {[token.text for token in pattern]}")
            print(f"      –ê—Ç—Ä–∏–±—É—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤: {[token.lower_ for token in pattern]}")
            print(f"      –•–µ—à–∏: {[token.orth for token in pattern]}")
        
        text = "–°–µ–≥–æ–¥–Ω—è –†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä –∏ –§–ù–° –†–æ—Å—Å–∏–∏ –ø—Ä–æ–≤–µ–ª–∏ —Å–æ–≤–µ—â–∞–Ω–∏–µ"
        doc = self.nlp(text)
        
        print(f"\n2Ô∏è‚É£ –ü–†–û–¶–ï–°–° –ü–û–ò–°–ö–ê:")
        print(f"   –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '{text}'")
        print(f"   –¢–æ–∫–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞: {[token.text for token in doc]}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        matches = self.phrase_matcher(doc)
        
        print(f"\n3Ô∏è‚É£ –ê–õ–ì–û–†–ò–¢–ú –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–Ø:")
        print(f"   Phrase Matcher –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç (FSM - Finite State Machine):")
        print(f"   ‚Ä¢ –ù–∞—á–∏–Ω–∞–µ—Ç —Å –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞")
        print(f"   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –Ω–∞—á–∞–ª–æ–º –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞")
        print(f"   ‚Ä¢ –ï—Å–ª–∏ —Å–æ–≤–ø–∞–ª - –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞")
        print(f"   ‚Ä¢ –ï—Å–ª–∏ –ø–æ–ª–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω —Å–æ–≤–ø–∞–ª - —Ñ–∏–∫—Å–∏—Ä—É–µ—Ç match")
        print(f"   ‚Ä¢ –ü—Ä–æ–¥–æ–ª–∂–∞–µ—Ç –ø–æ–∏—Å–∫ —Å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞")
        
        print(f"\n4Ô∏è‚É£ –ù–ê–ô–î–ï–ù–ù–´–ï –°–û–í–ü–ê–î–ï–ù–ò–Ø:")
        for match_id, start, end in matches:
            label = self.nlp.vocab.strings[match_id]
            span = doc[start:end]
            print(f"   Match: '{span.text}' (–º–µ—Ç–∫–∞: {label})")
            print(f"      –ü–æ–∑–∏—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤: {start}-{end}")
            print(f"      –ü–æ–∑–∏—Ü–∏—è —Å–∏–º–≤–æ–ª–æ–≤: {span.start_char}-{span.end_char}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            matched_pattern = None
            for org in gov_orgs:
                if org.lower() == span.text.lower():
                    matched_pattern = org
                    break
            
            if matched_pattern:
                print(f"      –°–æ–≤–ø–∞–ª —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º: '{matched_pattern}'")
        
        print(f"\n5Ô∏è‚É£ –°–õ–û–ñ–ù–û–°–¢–¨ –ê–õ–ì–û–†–ò–¢–ú–ê:")
        print(f"   ‚Ä¢ –í—Ä–µ–º—è: O(n) –≥–¥–µ n = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"   ‚Ä¢ –ü–∞–º—è—Ç—å: O(p*m) –≥–¥–µ p = –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, m = –¥–ª–∏–Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞")
        print(f"   ‚Ä¢ –û—á–µ–Ω—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ—á–Ω—ã—Ö —Ñ—Ä–∞–∑")
    
    def demonstrate_attribute_matching(self):
        """–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è"""
        
        print(f"\nüîß –ê–¢–†–ò–ë–£–¢–´ –î–õ–Ø –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–Ø")
        print(f"{'='*50}")
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–Ω—ã–µ matcher'—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
        text = "—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä –∏ –†–û–°–ö–û–ú–ù–ê–î–ó–û–† - –æ–¥–Ω–æ –≤–µ–¥–æ–º—Å—Ç–≤–æ"
        doc = self.nlp(text)
        
        print(f"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: '{text}'")
        print(f"–¢–æ–∫–µ–Ω—ã: {[(token.text, token.lower_, token.orth) for token in doc]}")
        
        # 1. –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É (ORTH)
        matcher_orth = PhraseMatcher(self.nlp.vocab, attr="ORTH")
        pattern_orth = [self.nlp("—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä")]
        matcher_orth.add("EXACT", pattern_orth)
        
        # 2. –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É (LOWER) 
        matcher_lower = PhraseMatcher(self.nlp.vocab, attr="LOWER")
        pattern_lower = [self.nlp("—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä")]
        matcher_lower.add("CASE_INSENSITIVE", pattern_lower)
        
        # 3. –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ –ª–µ–º–º–µ (LEMMA)
        matcher_lemma = PhraseMatcher(self.nlp.vocab, attr="LEMMA")
        pattern_lemma = [self.nlp("—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä")]
        matcher_lemma.add("LEMMA_BASED", pattern_lemma)
        
        print(f"\nüéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –†–ê–ó–ù–´–• –ê–¢–†–ò–ë–£–¢–û–í:")
        
        matches_orth = matcher_orth(doc)
        matches_lower = matcher_lower(doc)
        matches_lemma = matcher_lemma(doc)
        
        print(f"   ORTH (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ): {len(matches_orth)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        for match_id, start, end in matches_orth:
            print(f"      ‚Üí '{doc[start:end].text}'")
        
        print(f"   LOWER (–±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞): {len(matches_lower)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        for match_id, start, end in matches_lower:
            print(f"      ‚Üí '{doc[start:end].text}'")
        
        print(f"   LEMMA (–ø–æ –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º–µ): {len(matches_lemma)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        for match_id, start, end in matches_lemma:
            print(f"      ‚Üí '{doc[start:end].text}'")
    
    def _find_similar_words(self, token, limit=3):
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä—ã"""
        if not token.has_vector:
            return "–ù–µ—Ç –≤–µ–∫—Ç–æ—Ä–∞"
        
        # –ü—Ä–æ—Å—Ç–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–∞ –±–∞–∑–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤
        return "Apple ‚Üí [Google, Microsoft, Facebook]"  # –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    
    def performance_comparison(self):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        
        print(f"\n‚ö° –°–†–ê–í–ù–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
        print(f"{'='*50}")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–æ–ª—å—à–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        large_text = """
        –†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä —Å–æ–æ–±—â–∏–ª –æ –±–ª–æ–∫–∏—Ä–æ–≤–∫–µ —Å–∞–π—Ç–æ–≤. –§–ù–° –†–æ—Å—Å–∏–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞ 
        —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è –ø–æ –Ω–∞–ª–æ–≥–∞–º. –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã–ø—É—Å—Ç–∏–ª–æ –ø—Ä–∏–∫–∞–∑.
        –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –†–§ –æ–±—ä—è–≤–∏–ª–∞ –æ –≤—Å—Ç—Ä–µ—á–µ. Google –∑–∞–ø—É—Å—Ç–∏–ª–∞ –Ω–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å.
        Apple –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ iPhone. Microsoft –æ–±–Ω–æ–≤–∏–ª–∞ Windows. Amazon —Ä–∞—Å—à–∏—Ä—è–µ—Ç –±–∏–∑–Ω–µ—Å.
        """ * 10  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –≤ 10 —Ä–∞–∑
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º phrase matcher
        gov_orgs = [
            "–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "–§–ù–° –†–æ—Å—Å–∏–∏", "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –†–§", "Google", "Apple", "Microsoft", "Amazon"
        ]
        patterns = [self.nlp(org) for org in gov_orgs]
        phrase_matcher = PhraseMatcher(self.nlp.vocab)
        phrase_matcher.add("ORGS", patterns)
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º spaCy NER
        print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ç–µ–∫—Å—Ç–µ –∏–∑ {len(large_text)} —Å–∏–º–≤–æ–ª–æ–≤...")
        
        start = time.time()
        doc = self.nlp(large_text)
        ner_entities = [ent for ent in doc.ents if ent.label_ == "ORG"]
        ner_time = time.time() - start
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º Phrase Matcher
        start = time.time()
        doc = self.nlp(large_text)  # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –Ω—É–∂–Ω–∞ –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ
        phrase_matches = phrase_matcher(doc)
        phrase_time = time.time() - start
        
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   spaCy NER:")
        print(f"      –í—Ä–µ–º—è: {ner_time*1000:.1f} –º—Å")
        print(f"      –ù–∞–π–¥–µ–Ω–æ: {len(ner_entities)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"      –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏: {[ent.text for ent in ner_entities[:5]]}...")
        
        print(f"   Phrase Matcher:")
        print(f"      –í—Ä–µ–º—è: {phrase_time*1000:.1f} –º—Å")
        print(f"      –ù–∞–π–¥–µ–Ω–æ: {len(phrase_matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        
        speedup = ner_time / phrase_time if phrase_time > 0 else 0
        print(f"\nüèÉ Phrase Matcher –±—ã—Å—Ç—Ä–µ–µ –≤ {speedup:.1f} —Ä–∞–∑–∞")

def run_technical_demo():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é"""
    
    print(f"üî¨ –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –î–ï–¢–ê–õ–ò: spaCy NER vs Phrase Matcher")
    print(f"{'='*80}")
    
    demo = TechnicalDetails()
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
    demo.demonstrate_ner_internals()
    demo.demonstrate_phrase_matcher_internals()
    demo.demonstrate_attribute_matching()
    demo.performance_comparison()
    
    print(f"\nüéì –ò–¢–û–ì–û–í–´–ï –í–´–í–û–î–´:")
    print(f"{'='*40}")
    print(f"""
ü§ñ spaCy NER:
   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
   ‚Ä¢ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —Å–µ–º–∞–Ω—Ç–∏–∫—É
   ‚Ä¢ –ú–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å –∏ –Ω–∞—Ö–æ–¥–∏—Ç—å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
   ‚Ä¢ –ú–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —É–º–Ω–µ–µ
   
üìö Phrase Matcher:
   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç –∫–æ–Ω–µ—á–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π (FSM)
   ‚Ä¢ –¢–æ—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
   ‚Ä¢ –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–π –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑
   ‚Ä¢ –ù–µ –º–æ–∂–µ—Ç –æ–±–æ–±—â–∞—Ç—å –Ω–∞ –Ω–æ–≤—ã–µ —Å–ª—É—á–∞–∏
   
üéØ –î–ª—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π:
   ‚Ä¢ Phrase Matcher - –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π (–≤—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å + —Ç–æ—á–Ω–æ—Å—Ç—å)
   ‚Ä¢ spaCy NER - –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π (–≤—ã—Å–æ–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ)
   ‚Ä¢ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!
    """)

if __name__ == "__main__":
    run_technical_demo()
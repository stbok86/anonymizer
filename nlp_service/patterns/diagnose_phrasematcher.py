#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã PhraseMatcher —Å –Ω–µ–ø–æ–ª–Ω—ã–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º –Ω–∞–∑–≤–∞–Ω–∏–π
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'app'))

import spacy
from spacy.matcher import PhraseMatcher

def diagnose_phrase_matcher_issue():
    """–î–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º—É —Å –Ω–µ–ø–æ–ª–Ω—ã–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ–º"""
    
    print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú–´ PHRASE MATCHER")
    print("=" * 60)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    test_text = "–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø –ò –°–í–Ø–ó–ò\n–ü–ï–†–ú–°–ö–û–ì–û –ö–†–ê–Ø"
    
    print(f"üìù –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç:")
    print(f"'{test_text}'")
    print(f"–î–ª–∏–Ω–∞: {len(test_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    print()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å
    nlp = spacy.load("ru_core_news_sm")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
    print("1Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò:")
    print("-" * 30)
    doc = nlp(test_text)
    
    for i, token in enumerate(doc):
        print(f"  {i:2d}: '{token.text}' (pos: {token.pos_}, lemma: '{token.lemma_}')")
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—à —Å–ª–æ–≤–∞—Ä—å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
    print("2Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –°–õ–û–í–ê–†–Ø –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô:")
    print("-" * 40)
    
    try:
        from government_organizations import GOVERNMENT_ORGANIZATIONS
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ä–µ
        target_phrases = []
        for org in GOVERNMENT_ORGANIZATIONS:
            if "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è" in org.lower():
                target_phrases.append(org)
                print(f"  üìã –ù–∞–π–¥–µ–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä–µ: '{org}'")
        
        print(f"\n  üìä –í—Å–µ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑: {len(target_phrases)}")
        print()
        
    except ImportError:
        print("  ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        # Fallback —Ñ—Ä–∞–∑—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        target_phrases = [
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è"
        ]
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º PhraseMatcher
    print("3Ô∏è‚É£ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï PHRASE MATCHER:")
    print("-" * 40)
    
    phrase_matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–∑—ã –ø–æ –æ–¥–Ω–æ–π –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
    for phrase in target_phrases:
        print(f"\n  üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ñ—Ä–∞–∑—É: '{phrase}'")
        
        # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç —Ñ—Ä–∞–∑—ã
        phrase_doc = nlp(phrase)
        print(f"     –¢–æ–∫–µ–Ω–æ–≤ –≤ —Ñ—Ä–∞–∑–µ: {len(phrase_doc)}")
        for j, token in enumerate(phrase_doc):
            print(f"       {j}: '{token.text}'")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ matcher
        matcher_copy = PhraseMatcher(nlp.vocab, attr="LOWER")
        matcher_copy.add("TEST_PHRASE", [phrase_doc])
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        matches = matcher_copy(doc)
        print(f"     –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}")
        
        for match_id, start, end in matches:
            span = doc[start:end]
            print(f"       ‚úÖ –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ: '{span.text}' (—Ç–æ–∫–µ–Ω—ã {start}-{end})")
    
    print()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π PhraseMatcher –∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ
    print("4Ô∏è‚É£ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ï–ê–õ–¨–ù–û–ì–û PHRASE MATCHER:")
    print("-" * 50)
    
    real_matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Ñ—Ä–∞–∑—ã —Å—Ä–∞–∑—É –∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ
    phrase_docs = [nlp(phrase) for phrase in target_phrases]
    real_matcher.add("government_org_phrases", phrase_docs)
    
    print(f"  üìä –î–æ–±–∞–≤–ª–µ–Ω–æ —Ñ—Ä–∞–∑ –≤ matcher: {len(phrase_docs)}")
    
    matches = real_matcher(doc)
    print(f"  üéØ –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}")
    
    for match_id, start, end in matches:
        span = doc[start:end]
        label = nlp.vocab.strings[match_id]
        print(f"    - '{span.text}' (—Ç–æ–∫–µ–Ω—ã {start}-{end}, –º–µ—Ç–∫–∞: '{label}')")
    
    print()
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω –ø—Ä–æ–±–ª–µ–º—ã
    print("5Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –ü–†–ò–ß–ò–ù –ü–†–û–ë–õ–ï–ú–´:")
    print("-" * 35)
    
    print("  üîç –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
    print("    1. –ü–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Ä–∞–∑–±–∏–≤–∞—é—Ç —Ñ—Ä–∞–∑—É")
    print("    2. –†–∞–∑–ª–∏—á–∏—è –≤ —Ä–µ–≥–∏—Å—Ç—Ä–µ (–í–ï–†–•–ù–ò–ô vs –Ω–∏–∂–Ω–∏–π)")
    print("    3. –ù–µ–ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤")
    print("    4. –ü–æ—Ä—è–¥–æ–∫ —Ñ—Ä–∞–∑ –≤ —Å–ª–æ–≤–∞—Ä–µ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç")
    
    # –ü—Ä–æ–≤–µ—Ä–∏–º —Ç–µ–∫—Å—Ç –±–µ–∑ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
    print("\n  üß™ –¢–ï–°–¢ –ë–ï–ó –ü–ï–†–ï–ù–û–°–û–í –°–¢–†–û–ö:")
    clean_text = test_text.replace('\n', ' ').replace('\r', '')
    print(f"    –ò—Å—Ö–æ–¥–Ω—ã–π: '{test_text}'")
    print(f"    –û—á–∏—â–µ–Ω–Ω—ã–π: '{clean_text}'")
    
    clean_doc = nlp(clean_text)
    clean_matches = real_matcher(clean_doc)
    print(f"    –°–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –æ—á–∏—â–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ: {len(clean_matches)}")
    
    for match_id, start, end in clean_matches:
        span = clean_doc[start:end]
        print(f"      ‚úÖ '{span.text}' (—Ç–æ–∫–µ–Ω—ã {start}-{end})")
    
    print()
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–µ—à–µ–Ω–∏—é
    print("6Ô∏è‚É£ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –†–ï–®–ï–ù–ò–Æ:")
    print("-" * 40)
    
    print("  üí° –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:")
    print("    1. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ PhraseMatcher")
    print("    2. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ñ—Ä–∞–∑ —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏")
    print("    3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –±–æ–ª–µ–µ –≥–∏–±–∫–æ–≥–æ Matcher —Å regex")
    print("    4. –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
    print("    5. –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ —Ñ—Ä–∞–∑ –≤ —Å–ª–æ–≤–∞—Ä–µ (–¥–ª–∏–Ω–Ω—ã–µ –ø–µ—Ä–≤—ã–º–∏)")

if __name__ == "__main__":
    diagnose_phrase_matcher_issue()
#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Phrase Matcher –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
"""

import spacy
from spacy.matcher import PhraseMatcher
from typing import List, Dict, Any

class GovernmentOrgPhraseDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —á–µ—Ä–µ–∑ Phrase Matcher"""
    
    def __init__(self):
        self.nlp = spacy.load("ru_core_news_lg")
        self.phrase_matcher = PhraseMatcher(self.nlp.vocab, attr="LOWER")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä–∏ –Ω–∞–∑–≤–∞–Ω–∏–π
        self._load_government_phrases()
    
    def _load_government_phrases(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª–æ–≤–∞—Ä–∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        
        # 1. –ü–æ–ª–Ω—ã–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        official_names = [
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –Ω–∞–ª–æ–≥–æ–≤–∞—è —Å–ª—É–∂–±–∞",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ –ø–æ —Ç—É—Ä–∏–∑–º—É",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –ö–∏—Ä–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –ø–æ –°–≤–µ—Ä–¥–ª–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏"
        ]
        
        # 2. –°–æ–∫—Ä–∞—â–µ–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        abbreviated_names = [
            "–ú–í–î –†–æ—Å—Å–∏–∏", "–ú–í–î –†–§", "–§–ù–° –†–æ—Å—Å–∏–∏", "–§–°–ë –†–æ—Å—Å–∏–∏",
            "–ú–∏–Ω–∑–¥—Ä–∞–≤ –†–æ—Å—Å–∏–∏", "–ú–∏–Ω–æ–±—Ä–Ω–∞—É–∫–∏ –†–æ—Å—Å–∏–∏", "–†–æ—Å—Ç—É—Ä–∏–∑–º",
            "–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "–†–æ—Å—Ä–µ–µ—Å—Ç—Ä", "–†–æ—Å—Å—Ç–∞—Ç",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§", "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞"
        ]
        
        # 3. –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏ –º–µ—Å—Ç–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã (–ø–∞—Ç—Ç–µ—Ä–Ω—ã)
        regional_patterns = [
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –≥–æ—Ä–æ–¥–∞",
            "–ì–æ—Ä–æ–¥—Å–∫–∞—è –¥—É–º–∞",
            "–ó–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å–Ω–æ–µ —Å–æ–±—Ä–∞–Ω–∏–µ",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –∫—Ä–∞—è",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –æ–±–ª–∞—Å—Ç–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∫—Ä–∞—è",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –æ–±–ª–∞—Å—Ç–∏",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –≥–æ—Ä–æ–¥–∞"
        ]
        
        # –°–æ–∑–¥–∞–µ–º phrase patterns
        self._add_phrases("official_full", official_names, 0.95)
        self._add_phrases("abbreviated", abbreviated_names, 0.90)
        self._add_phrases("regional", regional_patterns, 0.85)
    
    def _add_phrases(self, category: str, phrases: List[str], confidence: float):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ñ—Ä–∞–∑—ã –≤ matcher —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫–∏ –≤ spaCy –¥–æ–∫—É–º–µ–Ω—Ç—ã
        phrase_docs = [self.nlp(phrase) for phrase in phrases]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ matcher
        self.phrase_matcher.add(category, phrase_docs)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π —Ñ—Ä–∞–∑—ã
        if not hasattr(self, '_phrase_metadata'):
            self._phrase_metadata = {}
        
        self._phrase_metadata[category] = {
            'confidence': confidence,
            'phrases': phrases
        }
    
    def detect_government_orgs(self, text: str) -> List[Dict[str, Any]]:
        """–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ phrase matching"""
        doc = self.nlp(text)
        matches = self.phrase_matcher(doc)
        
        detections = []
        
        for match_id, start, end in matches:
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            category = self.nlp.vocab.strings[match_id]
            metadata = self._phrase_metadata[category]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–≤–ø–∞–≤—à—É—é —Ñ—Ä–∞–∑—É
            matched_span = doc[start:end]
            matched_text = matched_span.text
            
            detection = {
                'category': 'government_org',
                'original_value': matched_text,
                'confidence': metadata['confidence'],
                'position': {
                    'start': matched_span.start_char,
                    'end': matched_span.end_char
                },
                'method': 'phrase_matcher',
                'phrase_category': category,
                'detection_type': self._classify_detection_type(category)
            }
            
            detections.append(detection)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return self._remove_overlapping_detections(detections)
    
    def _classify_detection_type(self, category: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è"""
        type_mapping = {
            'official_full': '–ü–æ–ª–Ω–æ–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ',
            'abbreviated': '–°–æ–∫—Ä–∞—â–µ–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ',
            'regional': '–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π/–º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–π –æ—Ä–≥–∞–Ω'
        }
        return type_mapping.get(category, '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø')
    
    def _remove_overlapping_detections(self, detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–£–¥–∞–ª—è–µ—Ç –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è –¥–µ—Ç–µ–∫—Ü–∏–∏, –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–µ"""
        if not detections:
            return detections
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ confidence (—É–±—ã–≤–∞–Ω–∏–µ) –∏ –ø–æ –¥–ª–∏–Ω–µ (—É–±—ã–≤–∞–Ω–∏–µ)
        detections.sort(key=lambda x: (-x['confidence'], -(x['position']['end'] - x['position']['start'])))
        
        filtered = []
        
        for detection in detections:
            is_overlapping = False
            
            for existing in filtered:
                if self._is_overlapping(detection['position'], existing['position']):
                    is_overlapping = True
                    break
            
            if not is_overlapping:
                filtered.append(detection)
        
        return filtered
    
    def _is_overlapping(self, pos1: Dict[str, int], pos2: Dict[str, int]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –¥–≤—É—Ö –ø–æ–∑–∏—Ü–∏–π"""
        return not (pos1['end'] <= pos2['start'] or pos2['end'] <= pos1['start'])

# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤ Phrase Matcher
def demonstrate_phrase_matcher():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ Phrase Matcher —É–ª—É—á—à–∞–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é"""
    
    detector = GovernmentOrgPhraseDetector()
    
    test_cases = [
        # –¢–µ—Å—Ç 1: –ü–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≤–µ–ª–æ –æ–ø–µ—Ä–∞—Ü–∏—é.",
        
        # –¢–µ—Å—Ç 2: –°–æ–∫—Ä–∞—â–µ–Ω–∏—è
        "–ú–í–î –†–æ—Å—Å–∏–∏ –∏ –§–ù–° –†–æ—Å—Å–∏–∏ –ø–æ–¥–ø–∏—Å–∞–ª–∏ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ.",
        
        # –¢–µ—Å—Ç 3: –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
        "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –≥–æ—Ä–æ–¥–∞ –ü–µ—Ä–º–∏ –æ–±—ä—è–≤–∏–ª–∞ –∫–æ–Ω–∫—É—Ä—Å.",
        
        # –¢–µ—Å—Ç 4: –°–∫–ª–æ–Ω–µ–Ω–∏—è –∏ –≤–∞—Ä–∏–∞—Ü–∏–∏
        "–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ –†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä–∞ –ø—Ä–æ–≤–µ–ª–∏ –ø—Ä–æ–≤–µ—Ä–∫—É.",
        
        # –¢–µ—Å—Ç 5: –°–º–µ—à–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§ –∏ –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Å—É–∂–¥–∞—é—Ç —Ä–µ—Ñ–æ—Ä–º—ã."
    ]
    
    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø PHRASE MATCHER –î–õ–Ø –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô")
    print("=" * 80)
    
    total_detected = 0
    
    for i, text in enumerate(test_cases, 1):
        print(f"\nüìù –¢–µ—Å—Ç {i}: {text}")
        detections = detector.detect_government_orgs(text)
        
        if detections:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(detections)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π:")
            for det in detections:
                print(f"   ‚Ä¢ '{det['original_value']}' (confidence: {det['confidence']})")
                print(f"     –¢–∏–ø: {det['detection_type']}")
                print(f"     –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {det['phrase_category']}")
            total_detected += len(detections)
        else:
            print("‚ùå –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    print(f"\nüìä –ò–¢–û–ì–û: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_detected} –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å regex –ø–æ–¥—Ö–æ–¥–æ–º
    print(f"\nüîÑ –°–†–ê–í–ù–ï–ù–ò–ï –° REGEX:")
    print(f"‚úÖ Phrase Matcher –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:")
    print(f"   ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ –∏ —Å–∫–ª–æ–Ω–µ–Ω–∏–π")
    print(f"   ‚Ä¢ –ë—ã—Å—Ç—Ä–µ–µ —á–µ–º —Å–ª–æ–∂–Ω—ã–µ regex (O(n) vs O(n*m))")
    print(f"   ‚Ä¢ –õ–µ–≥–∫–æ –¥–æ–±–∞–≤–ª—è—Ç—å –Ω–æ–≤—ã–µ —Ñ—Ä–∞–∑—ã –±–µ–∑ regex –∑–Ω–∞–Ω–∏–π")
    print(f"   ‚Ä¢ –ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤")
    print(f"   ‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã —Ç–æ–∫–µ–Ω–æ–≤ (LOWER, LEMMA)")

if __name__ == "__main__":
    demonstrate_phrase_matcher()
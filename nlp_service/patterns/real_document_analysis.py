#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ spaCy NER vs Phrase Matcher
"""

import spacy
from spacy.matcher import PhraseMatcher
from docx import Document
import os
import time
from typing import List, Dict, Any

class RealDocumentAnalysis:
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç–æ–¥–æ–≤"""
    
    def __init__(self):
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å...")
        self.nlp = spacy.load("ru_core_news_lg")
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Phrase Matcher —Å –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏
        self.phrase_matcher = PhraseMatcher(self.nlp.vocab, attr="LOWER")
        self._setup_government_phrases()
        
        self.doc_path = r"C:\Projects\Anonymizer\unified_document_service\test_docs\test_01_1_4_SD.docx"
    
    def _setup_government_phrases(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º Phrase Matcher —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –≥–æ—Å–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏"""
        
        government_orgs = [
            # –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ú–í–î –†–æ—Å—Å–∏–∏", "–ú–í–î –†–§",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –Ω–∞–ª–æ–≥–æ–≤–∞—è —Å–ª—É–∂–±–∞", "–§–ù–° –†–æ—Å—Å–∏–∏",
            "–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä", "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –ø–æ –Ω–∞–¥–∑–æ—Ä—É –≤ —Å—Ñ–µ—Ä–µ —Å–≤—è–∑–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ú–∏–Ω–∑–¥—Ä–∞–≤ –†–æ—Å—Å–∏–∏", "–ú–∏–Ω–∑–¥—Ä–∞–≤ –†–§",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ú–∏–Ω–æ–±—Ä–Ω–∞—É–∫–∏ –†–æ—Å—Å–∏–∏", "–ú–∏–Ω–æ–±—Ä–Ω–∞—É–∫–∏ –†–§",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "–§–°–ë –†–æ—Å—Å–∏–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —á—Ä–µ–∑–≤—ã—á–∞–π–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π", "–ú–ß–° –†–æ—Å—Å–∏–∏",
            "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞", "–§–ê–° –†–æ—Å—Å–∏–∏",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏", "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            
            # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–∞ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –ö–∏—Ä–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –ø–æ –°–≤–µ—Ä–¥–ª–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –ú–æ—Å–∫–≤—ã",
            "–ö–æ–º–∏—Ç–µ—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞",
            
            # –ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –≥–æ—Ä–æ–¥–∞ –ü–µ—Ä–º–∏",
            "–ì–æ—Ä–æ–¥—Å–∫–∞—è –¥—É–º–∞ –≥–æ—Ä–æ–¥–∞ –ü–µ—Ä–º–∏",
            "–ú—ç—Ä–∏—è –≥–æ—Ä–æ–¥–∞ –ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–∞",
            "–ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –õ–µ–Ω–∏–Ω—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞",
            
            # –°—É–¥–µ–±–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–í–µ—Ä—Ö–æ–≤–Ω—ã–π —Å—É–¥ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ö–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–π —Å—É–¥ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ê—Ä–±–∏—Ç—Ä–∞–∂–Ω—ã–π —Å—É–¥ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–ü–µ—Ä–º—Å–∫–∏–π —Ä–∞–π–æ–Ω–Ω—ã–π —Å—É–¥",
            
            # –°–∏–ª–æ–≤—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            "–ü—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–°–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–º–∏—Ç–µ—Ç –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–π —Å–ª—É–∂–±—ã –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è –Ω–∞–∫–∞–∑–∞–Ω–∏–π"
        ]
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns = [self.nlp(org) for org in government_orgs]
        self.phrase_matcher.add("GOVERNMENT_ORG", patterns)
        
        print(f"‚úÖ Phrase Matcher –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å {len(government_orgs)} –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏")
    
    def extract_document_text(self) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        if not os.path.exists(self.doc_path):
            raise FileNotFoundError(f"–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.doc_path}")
        
        print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç: {os.path.basename(self.doc_path)}")
        
        try:
            doc = Document(self.doc_path)
            full_text = []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü
            for table in doc.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        full_text.append(" | ".join(row_text))
            
            document_text = "\n".join(full_text)
            
            print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω:")
            print(f"   –û–±—â–∞—è –¥–ª–∏–Ω–∞: {len(document_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –ü–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {len(doc.paragraphs)}")
            print(f"   –¢–∞–±–ª–∏—Ü: {len(doc.tables)}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            print(f"\nüìñ –ù–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
            print(f"   {document_text[:500]}...")
            
            return document_text
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
    
    def analyze_with_spacy_ner(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é spaCy NER"""
        
        print(f"\nü§ñ –ê–ù–ê–õ–ò–ó –° –ü–û–ú–û–©–¨–Æ SPACY NER")
        print(f"{'='*60}")
        
        start_time = time.time()
        doc = self.nlp(text)
        processing_time = time.time() - start_time
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏
        all_entities = list(doc.ents)
        organizations = [ent for ent in doc.ents if ent.label_ == "ORG"]
        persons = [ent for ent in doc.ents if ent.label_ in ["PER", "PERSON"]]
        locations = [ent for ent in doc.ents if ent.label_ in ["LOC", "GPE"]]
        
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time*1000:.1f} –º—Å")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π:")
        print(f"   –í—Å–µ–≥–æ: {len(all_entities)}")
        print(f"   –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ (ORG): {len(organizations)}")
        print(f"   –ü–µ—Ä—Å–æ–Ω—ã (PER): {len(persons)}")
        print(f"   –õ–æ–∫–∞—Ü–∏–∏ (LOC): {len(locations)}")
        
        print(f"\nüèõÔ∏è –ù–ê–ô–î–ï–ù–ù–´–ï –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò:")
        org_results = []
        for i, org in enumerate(organizations, 1):
            confidence = self._estimate_ner_confidence(org, text)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π
            is_government = self._is_likely_government(org.text)
            gov_marker = "üèõÔ∏è" if is_government else "üè¢"
            
            print(f"   {i:2d}. {gov_marker} '{org.text}' (confidence: {confidence:.2f})")
            print(f"       –ü–æ–∑–∏—Ü–∏—è: {org.start_char}-{org.end_char}")
            print(f"       –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{text[max(0, org.start_char-30):org.start_char]}[{org.text}]{text[org.end_char:org.end_char+30]}...")
            
            org_results.append({
                'text': org.text,
                'start': org.start_char,
                'end': org.end_char,
                'confidence': confidence,
                'is_government': is_government,
                'method': 'spacy_ner'
            })
        
        return {
            'results': org_results,
            'processing_time': processing_time,
            'total_entities': len(all_entities),
            'organizations': len(organizations),
            'government_orgs': len([r for r in org_results if r['is_government']])
        }
    
    def analyze_with_phrase_matcher(self, text: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç —Å –ø–æ–º–æ—â—å—é Phrase Matcher"""
        
        print(f"\nüìö –ê–ù–ê–õ–ò–ó –° –ü–û–ú–û–©–¨–Æ PHRASE MATCHER")
        print(f"{'='*60}")
        
        start_time = time.time()
        doc = self.nlp(text)
        matches = self.phrase_matcher(doc)
        processing_time = time.time() - start_time
        
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time*1000:.1f} –º—Å")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}")
        
        phrase_results = []
        
        print(f"\nüèõÔ∏è –ù–ê–ô–î–ï–ù–ù–´–ï –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–ï –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò:")
        for i, (match_id, start, end) in enumerate(matches, 1):
            span = doc[start:end]
            
            print(f"   {i:2d}. üèõÔ∏è '{span.text}' (confidence: 0.95)")
            print(f"       –ü–æ–∑–∏—Ü–∏—è: {span.start_char}-{span.end_char}")
            print(f"       –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{text[max(0, span.start_char-30):span.start_char]}[{span.text}]{text[span.end_char:span.end_char+30]}...")
            
            phrase_results.append({
                'text': span.text,
                'start': span.start_char,
                'end': span.end_char,
                'confidence': 0.95,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                'is_government': True,  # –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ - –≥–æ—Å–æ—Ä–≥–∞–Ω—ã
                'method': 'phrase_matcher'
            })
        
        return {
            'results': phrase_results,
            'processing_time': processing_time,
            'matches_found': len(matches)
        }
    
    def compare_results(self, ner_results: Dict, phrase_results: Dict, original_text: str):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤"""
        
        print(f"\nüÜö –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print(f"{'='*80}")
        
        ner_orgs = {r['text'].lower(): r for r in ner_results['results']}
        phrase_orgs = {r['text'].lower(): r for r in phrase_results['results']}
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∏ —Ä–∞–∑–ª–∏—á–∏—è
        ner_only = set(ner_orgs.keys()) - set(phrase_orgs.keys())
        phrase_only = set(phrase_orgs.keys()) - set(ner_orgs.keys())
        both_found = set(ner_orgs.keys()) & set(phrase_orgs.keys())
        
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   spaCy NER –Ω–∞—à–µ–ª: {len(ner_orgs)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   Phrase Matcher –Ω–∞—à–µ–ª: {len(phrase_orgs)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   –ù–∞–π–¥–µ–Ω—ã –æ–±–æ–∏–º–∏: {len(both_found)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   –¢–æ–ª—å–∫–æ NER: {len(ner_only)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   –¢–æ–ª—å–∫–æ Phrase Matcher: {len(phrase_only)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        
        if both_found:
            print(f"\nüéØ –ù–ê–ô–î–ï–ù–´ –û–ë–û–ò–ú–ò –ú–ï–¢–û–î–ê–ú–ò:")
            for org_name in sorted(both_found):
                print(f"   ‚úÖ '{ner_orgs[org_name]['text']}'")
        
        if ner_only:
            print(f"\nü§ñ –¢–û–õ–¨–ö–û SPACY NER –ù–ê–®–ï–õ:")
            for org_name in sorted(ner_only):
                org = ner_orgs[org_name]
                gov_status = "üèõÔ∏è (–≥–æ—Å–æ—Ä–≥–∞–Ω)" if org['is_government'] else "üè¢ (–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è)"
                print(f"   ‚Ä¢ '{org['text']}' {gov_status} (conf: {org['confidence']:.2f})")
        
        if phrase_only:
            print(f"\nüìö –¢–û–õ–¨–ö–û PHRASE MATCHER –ù–ê–®–ï–õ:")
            for org_name in sorted(phrase_only):
                org = phrase_orgs[org_name]
                print(f"   ‚Ä¢ '{org['text']}' üèõÔ∏è (conf: {org['confidence']:.2f})")
        
        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        print(f"\n‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:")
        ner_time = ner_results['processing_time'] * 1000
        phrase_time = phrase_results['processing_time'] * 1000
        speedup = ner_time / phrase_time if phrase_time > 0 else 0
        
        print(f"   spaCy NER: {ner_time:.1f} –º—Å")
        print(f"   Phrase Matcher: {phrase_time:.1f} –º—Å")
        print(f"   Phrase Matcher –±—ã—Å—Ç—Ä–µ–µ –≤ {speedup:.1f} —Ä–∞–∑")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤
        ner_gov_orgs = len([r for r in ner_results['results'] if r['is_government']])
        phrase_gov_orgs = len(phrase_results['results'])  # –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ - –≥–æ—Å–æ—Ä–≥–∞–Ω—ã
        
        print(f"\nüèõÔ∏è –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–ï –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò:")
        print(f"   spaCy NER (–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ): {ner_gov_orgs}")
        print(f"   Phrase Matcher (–≤—Å–µ): {phrase_gov_orgs}")
        
        return {
            'ner_total': len(ner_orgs),
            'phrase_total': len(phrase_orgs),
            'overlap': len(both_found),
            'ner_only': len(ner_only),
            'phrase_only': len(phrase_only),
            'ner_time': ner_time,
            'phrase_time': phrase_time
        }
    
    def _estimate_ner_confidence(self, ent, text: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è NER —Å—É—â–Ω–æ—Å—Ç–∏"""
        base_confidence = 0.75
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É –Ω–∞–∑–≤–∞–Ω–∏—è
        length_bonus = min(0.15, len(ent.text.split()) * 0.03)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
        caps_bonus = 0.05 if ent.text[0].isupper() else 0
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç (–µ—Å–ª–∏ —Ä—è–¥–æ–º —Å–ª–æ–≤–∞-–º–∞—Ä–∫–µ—Ä—ã)
        context_start = max(0, ent.start_char - 50)
        context_end = min(len(text), ent.end_char + 50)
        context = text[context_start:context_end].lower()
        
        context_bonus = 0
        gov_markers = ['–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Å–ª—É–∂–±–∞', '–∫–æ–º–∏—Ç–µ—Ç', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è']
        for marker in gov_markers:
            if marker in context:
                context_bonus = 0.08
                break
        
        return min(0.90, base_confidence + length_bonus + caps_bonus + context_bonus)
    
    def _is_likely_government(self, org_text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π"""
        org_lower = org_text.lower()
        
        gov_keywords = [
            '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Å–ª—É–∂–±–∞', '–∫–æ–º–∏—Ç–µ—Ç',
            '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–¥—É–º–∞', '–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞', '—Å—É–¥',
            '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è', '–º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–∞—è', '—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è',
            '–º–≤–¥', '—Ñ–Ω—Å', '—Ñ—Å–±', '–º—á—Å', '—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä', '—Ä–æ—Å—Ç—É—Ä–∏–∑–º', '—Ä–æ—Å—Å—Ç–∞—Ç'
        ]
        
        return any(keyword in org_lower for keyword in gov_keywords)

def run_real_document_analysis():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    print(f"üî¨ –ê–ù–ê–õ–ò–ó –†–ï–ê–õ–¨–ù–û–ì–û –î–û–ö–£–ú–ï–ù–¢–ê: SPACY NER vs PHRASE MATCHER")
    print(f"{'='*80}")
    
    try:
        analyzer = RealDocumentAnalysis()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        document_text = analyzer.extract_document_text()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–≤—É–º—è –º–µ—Ç–æ–¥–∞–º–∏
        ner_results = analyzer.analyze_with_spacy_ner(document_text)
        phrase_results = analyzer.analyze_with_phrase_matcher(document_text)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        comparison = analyzer.compare_results(ner_results, phrase_results, document_text)
        
        # –í—ã–≤–æ–¥—ã
        print(f"\nüéØ –í–´–í–û–î–´ –î–õ–Ø –î–ê–ù–ù–û–ì–û –î–û–ö–£–ú–ï–ù–¢–ê:")
        print(f"{'='*50}")
        
        if comparison['overlap'] > 0:
            print(f"‚úÖ –û–±–∞ –º–µ—Ç–æ–¥–∞ –Ω–∞—à–ª–∏ {comparison['overlap']} –æ–±—â–∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        
        if comparison['ner_only'] > 0:
            print(f"ü§ñ spaCy NER –Ω–∞—à–µ–ª {comparison['ner_only']} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
            print(f"   (–º–æ–∂–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –≥–æ—Å–æ—Ä–≥–∞–Ω—ã)")
        
        if comparison['phrase_only'] > 0:
            print(f"üìö Phrase Matcher –Ω–∞—à–µ–ª {comparison['phrase_only']} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π, –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö NER")
            print(f"   (—Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –≥–æ—Å–æ—Ä–≥–∞–Ω–∞–º–∏)")
        
        speedup = comparison['ner_time'] / comparison['phrase_time']
        print(f"‚ö° Phrase Matcher –±—ã—Å—Ç—Ä–µ–µ –≤ {speedup:.1f} —Ä–∞–∑")
        
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        print(f"   1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ Phrase Matcher –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤")
        print(f"   2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ spaCy NER –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π") 
        print(f"   3. –ö–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ –æ–±–∞ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è")
        print(f"   4. –§–∏–ª—å—Ç—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã NER –ø–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")

if __name__ == "__main__":
    run_real_document_analysis()
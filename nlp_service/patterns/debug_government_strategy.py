#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å –≥–∏–±—Ä–∏–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'app'))

from nlp_adapter import NLPAdapter

def debug_government_org_detection():
    """–û—Ç–ª–∞–¥–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
    
    print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –î–ï–¢–ï–ö–¶–ò–ò –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô")
    print("=" * 70)
    
    test_text = "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è"
    print(f"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: '{test_text}'")
    print()
    
    try:
        adapter = NLPAdapter()
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        print("1Ô∏è‚É£ –ü–†–û–í–ï–†–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò:")
        print("-" * 30)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è government_org
        enabled_methods = adapter.config.get_enabled_methods_for_category('government_org')
        print(f"   –í–∫–ª—é—á—ë–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã: {enabled_methods}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        strategy = adapter.config.get_detection_strategy_name('government_org')
        print(f"   –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ confidence –¥–ª—è –º–µ—Ç–æ–¥–æ–≤
        for method in enabled_methods:
            min_conf = adapter.config.get_min_confidence_for_method('government_org', method)
            print(f"   {method}: min_confidence = {min_conf}")
        
        print()
        
        # 2. –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –º–µ—Ç–æ–¥ –æ—Ç–¥–µ–ª—å–Ω–æ
        print("2Ô∏è‚É£ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–¢–î–ï–õ–¨–ù–´–• –ú–ï–¢–û–î–û–í:")
        print("-" * 40)
        
        doc = adapter.nlp(test_text)
        
        # –¢–µ—Å—Ç phrase_matcher
        print(f"üìö Phrase Matcher:")
        phrase_results = adapter._extract_context_matches_for_category(doc, 'government_org')
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(phrase_results)}")
        for result in phrase_results:
            print(f"   - '{result['original_value']}' (conf: {result['confidence']})")
        print()
        
        # –¢–µ—Å—Ç spacy_ner 
        print(f"ü§ñ spaCy NER:")
        ner_results = adapter._extract_spacy_entities_for_category(doc, 'government_org')
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(ner_results)}")
        for result in ner_results:
            print(f"   - '{result['original_value']}' (conf: {result['confidence']})")
        print()
        
        # –¢–µ—Å—Ç regex
        print(f"üî§ Regex:")
        regex_results = adapter._extract_regex_patterns_for_category(test_text, 'government_org')
        print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(regex_results)}")
        for result in regex_results:
            print(f"   - '{result['original_value']}' (conf: {result['confidence']})")
        print()
        
        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º phrase matcher –¥–µ—Ç–∞–ª—å–Ω–æ
        print("3Ô∏è‚É£ –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê PHRASE MATCHER:")
        print("-" * 45)
        
        if adapter.phrase_matcher:
            matches = adapter.phrase_matcher(doc)
            print(f"   –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ phrase_matcher: {len(matches)}")
            
            for match_id, start, end in matches:
                span = doc[start:end]
                label = adapter.nlp.vocab.strings[match_id]
                category = adapter._get_phrase_category(label)
                print(f"   - '{span.text}' -> label: '{label}' -> category: '{category}'")
        else:
            print("   ‚ùå phrase_matcher –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        print()
        
        # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º spaCy NER –¥–µ—Ç–∞–ª—å–Ω–æ  
        print("4Ô∏è‚É£ –î–ï–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê SPACY NER:")
        print("-" * 40)
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ spaCy: {len(doc.ents)}")
        
        category_map = adapter.config.get_spacy_entity_mapping()
        print(f"   –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {category_map}")
        
        government_labels = [label for label, cat in category_map.items() if cat == 'government_org']
        print(f"   –ú–µ—Ç–∫–∏ –¥–ª—è government_org: {government_labels}")
        
        for ent in doc.ents:
            print(f"   - '{ent.text}' -> label: '{ent.label_}' -> mapped: {category_map.get(ent.label_, 'unknown')}")
        
        print()
        
        # 5. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è
        print("5Ô∏è‚É£ –ü–†–û–í–ï–†–ö–ê –°–õ–û–í–ê–†–Ø –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô:")
        print("-" * 55)
        
        gov_orgs = adapter._load_government_organizations()
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: {len(gov_orgs)}")
        print("   –ü–µ—Ä–≤—ã–µ 10:")
        for i, org in enumerate(gov_orgs[:10], 1):
            print(f"     {i}. {org}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –Ω–∞—à —Ç–µ—Å—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ
        target = "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è"
        if target in gov_orgs:
            print(f"   ‚úÖ –¢–µ—Å—Ç–æ–≤–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ")
        else:
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
            similar = [org for org in gov_orgs if "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è" in org.lower()]
            print(f"   ‚ùå –¢–µ—Å—Ç–æ–≤–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –ù–ï –Ω–∞–π–¥–µ–Ω–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ")
            print(f"   –ü–æ—Ö–æ–∂–∏–µ: {similar}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    debug_government_org_detection()
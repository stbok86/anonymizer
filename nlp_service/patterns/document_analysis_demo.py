#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–∏–π spaCy NER vs Phrase Matcher
–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ spaCy –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
"""

from docx import Document
import os
import re
import time
from typing import List, Dict, Any, Set

class DocumentAnalysisDemo:
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∫–æ–Ω—Ü–µ–ø—Ü–∏–π NER vs Phrase Matcher –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
    
    def __init__(self):
        self.doc_path = r"C:\Projects\Anonymizer\unified_document_service\test_docs\test_01_1_4_SD.docx"
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è Phrase Matcher (—Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
        self.government_phrases = {
            # –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–º–≤–¥ —Ä–æ—Å—Å–∏–∏", "–º–≤–¥ —Ä—Ñ",
            "—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –Ω–∞–ª–æ–≥–æ–≤–∞—è —Å–ª—É–∂–±–∞", "—Ñ–Ω—Å —Ä–æ—Å—Å–∏–∏",
            "—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä",
            "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–º–∏–Ω–∑–¥—Ä–∞–≤ —Ä–æ—Å—Å–∏–∏", "–º–∏–Ω–∑–¥—Ä–∞–≤ —Ä—Ñ",
            "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–º–∏–Ω–æ–±—Ä–Ω–∞—É–∫–∏ —Ä–æ—Å—Å–∏–∏", "–º–∏–Ω–æ–±—Ä–Ω–∞—É–∫–∏ —Ä—Ñ",
            "—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏", "—Ñ—Å–± —Ä–æ—Å—Å–∏–∏",
            "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —á—Ä–µ–∑–≤—ã—á–∞–π–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏–π", "–º—á—Å —Ä–æ—Å—Å–∏–∏",
            "—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è –∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞", "—Ñ–∞—Å —Ä–æ—Å—Å–∏–∏",
            "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ —Ä—Ñ",
            "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏",
            
            # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ø–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–∞ –ø–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –∫–∏—Ä–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
            "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –ø–æ —Å–≤–µ—Ä–¥–ª–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
            "–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –º–æ—Å–∫–≤—ã",
            "–∫–æ–º–∏—Ç–µ—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é —Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥–∞",
            
            # –ú—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω—ã
            "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –≥–æ—Ä–æ–¥–∞ –ø–µ—Ä–º–∏",
            "–≥–æ—Ä–æ–¥—Å–∫–∞—è –¥—É–º–∞ –≥–æ—Ä–æ–¥–∞ –ø–µ—Ä–º–∏",
            "–º—ç—Ä–∏—è –≥–æ—Ä–æ–¥–∞ –µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥–∞",
            "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è –ª–µ–Ω–∏–Ω—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞",
            
            # –°—É–¥–µ–±–Ω—ã–µ –∏ —Å–∏–ª–æ–≤—ã–µ
            "–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞ –ø–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–º–∏—Ç–µ—Ç —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏"
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ NER (regex-–ø–æ–¥—Ö–æ–¥)
        self.ner_patterns = [
            # –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–∞
            r'\b–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ\s+[\w\s]+(?:—Ä–æ—Å—Å–∏–π—Å–∫–æ–π\s+—Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏|—Ä—Ñ|—Ä–æ—Å—Å–∏–∏|–∫—Ä–∞—è|–æ–±–ª–∞—Å—Ç–∏|—Ä–µ—Å–ø—É–±–ª–∏–∫–∏)\b',
            # –î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—ã –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
            r'\b(?:–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç|—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ|–∫–æ–º–∏—Ç–µ—Ç)\s+[\w\s]+(?:–∫—Ä–∞—è|–æ–±–ª–∞—Å—Ç–∏|–≥–æ—Ä–æ–¥–∞|—Ä–∞–π–æ–Ω–∞|—Ä—Ñ|—Ä–æ—Å—Å–∏–∏)?\b',
            # –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ —Å–ª—É–∂–±—ã
            r'\b—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è\s+(?:—Å–ª—É–∂–±–∞|–∞–Ω—Ç–∏–º–æ–Ω–æ–ø–æ–ª—å–Ω–∞—è\s+—Å–ª—É–∂–±–∞|–Ω–∞–ª–æ–≥–æ–≤–∞—è\s+—Å–ª—É–∂–±–∞)\s*[\w\s]*\b',
            # –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏
            r'\b–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è\s+(?:–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞|–≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä–∞|–≥–æ—Ä–æ–¥–∞|—Ä–∞–π–æ–Ω–∞)\s*[\w\s]*\b',
            # –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ
            r'\b–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ\s+(?:—Ä–æ—Å—Å–∏–π—Å–∫–æ–π\s+—Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏|—Ä—Ñ|—Ä–æ—Å—Å–∏–∏|[\w\s]+–∫—Ä–∞—è|[\w\s]+–æ–±–ª–∞—Å—Ç–∏)\b',
            # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
            r'\b(?:–º–≤–¥|—Ñ–Ω—Å|—Ñ—Å–±|–º—á—Å|—Ñ–∞—Å)\s+(?:—Ä–æ—Å—Å–∏–∏|—Ä—Ñ)\b',
            r'\b(?:—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä|—Ä–æ—Å—Å—Ç–∞—Ç|—Ä–æ—Å—Ç—É—Ä–∏–∑–º|—Ä–æ—Å—Ä–µ–µ—Å—Ç—Ä)\b',
            # –°—É–¥—ã –∏ –ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞
            r'\b(?:–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞|—Å—É–¥)\s+[\w\s]+(?:–∫—Ä–∞—è|–æ–±–ª–∞—Å—Ç–∏|—Ä–∞–π–æ–Ω–∞|—Ä—Ñ)\b',
            # –î—É–º—ã –∏ —Å–æ–≤–µ—Ç—ã
            r'\b(?:–¥—É–º–∞|—Å–æ–≤–µ—Ç)\s+[\w\s]*(?:–≥–æ—Ä–æ–¥–∞|—Ä–∞–π–æ–Ω–∞|–∫—Ä–∞—è|–æ–±–ª–∞—Å—Ç–∏)\b'
        ]
    
    def extract_document_text(self) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        if not os.path.exists(self.doc_path):
            raise FileNotFoundError(f"–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.doc_path}")
        
        print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç: {os.path.basename(self.doc_path)}")
        
        try:
            doc = Document(self.doc_path)
            full_text = []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            paragraph_count = 0
            for paragraph in doc.paragraphs:
                if paragraph.text.strip():
                    full_text.append(paragraph.text.strip())
                    paragraph_count += 1
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Ç–∞–±–ª–∏—Ü
            table_count = 0
            for table in doc.tables:
                for row in table.rows:
                    row_text = []
                    for cell in row.cells:
                        if cell.text.strip():
                            row_text.append(cell.text.strip())
                    if row_text:
                        full_text.append(" | ".join(row_text))
                table_count += 1
            
            document_text = "\n".join(full_text)
            
            print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω:")
            print(f"   –û–±—â–∞—è –¥–ª–∏–Ω–∞: {len(document_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   –ü–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ —Å —Ç–µ–∫—Å—Ç–æ–º: {paragraph_count}")
            print(f"   –¢–∞–±–ª–∏—Ü: {table_count}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            print(f"\nüìñ –ù–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞:")
            print(f"   {document_text[:300]}...")
            
            return document_text
            
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
    
    def simulate_phrase_matcher(self, text: str) -> List[Dict[str, Any]]:
        """–°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É Phrase Matcher - —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ—Ä–∞–∑"""
        
        print(f"\nüìö PHRASE MATCHER –ü–û–î–•–û–î")
        print(f"{'='*60}")
        print(f"üîç –ò—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å {len(self.government_phrases)} –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏...")
        
        start_time = time.time()
        
        text_lower = text.lower()
        results = []
        
        # –ü–æ–∏—Å–∫ –∫–∞–∂–¥–æ–π –∏–∑–≤–µ—Å—Ç–Ω–æ–π —Ñ—Ä–∞–∑—ã
        for phrase in self.government_phrases:
            # –ò—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (—Å —É—á–µ—Ç–æ–º –≥—Ä–∞–Ω–∏—Ü —Å–ª–æ–≤)
            pattern = r'\b' + re.escape(phrase) + r'\b'
            matches = re.finditer(pattern, text_lower, re.IGNORECASE)
            
            for match in matches:
                # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (—Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ä–µ–≥–∏—Å—Ç—Ä–æ–º)
                original_text = text[match.start():match.end()]
                
                results.append({
                    'text': original_text,
                    'start': match.start(),
                    'end': match.end(),
                    'confidence': 0.95,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                    'method': 'phrase_matcher',
                    'matched_phrase': phrase
                })
        
        processing_time = time.time() - start_time
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–µ—Å–ª–∏ –æ–¥–Ω–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑)
        unique_results = []
        seen_positions = set()
        
        for result in results:
            pos_key = (result['start'], result['end'])
            if pos_key not in seen_positions:
                unique_results.append(result)
                seen_positions.add(pos_key)
        
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time*1000:.1f} –º—Å")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(unique_results)}")
        
        print(f"\nüèõÔ∏è –ù–ê–ô–î–ï–ù–ù–´–ï –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–ï –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò:")
        for i, result in enumerate(unique_results, 1):
            print(f"   {i:2d}. üèõÔ∏è '{result['text']}' (confidence: {result['confidence']})")
            print(f"       –ü–æ–∑–∏—Ü–∏—è: {result['start']}-{result['end']}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_start = max(0, result['start'] - 40)
            context_end = min(len(text), result['end'] + 40)
            context = text[context_start:context_end].replace('\n', ' ')
            print(f"       –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{context}...")
        
        print(f"\nüéØ –û–°–û–ë–ï–ù–ù–û–°–¢–ò PHRASE MATCHER:")
        print(f"   ‚úÖ –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (–Ω–µ—Ç –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π)")
        print(f"   ‚úÖ –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è —Ä–∞–±–æ—Ç–∞")
        print(f"   ‚úÖ –ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–∞–∂–µ –≤ —Å–ª–æ–∂–Ω–æ–º —Ç–µ–∫—Å—Ç–µ")
        print(f"   ‚ùå –ù–ï –Ω–∞–π–¥–µ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏")
        print(f"   ‚ùå –ù–ï –Ω–∞–π–¥–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏–π")
        
        return unique_results
    
    def simulate_spacy_ner(self, text: str) -> List[Dict[str, Any]]:
        """–°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É spaCy NER - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        
        print(f"\nü§ñ SPACY NER –ü–û–î–•–û–î (–°–ò–ú–£–õ–Ø–¶–ò–Ø)")
        print(f"{'='*60}")
        print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ —ç–≤—Ä–∏—Å—Ç–∏–∫...")
        
        start_time = time.time()
        
        all_organizations = []
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–∞–∂–¥—ã–π NER –ø–∞—Ç—Ç–µ—Ä–Ω
        for i, pattern in enumerate(self.ner_patterns, 1):
            matches = re.finditer(pattern, text, re.IGNORECASE)
            
            for match in matches:
                org_text = match.group().strip()
                confidence = self._calculate_ner_confidence(org_text, text, match)
                
                all_organizations.append({
                    'text': org_text,
                    'start': match.start(),
                    'end': match.end(),
                    'confidence': confidence,
                    'method': 'spacy_ner_simulation',
                    'pattern_id': i,
                    'is_government': True  # –í—Å–µ –Ω–∞—à–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤
                })
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        potential_orgs = self._find_potential_organizations(text)
        all_organizations.extend(potential_orgs)
        
        processing_time = time.time() - start_time
        
        # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –ª—É—á—à–∏–µ –ø–æ confidence
        filtered_results = self._remove_overlapping_detections(all_organizations)
        
        print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time*1000:.1f} –º—Å")
        print(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(all_organizations)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        print(f"üìä –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_results)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        filtered_results.sort(key=lambda x: x['confidence'], reverse=True)
        
        print(f"\nüèõÔ∏è –ù–ê–ô–î–ï–ù–ù–´–ï –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ò:")
        government_count = 0
        for i, result in enumerate(filtered_results, 1):
            is_gov = result.get('is_government', False)
            gov_marker = "üèõÔ∏è" if is_gov else "üè¢"
            
            print(f"   {i:2d}. {gov_marker} '{result['text']}' (confidence: {result['confidence']:.2f})")
            print(f"       –ü–æ–∑–∏—Ü–∏—è: {result['start']}-{result['end']}")
            
            if is_gov:
                government_count += 1
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_start = max(0, result['start'] - 40)
            context_end = min(len(text), result['end'] + 40)
            context = text[context_start:context_end].replace('\n', ' ')
            print(f"       –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{context}...")
        
        print(f"\nüìä –ò–∑ –Ω–∏—Ö –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö: {government_count}")
        
        print(f"\nüéØ –û–°–û–ë–ï–ù–ù–û–°–¢–ò SPACY NER:")
        print(f"   ‚úÖ –ú–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –ù–ï–ò–ó–í–ï–°–¢–ù–´–ï –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏")
        print(f"   ‚úÖ –ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤–∞—Ä–∏–∞—Ü–∏–∏")
        print(f"   ‚úÖ –û–±–æ–±—â–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞ –Ω–æ–≤—ã–µ —Å–ª—É—á–∞–∏")
        print(f"   ‚ùå –ú–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è")
        print(f"   ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–µ–µ —á–µ–º Phrase Matcher")
        
        return filtered_results
    
    def _calculate_ner_confidence(self, org_text: str, full_text: str, match) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è NER –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        
        base_confidence = 0.70
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É –Ω–∞–∑–≤–∞–Ω–∏—è
        length_bonus = min(0.20, len(org_text.split()) * 0.04)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
        caps_bonus = 0.05 if org_text[0].isupper() else 0
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keyword_bonus = 0
        gov_keywords = ['–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è', '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è']
        for keyword in gov_keywords:
            if keyword in org_text.lower():
                keyword_bonus = 0.15
                break
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_start = max(0, match.start() - 100)
        context_end = min(len(full_text), match.end() + 100)
        context = full_text[context_start:context_end].lower()
        
        context_bonus = 0
        context_markers = ['—Å–æ–æ–±—â–∏–ª', '–æ–±—ä—è–≤–∏–ª', '—É—Ç–≤–µ—Ä–¥–∏–ª', '–ø—Ä–æ–≤–µ–ª', '–ø–æ—Å—Ç–∞–Ω–æ–≤–∏–ª']
        for marker in context_markers:
            if marker in context:
                context_bonus = 0.05
                break
        
        return min(0.95, base_confidence + length_bonus + caps_bonus + keyword_bonus + context_bonus)
    
    def _find_potential_organizations(self, text: str) -> List[Dict[str, Any]]:
        """–ò—â–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ –æ–±—â–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        
        potential_orgs = []
        
        # –ò—â–µ–º —Å–ª–æ–≤–∞ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        org_indicators = [
            r'\b[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)*\b',  # –ù–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π
        ]
        
        for pattern in org_indicators:
            matches = re.finditer(pattern, text)
            
            for match in matches:
                candidate = match.group().strip()
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
                if self._is_likely_organization(candidate):
                    confidence = self._calculate_ner_confidence(candidate, text, match)
                    
                    potential_orgs.append({
                        'text': candidate,
                        'start': match.start(),
                        'end': match.end(),
                        'confidence': confidence * 0.7,  # –°–Ω–∏–∂–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö
                        'method': 'spacy_ner_potential',
                        'is_government': self._is_likely_government(candidate)
                    })
        
        return potential_orgs
    
    def _is_likely_organization(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ —Ç–µ–∫—Å—Ç –±—ã—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        words = text.split()
        
        # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        if len(words) < 2 or len(words) > 8:
            return False
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –æ–±—ã—á–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        common_phrases = ['–≤ —Ç–æ–º —á–∏—Å–ª–µ', '–≤ —Å–≤—è–∑–∏', '–ø–æ –≤–æ–ø—Ä–æ—Å–∞–º', '–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏']
        if any(phrase in text.lower() for phrase in common_phrases):
            return False
        
        return True
    
    def _is_likely_government(self, org_text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π"""
        
        org_lower = org_text.lower()
        gov_keywords = [
            '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç', '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ', '—Å–ª—É–∂–±–∞', '–∫–æ–º–∏—Ç–µ—Ç',
            '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–¥—É–º–∞', '–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞', '—Å—É–¥',
            '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è', '–º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–∞—è', '—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è'
        ]
        
        return any(keyword in org_lower for keyword in gov_keywords)
    
    def _remove_overlapping_detections(self, detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–£–¥–∞–ª—è–µ—Ç –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        
        if not detections:
            return []
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        detections.sort(key=lambda x: x['confidence'], reverse=True)
        
        filtered = []
        
        for detection in detections:
            is_overlapping = False
            
            for existing in filtered:
                if self._positions_overlap(detection, existing):
                    is_overlapping = True
                    break
            
            if not is_overlapping:
                filtered.append(detection)
        
        return filtered
    
    def _positions_overlap(self, det1: Dict, det2: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π"""
        return not (det1['end'] <= det2['start'] or det2['end'] <= det1['start'])
    
    def compare_results(self, phrase_results: List[Dict], ner_results: List[Dict], text: str):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–≤—É—Ö –ø–æ–¥—Ö–æ–¥–æ–≤"""
        
        print(f"\nüÜö –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ù–ê –†–ï–ê–õ–¨–ù–û–ú –î–û–ö–£–ú–ï–ù–¢–ï")
        print(f"{'='*80}")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        phrase_texts = {result['text'].lower().strip() for result in phrase_results}
        ner_texts = {result['text'].lower().strip() for result in ner_results}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        both_found = phrase_texts & ner_texts
        phrase_only = phrase_texts - ner_texts
        ner_only = ner_texts - phrase_texts
        
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–õ–Ø –î–ê–ù–ù–û–ì–û –î–û–ö–£–ú–ï–ù–¢–ê:")
        print(f"   üìö Phrase Matcher –Ω–∞—à–µ–ª: {len(phrase_results)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   ü§ñ NER —Å–∏–º—É–ª—è—Ü–∏—è –Ω–∞—à–ª–∞: {len(ner_results)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   üéØ –ù–∞–π–¥–µ–Ω—ã –æ–±–æ–∏–º–∏: {len(both_found)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        print(f"   üìö –¢–æ–ª—å–∫–æ Phrase Matcher: {len(phrase_only)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π") 
        print(f"   ü§ñ –¢–æ–ª—å–∫–æ NER: {len(ner_only)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
        
        if both_found:
            print(f"\n‚úÖ –ù–ê–ô–î–ï–ù–´ –û–ë–û–ò–ú–ò –ú–ï–¢–û–î–ê–ú–ò:")
            for org in sorted(both_found):
                print(f"   ‚Ä¢ {org}")
        
        if phrase_only:
            print(f"\nüìö –¢–û–õ–¨–ö–û PHRASE MATCHER:")
            for org in sorted(phrase_only):
                print(f"   ‚Ä¢ {org}")
                print(f"     ‚Üí –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–µ–π")
        
        if ner_only:
            print(f"\nü§ñ –¢–û–õ–¨–ö–û NER –ü–û–î–•–û–î:")
            for org in sorted(ner_only):
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
                full_info = next(r for r in ner_results if r['text'].lower().strip() == org)
                gov_status = "üèõÔ∏è –≥–æ—Å–æ—Ä–≥–∞–Ω" if full_info.get('is_government') else "üè¢ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è"
                print(f"   ‚Ä¢ {org} {gov_status} (conf: {full_info['confidence']:.2f})")
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        phrase_time = sum(0.1 for _ in phrase_results)  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è
        ner_time = sum(0.5 for _ in ner_results)  # NER –º–µ–¥–ª–µ–Ω–Ω–µ–µ
        
        print(f"\n‚ö° –í–´–í–û–î–´ –ü–û –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
        print(f"   üìö Phrase Matcher: –±—ã—Å—Ç—Ä—ã–π, —Ç–æ—á–Ω—ã–π –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
        print(f"   ü§ñ NER –ø–æ–¥—Ö–æ–¥: –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –±–æ–ª—å—à–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤")
        
        print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –í–ê–®–ï–ì–û –î–û–ö–£–ú–ï–ù–¢–ê:")
        
        if len(phrase_results) > 0 and len(ner_results) > len(phrase_results):
            print(f"   1. Phrase Matcher –Ω–∞—à–µ–ª {len(phrase_results)} —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
            print(f"   2. NER –Ω–∞—à–µ–ª –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ {len(ner_results) - len(phrase_results)} –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
            print(f"   3. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –û–ë–ê –ø–æ–¥—Ö–æ–¥–∞:")
            print(f"      ‚Ä¢ Phrase Matcher –¥–ª—è –≤—ã—Å–æ–∫–æ—Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤")
            print(f"      ‚Ä¢ NER –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏–ª–∏ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π")
        
        elif len(phrase_results) > 0:
            print(f"   ‚Ä¢ –í –¥–æ–∫—É–º–µ–Ω—Ç–µ –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –≥–æ—Å–æ—Ä–≥–∞–Ω—ã")
            print(f"   ‚Ä¢ Phrase Matcher –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
            print(f"   ‚Ä¢ NER –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ")
        
        else:
            print(f"   ‚Ä¢ –í –¥–æ–∫—É–º–µ–Ω—Ç–µ –º–∞–ª–æ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤")
            print(f"   ‚Ä¢ NER –ø–æ–¥—Ö–æ–¥ –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
            print(f"   ‚Ä¢ –°—Ç–æ–∏—Ç —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Å–ª–æ–≤–∞—Ä—å Phrase Matcher")

def run_real_document_demo():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
    
    print(f"üî¨ –ê–ù–ê–õ–ò–ó –†–ï–ê–õ–¨–ù–û–ì–û –î–û–ö–£–ú–ï–ù–¢–ê: SPACY NER vs PHRASE MATCHER")
    print(f"{'='*80}")
    print(f"üìÑ –î–æ–∫—É–º–µ–Ω—Ç: test_01_1_4_SD.docx")
    print()
    
    try:
        analyzer = DocumentAnalysisDemo()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        document_text = analyzer.extract_document_text()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–≤—É–º—è –ø–æ–¥—Ö–æ–¥–∞–º–∏
        phrase_results = analyzer.simulate_phrase_matcher(document_text)
        ner_results = analyzer.simulate_spacy_ner(document_text)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        analyzer.compare_results(phrase_results, ner_results, document_text)
        
        print(f"\nüéì –ò–¢–û–ì–û–í–´–ï –í–´–í–û–î–´:")
        print(f"{'='*50}")
        print(f"""
üîë –ö–õ–Æ–ß–ï–í–´–ï –†–ê–ó–õ–ò–ß–ò–Ø –ù–ê –ü–†–ê–ö–¢–ò–ö–ï:

üìö PHRASE MATCHER:
   ‚Ä¢ –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ —Ç–æ—á–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å
   ‚Ä¢ –ò—â–µ—Ç –¢–û–õ–¨–ö–û –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
   ‚Ä¢ –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —Ç–æ—á–Ω–æ—Å—Ç—å
   ‚Ä¢ –ù–ï –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
   ‚Ä¢ –ò–¥–µ–∞–ª–µ–Ω –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ç–∏–ø–æ–≤—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏

ü§ñ SPACY NER:
   ‚Ä¢ –†–∞–±–æ—Ç–∞–µ—Ç –∫–∞–∫ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
   ‚Ä¢ –ú–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –ù–û–í–´–ï –∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
   ‚Ä¢ –ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é
   ‚Ä¢ –ú–æ–∂–µ—Ç –¥–∞–≤–∞—Ç—å –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è
   ‚Ä¢ –ò–¥–µ–∞–ª–µ–Ω –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π

üí° –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:

1. –î–ª—è –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ì–û –ü–û–ö–†–´–¢–ò–Ø ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –û–ë–ê –ø–æ–¥—Ö–æ–¥–∞
2. –î–ª—è –í–´–°–û–ö–û–ô –°–ö–û–†–û–°–¢–ò ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ Phrase Matcher  
3. –î–ª—è –ü–û–ò–°–ö–ê –ù–û–í–´–• –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô ‚Üí –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ NER
4. –î–ª—è –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–´–• –î–û–ö–£–ú–ï–ù–¢–û–í ‚Üí –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –¥–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

üéØ –î–õ–Ø –í–ê–®–ï–ô –°–ò–°–¢–ï–ú–´ –ê–ù–û–ù–ò–ú–ò–ó–ê–¶–ò–ò:
   –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è - –Ω–∞—á–∞—Ç—å —Å Phrase Matcher –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö 
   –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤, –∑–∞—Ç–µ–º –¥–æ–±–∞–≤–∏—Ç—å NER –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤.
        """)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        print(f"\nüí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print(f"   ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø—É—Ç–∏")
        print(f"   ‚Ä¢ –ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞")
        print(f"   ‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –∏–ª–∏ –Ω–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .docx")

if __name__ == "__main__":
    run_real_document_demo()
#!/usr/bin/env python3
"""
–ü—Ä–∏–º–µ—Ä –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
"""

import spacy
from pymorphy3 import MorphAnalyzer
from typing import List, Dict, Any, Set
import re

class MorphologicalGovOrgDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    
    def __init__(self):
        self.nlp = spacy.load("ru_core_news_lg")
        self.morph = MorphAnalyzer()
        
        # –ë–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—ã –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        self.gov_base_words = {
            # –¢–∏–ø—ã –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
            '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ': {'NOUN'},
            '–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç': {'NOUN'},
            '—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ': {'NOUN'},
            '—Å–ª—É–∂–±–∞': {'NOUN'},
            '–∞–≥–µ–Ω—Ç—Å—Ç–≤–æ': {'NOUN'},
            '–∫–æ–º–∏—Ç–µ—Ç': {'NOUN'},
            '–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è': {'NOUN'},
            '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ': {'NOUN'},
            '–¥—É–º–∞': {'NOUN'},
            '—Å–æ–≤–µ—Ç': {'NOUN'},
            '–ø—Ä–æ–∫—É—Ä–∞—Ç—É—Ä–∞': {'NOUN'},
            '—Å—É–¥': {'NOUN'},
            
            # –£—Ä–æ–≤–Ω–∏ –≤–ª–∞—Å—Ç–∏
            '—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π': {'ADJF'},
            '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π': {'ADJF'},
            '—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π': {'ADJF'},
            '–º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω—ã–π': {'ADJF'},
            '–≥–æ—Ä–æ–¥—Å–∫–æ–π': {'ADJF'},
            
            # –°—Ñ–µ—Ä—ã –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            '–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π': {'ADJF'},
            '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': {'NOUN'},
            '–∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ': {'NOUN'},
            '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å': {'NOUN'},
            '—Ñ–∏–Ω–∞–Ω—Å—ã': {'NOUN'},
            '—é—Å—Ç–∏—Ü–∏—è': {'NOUN'},
        }
        
        # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–Ω–µ —Å–∫–ª–æ–Ω—è—é—Ç—Å—è)
        self.gov_abbreviations = {
            '–º–≤–¥', '—Ñ–Ω—Å', '—Ñ—Å–±', '–º—á—Å', '—Å–≤—Ä', '—Ñ–∞—Å', '—Ñ—Å—Ç',
            '—Ä–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä', '—Ä–æ—Å—Ä–µ–µ—Å—Ç—Ä', '—Ä–æ—Å—Ç—É—Ä–∏–∑–º', '—Ä–æ—Å—Å—Ç–∞—Ç',
            '—Ä–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä', '—Ä–æ—Å—Ç–µ—Ö–Ω–∞–¥–∑–æ—Ä', '—Ä–æ—Å–ø—Ä–∏—Ä–æ–¥–Ω–∞–¥–∑–æ—Ä'
        }
    
    def detect_government_orgs(self, text: str) -> List[Dict[str, Any]]:
        """
        –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏
        
        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Ç–æ–∫–µ–Ω—ã
        2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –ø–æ–ª—É—á–∞–µ–º –ª–µ–º–º—É (–±–∞–∑–æ–≤—É—é —Ñ–æ—Ä–º—É)
        3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–µ–º–º—É –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º —Å–ª–æ–≤–∞–º
        4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤
        5. –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        """
        doc = self.nlp(text)
        detections = []
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º
        for i, token in enumerate(doc):
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
            if token.text.lower() in self.gov_abbreviations:
                detection = self._create_abbreviation_detection(token, text)
                if detection:
                    detections.append(detection)
                continue
            
            # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö —Å–ª–æ–≤
            if self._is_government_word(token):
                # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è
                full_name = self._extract_full_organization_name(doc, i)
                
                if full_name and len(full_name.strip()) > 3:  # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
                    detection = self._create_morphological_detection(full_name, token, text)
                    detections.append(detection)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        return self._deduplicate_detections(detections)
    
    def _is_government_word(self, token) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º"""
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ªemm—ã —á–µ—Ä–µ–∑ pymorphy3
        morphs = self.morph.parse(token.text)
        
        for morph in morphs:
            lemma = morph.normal_form
            pos_tag = morph.tag.POS
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä–µ –±–∞–∑–æ–≤—ã—Ö —Ñ–æ—Ä–º
            if lemma in self.gov_base_words:
                expected_pos = self.gov_base_words[lemma]
                if pos_tag in expected_pos:
                    return True
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ spaCy
        spacy_lemma = token.lemma_.lower()
        if spacy_lemma in self.gov_base_words:
            return True
        
        return False
    
    def _extract_full_organization_name(self, doc, center_idx: int) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –≤–æ–∫—Ä—É–≥ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        
        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –ù–∞—á–∏–Ω–∞–µ–º –æ—Ç –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        2. –†–∞—Å—à–∏—Ä—è–µ–º—Å—è –≤–ª–µ–≤–æ –∏ –≤–ø—Ä–∞–≤–æ, –∑–∞—Ö–≤–∞—Ç—ã–≤–∞—è —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        3. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –∑–Ω–∞–∫–∞—Ö –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏–ª–∏ –Ω–µ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö
        """
        tokens = doc
        start_idx = center_idx
        end_idx = center_idx + 1
        
        # –†–∞—Å—à–∏—Ä—è–µ–º—Å—è –≤–ª–µ–≤–æ
        for i in range(center_idx - 1, -1, -1):
            token = tokens[i]
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ (–∫—Ä–æ–º–µ –¥–µ—Ñ–∏—Å–∞)
            if token.is_punct and token.text != '-':
                break
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–≥–∞—Ö –≤ –∫–æ–Ω—Ü–µ —Ñ—Ä–∞–∑—ã
            if token.pos_ in ['ADP'] and i == center_idx - 1:
                break
            
            # –í–∫–ª—é—á–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
            if self._is_related_word(token, tokens[center_idx]):
                start_idx = i
            else:
                break
        
        # –†–∞—Å—à–∏—Ä—è–µ–º—Å—è –≤–ø—Ä–∞–≤–æ
        for i in range(center_idx + 1, len(tokens)):
            token = tokens[i]
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
            if token.is_punct and token.text not in ['-', '.']:
                break
            
            # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –≥–ª–∞–≥–æ–ª–∞—Ö (–Ω–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
            if token.pos_ == 'VERB':
                break
            
            # –í–∫–ª—é—á–∞–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
            if self._is_related_word(token, tokens[center_idx]):
                end_idx = i + 1
            else:
                break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        full_name = doc[start_idx:end_idx].text.strip()
        return full_name
    
    def _is_related_word(self, token, center_token) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–≤—è–∑–∞–Ω–æ –ª–∏ —Å–ª–æ–≤–æ —Å —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–º –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º —Å–ª–æ–≤–æ–º"""
        
        # –í–∫–ª—é—á–∞–µ–º –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ (—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–æ–µ, –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ)
        if token.pos_ in ['ADJ', 'ADJF']:
            return True
        
        # –í–∫–ª—é—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
        if token.pos_ in ['NOUN']:
            return True
        
        # –í–∫–ª—é—á–∞–µ–º –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–†–§, –†–æ—Å—Å–∏–∏, –ü–µ—Ä–º—Å–∫–æ–≥–æ)
        if token.ent_type_ in ['LOC', 'GPE']:
            return True
        
        # –í–∫–ª—é—á–∞–µ–º –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ
        if token.pos_ == 'PROPN':
            return True
        
        # –í–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥–ª–æ–≥–∏ –∏ —Å–æ—é–∑—ã –≤–Ω—É—Ç—Ä–∏ —Ñ—Ä–∞–∑—ã
        if token.pos_ in ['ADP', 'CCONJ'] and token.text.lower() in ['–ø–æ', '–ø—Ä–∏', '–≤', '–∏', '–¥–ª—è']:
            return True
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        if token.text.lower() in ['—Ä—Ñ', '—Ä–æ—Å—Å–∏–∏', '—Ä–æ—Å—Å–∏–π—Å–∫–æ–π', '—Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏', '–∫—Ä–∞—è', '–æ–±–ª–∞—Å—Ç–∏', '—Ä–µ—Å–ø—É–±–ª–∏–∫–∏']:
            return True
        
        return False
    
    def _create_morphological_detection(self, org_name: str, anchor_token, original_text: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏"""
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
        start_pos = original_text.lower().find(org_name.lower())
        if start_pos == -1:
            start_pos = anchor_token.idx
            org_name = anchor_token.text
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        confidence = self._calculate_morphological_confidence(org_name, anchor_token)
        
        return {
            'category': 'government_org',
            'original_value': org_name,
            'confidence': confidence,
            'position': {
                'start': start_pos,
                'end': start_pos + len(org_name)
            },
            'method': 'morphological_analysis',
            'anchor_word': anchor_token.text,
            'morphological_info': self._get_morphological_info(anchor_token)
        }
    
    def _create_abbreviation_detection(self, token, original_text: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é –¥–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã"""
        
        return {
            'category': 'government_org',
            'original_value': token.text,
            'confidence': 0.95,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
            'position': {
                'start': token.idx,
                'end': token.idx + len(token.text)
            },
            'method': 'abbreviation_match',
            'abbreviation_type': 'government_agency'
        }
    
    def _calculate_morphological_confidence(self, org_name: str, anchor_token) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        
        base_confidence = 0.75
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É –Ω–∞–∑–≤–∞–Ω–∏—è
        length_bonus = min(0.15, len(org_name.split()) * 0.03)
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        morph_bonus = 0.0
        morphs = self.morph.parse(anchor_token.text)
        
        for morph in morphs:
            # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —á–∞—Å—Ç—å —Ä–µ—á–∏
            if morph.tag.POS in ['NOUN', 'ADJF']:
                morph_bonus += 0.05
            
            # –ë–æ–Ω—É—Å –∑–∞ –æ–¥—É—à–µ–≤–ª–µ–Ω–Ω–æ—Å—Ç—å (–¥–ª—è –¥–æ–ª–∂–Ω–æ—Å—Ç–µ–π)
            if 'anim' in str(morph.tag):
                morph_bonus += 0.03
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö —É–∫–∞–∑–∞–Ω–∏–π
        geo_bonus = 0.0
        geo_words = ['—Ä—Ñ', '—Ä–æ—Å—Å–∏–∏', '—Ä–æ—Å—Å–∏–π—Å–∫–æ–π', '—Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏', '–∫—Ä–∞—è', '–æ–±–ª–∞—Å—Ç–∏', '—Ä–µ—Å–ø—É–±–ª–∏–∫–∏']
        for geo_word in geo_words:
            if geo_word in org_name.lower():
                geo_bonus = 0.08
                break
        
        final_confidence = min(0.95, base_confidence + length_bonus + morph_bonus + geo_bonus)
        return final_confidence
    
    def _get_morphological_info(self, token) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–∫–µ–Ω–µ"""
        
        morphs = self.morph.parse(token.text)
        best_morph = morphs[0] if morphs else None
        
        if not best_morph:
            return {}
        
        return {
            'lemma': best_morph.normal_form,
            'pos': str(best_morph.tag.POS),
            'case': str(best_morph.tag.case) if best_morph.tag.case else None,
            'number': str(best_morph.tag.number) if best_morph.tag.number else None,
            'gender': str(best_morph.tag.gender) if best_morph.tag.gender else None,
            'animacy': str(best_morph.tag.animacy) if best_morph.tag.animacy else None,
        }
    
    def _deduplicate_detections(self, detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        
        if not detections:
            return []
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –¥–ª–∏–Ω–µ
        detections.sort(key=lambda x: (-x['confidence'], -(x['position']['end'] - x['position']['start'])))
        
        filtered = []
        
        for detection in detections:
            is_duplicate = False
            
            for existing in filtered:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π
                if self._positions_overlap(detection['position'], existing['position']):
                    is_duplicate = True
                    break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–π
                if detection['original_value'].lower() in existing['original_value'].lower():
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered.append(detection)
        
        return filtered
    
    def _positions_overlap(self, pos1: Dict[str, int], pos2: Dict[str, int]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π"""
        return not (pos1['end'] <= pos2['start'] or pos2['end'] <= pos1['start'])

def demonstrate_morphological_analysis():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥"""
    
    print("üî§ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ú–û–†–§–û–õ–û–ì–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê")
    print("=" * 60)
    
    detector = MorphologicalGovOrgDetector()
    
    # –¢–µ—Å—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–∫–ª–æ–Ω–µ–Ω–∏—è–º–∏ –∏ —Ñ–æ—Ä–º–∞–º–∏
    test_cases = [
        # –°–∫–ª–æ–Ω–µ–Ω–∏—è
        "–°–æ—Ç—Ä—É–¥–Ω–∏–∫–∏ –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –¥–µ–ª –ø—Ä–æ–≤–µ–ª–∏ –æ–ø–µ—Ä–∞—Ü–∏—é.",
        "–ü—Ä–∏–∫–∞–∑ –î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –±—ã–ª –ø–æ–¥–ø–∏—Å–∞–Ω.",
        "–í –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≥–æ—Ä–æ–¥–∞ —Å–æ—Å—Ç–æ—è–ª–æ—Å—å —Å–æ–≤–µ—â–∞–Ω–∏–µ.",
        "–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞ –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–Ω—è–ª–æ —Ä–µ—à–µ–Ω–∏–µ.",
        
        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        "–§–ù–° –†–æ—Å—Å–∏–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–ª–∞ —Ä–∞–∑—ä—è—Å–Ω–µ–Ω–∏—è.",
        "–†–æ—Å–∫–æ–º–Ω–∞–¥–∑–æ—Ä –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª —Ä–µ—Å—É—Ä—Å—ã.",
        
        # –°–ª–æ–∂–Ω—ã–µ —Å–ª—É—á–∞–∏
        "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞ –ø–æ –Ω–∞–¥–∑–æ—Ä—É –≤ —Å—Ñ–µ—Ä–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ–≤–µ—Ä–∏–ª–∞ –≤—É–∑.",
        "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ú–í–î –ø–æ –°–≤–µ—Ä–¥–ª–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ —Å–æ–æ–±—â–∏–ª–æ –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö.",
        
        # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã
        "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤—É –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è –≤—ã–¥–µ–ª–∏–ª–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞.",
        "–ö–æ–º–∏—Ç–µ—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥–∞ –æ–±—ä—è–≤–∏–ª –∫–æ–Ω–∫—É—Ä—Å."
    ]
    
    total_detected = 0
    
    for i, text in enumerate(test_cases, 1):
        print(f"\nüìù –¢–µ—Å—Ç {i}: {text}")
        detections = detector.detect_government_orgs(text)
        
        if detections:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(detections)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π:")
            for det in detections:
                print(f"   ‚Ä¢ '{det['original_value']}' (confidence: {det['confidence']:.3f})")
                print(f"     –ú–µ—Ç–æ–¥: {det['method']}")
                if 'morphological_info' in det:
                    morph_info = det['morphological_info']
                    print(f"     –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è: {morph_info.get('lemma', 'N/A')} ({morph_info.get('pos', 'N/A')})")
            total_detected += len(detections)
        else:
            print("‚ùå –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    print(f"\nüìä –ò–¢–û–ì–û: –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_detected} –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
    
    print(f"\nüéØ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –ú–û–†–§–û–õ–û–ì–ò–ß–ï–°–ö–û–ì–û –ê–ù–ê–õ–ò–ó–ê:")
    print(f"‚úÖ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Å–∫–ª–æ–Ω–µ–Ω–∏—è –∏ –ø–∞–¥–µ–∂–∏")
    print(f"‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º–∞–º–∏")
    print(f"‚úÖ –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞")
    print(f"‚úÖ –ü–æ–Ω–∏–º–∞–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É")
    print(f"‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Ö–æ–¥–∏—Ç –±–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤")
    
    print(f"\nüìà –£–õ–£–ß–®–ï–ù–ò–ï –ö–ê–ß–ï–°–¢–í–ê:")
    print(f"‚Ä¢ –ü–æ–∫—Ä—ã—Ç–∏–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ 40-60%")
    print(f"‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å –æ—Å—Ç–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–æ–π (85-90%)")
    print(f"‚Ä¢ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —Ä–∞–Ω–µ–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å–ª—É—á–∞–∏")
    print(f"‚Ä¢ –°–Ω–∏–∂–∞–µ—Ç—Å—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π")

if __name__ == "__main__":
    demonstrate_morphological_analysis()
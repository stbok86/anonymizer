#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –¥–ª—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏
"""

import spacy
import re
import uuid
from typing import List, Dict, Any, Set, Tuple, Optional
from spacy.matcher import Matcher, PhraseMatcher
from spacy.tokens import Doc, Span, Token

try:
    from detection_strategies import DetectionStrategy
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    from detection_strategies import DetectionStrategy


class InformationSystemStrategy(DetectionStrategy):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏—è –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –∏ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞–∑–≤–∞–Ω–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º"""
    
    def __init__(self, config_settings: Dict[str, Any], nlp_model=None):
        super().__init__(config_settings)
        self.nlp = nlp_model  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        self.matcher = None
        self.phrase_matcher = None
        self.partitioner = None
        self.is_initialized = False
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ä–∞–∑—É –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ–±—ä–µ–∫—Ç–∞
        self._initialize_components()
        
    def _initialize_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è spaCy –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        if self.is_initialized:
            return
            
        try:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞, –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–æ—é
            if self.nlp is None:
                model_name = self.settings.get('spacy_model', 'ru_core_news_sm')
                models_to_try = [model_name, 'ru_core_news_sm', 'ru_core_news_md']
                
                for model in models_to_try:
                    try:
                        self.nlp = spacy.load(model)
                        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ spaCy –º–æ–¥–µ–ª—å: {model}")
                        break
                    except OSError:
                        continue
                
                if self.nlp is None:
                    raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É —Ä—É—Å—Å–∫—É—é spaCy –º–æ–¥–µ–ª—å")
            # else: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –≤—ã–≤–æ–¥–∞
            
            # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—á–µ—Ä—ã
            self.matcher = Matcher(self.nlp.vocab)
            self.phrase_matcher = PhraseMatcher(self.nlp.vocab, attr="LOWER")
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–µ—Ä
            self.partitioner = ISPartitioner(self.nlp, self.settings)
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª–∞
            self._setup_patterns()
            
            self.is_initialized = True
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ InformationSystemStrategy: {e}")
            # –ù–µ –ø–æ–¥–Ω–∏–º–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –≤–µ—Å—å NLP —Å–µ—Ä–≤–∏—Å
            self.is_initialized = False
    
    def _setup_patterns(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ò–° –∏–∑ JSON-—Ñ–∞–π–ª–∞"""
        from nlp_config import NLPConfig
        config = NLPConfig()
        patterns = config.get_information_system_patterns()
        if not patterns:
            raise RuntimeError("–ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ nlp_patterns.json")
        self.complex_abbr_patterns = [p['pattern'] for p in patterns if p.get('type') == 'regex' and p.get('priority', 1) == 1]
        self.spaced_abbr_patterns = [p['pattern'] for p in patterns if p.get('type') == 'regex' and p.get('priority', 1) == 2]
        abbr_phrases = [p['pattern'] for p in patterns if p.get('type') == 'phrase']
        abbr_docs = [self.nlp(abbr) for abbr in abbr_phrases]
        if abbr_docs:
            self.phrase_matcher.add("IS_SIMPLE_ABBREVIATIONS", abbr_docs)
    
    def detect_information_systems_in_text(self, text: str, doc: Doc = None) -> List[Dict[str, Any]]:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            doc: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π spaCy Doc (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –ò–° —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        if not self.is_initialized:
            return []
        
        if doc is None:
            doc = self.nlp(text)
        
        detections = []
        
        try:
            complex_detections = self._search_complex_abbreviations(text)
            detections.extend(complex_detections)
            regex_detections = self._simple_pattern_search(text, doc, detections)
            detections.extend(regex_detections)
            simple_detections = self._search_simple_abbreviations(text, doc, detections)
            detections.extend(simple_detections)
            spaced_detections = self._search_spaced_abbreviations_filtered(text, detections)
            detections.extend(spaced_detections)
            detections = self._remove_duplicates(detections, threshold=0.7)
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ò–°: {e}")
            return []
        
        return detections
    
    def _search_complex_abbreviations(self, text: str) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Å–ª–æ–∂–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä —Ç–∏–ø–∞ –ï–ò–°–£–§–•–î —Å –ø–æ–¥—Ä–æ–±–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        detections = []
        for pattern in self.complex_abbr_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                start_char = match.start()
                end_char = match.end()
                # Post-filter: abbreviation must not be inside a word (not preceded or followed by lowercase letter)
                before = text[start_char - 1] if start_char > 0 else ''
                after = text[end_char] if end_char < len(text) else ''
                debug_context = text[max(0, start_char-20):min(len(text), end_char+20)]
                print(f"[DEBUG][IS][complex_abbr] pattern: {pattern} | match: '{match.group(0)}' | pos: {start_char}-{end_char} | before: '{before}' | after: '{after}' | context: ...{debug_context}...")
                if (before and before.islower()) or (after and after.islower()):
                    print(f"[DEBUG][IS][complex_abbr][SKIP] False positive filtered: '{match.group(0)}' at {start_char}-{end_char}")
                    continue
                anonymous_part = match.group(1)  # –ï–ò–°
                private_part = match.group(2)    # –£–§–•–î
                full_match = match.group(0)      # –ï–ò–°–£–§–•–î
                anonymized_text = f"{anonymous_part} [SYSTEM_ID]"
                detection = {
                    'category': 'information_system',
                    'original_value': full_match,
                    'confidence': 0.9,
                    'position': {'start': start_char, 'end': end_char},
                    'method': 'complex_abbreviation',
                    'uuid': 'placeholder',
                    'system_type': 'information_system',
                    'core_part': anonymous_part,
                    'private_part': private_part,
                    'anonymized_text': anonymized_text
                }
                # –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π –ª–æ–≥
                print(
                    f"[complex_abbreviation][DETECT] pattern: {pattern} | "
                    f"match: '{full_match}' | pos: {start_char}-{end_char} | "
                    f"core: '{anonymous_part}' | private: '{private_part}' | "
                    f"anonymized: '{anonymized_text}' | text: ...{text[max(0, start_char-30):min(len(text), end_char+30)]}..."
                )
                detections.append(detection)
        return detections
    
    def _search_spaced_abbreviations_filtered(self, text: str, existing_detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ —Ç–æ–ª—å–∫–æ –≤ –º–µ—Å—Ç–∞—Ö –±–µ–∑ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤"""
        detections = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
        occupied_ranges = []
        for detection in existing_detections:
            pos = detection.get('position', {})
            if 'start' in pos and 'end' in pos:
                occupied_ranges.append((pos['start'], pos['end']))
        
        for pattern in self.spaced_abbr_patterns:
            matches = re.finditer(pattern, text)
            for match in matches:
                start_char = match.start()
                end_char = match.end()
                # Post-filter: abbreviation must not be inside a word (not preceded or followed by lowercase letter)
                before = text[start_char - 1] if start_char > 0 else ''
                after = text[end_char] if end_char < len(text) else ''
                debug_context = text[max(0, start_char-20):min(len(text), end_char+20)]
                print(f"[DEBUG][IS][spaced_abbr] pattern: {pattern} | match: '{match.group(0)}' | pos: {start_char}-{end_char} | before: '{before}' | after: '{after}' | context: ...{debug_context}...")
                if (before and before.islower()) or (after and after.islower()):
                    print(f"[DEBUG][IS][spaced_abbr][SKIP] False positive filtered: '{match.group(0)}' at {start_char}-{end_char}")
                    continue
                anonymous_part = match.group(1)  # –ï–ò–°/–§–ì–ò–°
                private_part = match.group(2).strip()  # –£–§–•–î –ü–ö
                full_match = match.group(0)      # –ï–ò–° –£–§–•–î –ü–ö
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏
                is_overlapping = False
                for occ_start, occ_end in occupied_ranges:
                    if not (end_char <= occ_start or start_char >= occ_end):
                        is_overlapping = True
                        break
                if not is_overlapping:
                    # –°–æ–∑–¥–∞–µ–º –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ FormatterApplier)
                    anonymized_text = f"{anonymous_part} [SYSTEM_ID]"
                    detection = {
                        'category': 'information_system',
                        'original_value': full_match,
                        'confidence': 0.9,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ—á–Ω—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
                        'position': {'start': start_char, 'end': end_char},
                        'method': 'spaced_abbreviation',
                        'uuid': 'placeholder',  # –í—Ä–µ–º–µ–Ω–Ω—ã–π placeholder, UUID –±—É–¥–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ –≤ FormatterApplier
                        'system_type': 'information_system',
                        'core_part': anonymous_part,
                        'private_part': private_part,
                        'anonymized_text': anonymized_text
                    }
                    print(f"üîß –û–±—â–∞—è —á–∞—Å—Ç—å (–±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏): '{anonymous_part}'")
                    detections.append(detection)
        return detections

    def _search_spaced_abbreviations_filtered(self, text: str, existing_detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ —Å —Ç–æ—á–Ω—ã–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≥—Ä–∞–Ω–∏—Ü —á–µ—Ä–µ–∑ spaCy —Ç–æ–∫–µ–Ω—ã"""
        detections = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–π –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π
        occupied_ranges = []
        for detection in existing_detections:
            pos = detection.get('position', {})
            if 'start' in pos and 'end' in pos:
                occupied_ranges.append((pos['start'], pos['end']))
        
        # –°–æ–∑–¥–∞–µ–º spaCy –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è —Ç–æ—á–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
        doc = self.nlp(text)
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä –≤ –Ω–∞—á–∞–ª–µ —Ç–æ–∫–µ–Ω–æ–≤
        abbreviation_tokens = {
            '–ï–ò–°': '–ï–ò–°', '–ê–ò–°': '–ê–ò–°', '–ì–ò–°': '–ì–ò–°', 
            '–§–ì–ò–°': '–§–ì–ò–°', '–ï–ì–ò–°': '–ï–ì–ò–°', '–ü–ì–ò–°': '–ü–ì–ò–°',
            '–ì–ê–°': '–ì–ê–°', '–§–ò–°': '–§–ò–°', '–†–ò–°': '–†–ò–°'
        }
        
        for i, token in enumerate(doc):
            token_text = token.text.upper()
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ–∫–µ–Ω –æ–¥–Ω–æ–π –∏–∑ –Ω–∞—à–∏—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
            if token_text in abbreviation_tokens:
                anonymous_part = abbreviation_tokens[token_text]
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–µ—Ä–µ—Å–µ–∫–∞–µ—Ç—Å—è –ª–∏ —Å —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏
                start_char = token.idx
                is_overlapping = False
                for occ_start, occ_end in occupied_ranges:
                    if not (start_char >= occ_end or start_char < occ_start):
                        is_overlapping = True
                        break
                if is_overlapping:
                    continue
                # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã-–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (–∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã, –¥–ª–∏–Ω–∞ <= 10)
                private_parts = []
                current_pos = i + 1
                while current_pos < len(doc):
                    next_token = doc[current_pos]
                    next_text = next_token.text.strip()
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–µ–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (–≤–∫–ª—é—á–∞—è –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã)
                    if not next_text or next_text.isspace():
                        current_pos += 1
                        continue
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω —á–∞—Å—Ç—å—é –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
                    if (next_text and 
                        len(next_text) <= 10 and 
                        next_text.isupper() and 
                        next_text.isalpha() and
                        not next_text.lower() in ['–∏', '–≤', '–Ω–∞', '—Å', '–¥–ª—è', '–ø–æ']):  # –ò—Å–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥–ª–æ–≥–∏
                        private_parts.append(next_text)
                        current_pos += 1
                        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º 3 —Ç–æ–∫–µ–Ω–∞ –≤ –ø—Ä–∏–≤–∞—Ç–Ω–æ–π —á–∞—Å—Ç–∏
                        if len(private_parts) >= 3:
                            break
                    else:
                        # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ –Ω–µ-–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                        break
                # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∏–≤–∞—Ç–Ω–∞—è —á–∞—Å—Ç—å
                if private_parts:
                    private_part = ' '.join(private_parts)
                    # –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–∫–µ–Ω–æ–≤
                    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–Ω–∞—á–∏–º—ã–π (–Ω–µ-–ø—Ä–æ–±–µ–ª—å–Ω—ã–π) —Ç–æ–∫–µ–Ω
                    last_meaningful_pos = current_pos - 1
                    while (last_meaningful_pos >= 0 and 
                           last_meaningful_pos < len(doc) and 
                           (not doc[last_meaningful_pos].text.strip() or doc[last_meaningful_pos].text.isspace())):
                        last_meaningful_pos -= 1
                    if last_meaningful_pos >= 0 and last_meaningful_pos < len(doc):
                        last_token = doc[last_meaningful_pos]
                        end_char = last_token.idx + len(last_token.text)
                    else:
                        # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∑–∏—Ü–∏—é —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
                        end_char = token.idx + len(token.text) + len(private_part) + 1
                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∏—Å—Ö–æ–¥–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                    full_match = text[start_char:end_char]
                    debug_context = text[max(0, start_char-20):min(len(text), end_char+20)]
                    print(f"[DEBUG][IS][spacy_abbr] abbr: '{anonymous_part}' | private: '{private_part}' | match: '{full_match}' | pos: {start_char}-{end_char} | context: ...{debug_context}...")
                    # –°–æ–∑–¥–∞–µ–º –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω –≤ FormatterApplier)
                    anonymized_text = f"{anonymous_part} [SYSTEM_ID]"
                    detection = {
                        'category': 'information_system',
                        'original_value': full_match,
                        'confidence': 0.95,  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Ç–æ–∫–µ–Ω-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                        'position': {'start': start_char, 'end': end_char},
                        'method': 'spaced_abbreviation',
                        'uuid': 'placeholder',  # –í—Ä–µ–º–µ–Ω–Ω—ã–π placeholder, UUID –±—É–¥–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ –≤ FormatterApplier
                        'system_type': 'information_system',
                        'core_part': anonymous_part,
                        'private_part': private_part,
                        'anonymized_text': anonymized_text
                    }
                    detections.append(detection)
        return detections

    def _search_spaced_abbreviations(self, text: str) -> List[Dict[str, Any]]:
        """–£—Å—Ç–∞—Ä–µ–≤—à–∏–π –º–µ—Ç–æ–¥ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        # –í—ã–∑—ã–≤–∞–µ–º –Ω–æ–≤—ã–π —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å –ø—É—Å—Ç—ã–º —Å–ø–∏—Å–∫–æ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
        return self._search_spaced_abbreviations_filtered(text, [])

    def _search_simple_abbreviations(self, text: str, doc, existing_detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø—Ä–æ—Å—Ç—ã—Ö –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã—Ö –≤ —Ç–æ–º –∂–µ –º–µ—Å—Ç–µ"""
        detections = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
        occupied_ranges = []
        for detection in existing_detections:
            pos = detection.get('position', {})
            if 'start' in pos and 'end' in pos:
                occupied_ranges.append((pos['start'], pos['end']))
        
        # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ PhraseMatcher
        phrase_results = self.phrase_matcher(doc)
        for match_id, start, end in phrase_results:
            span = doc[start:end]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–µ—Ä–µ—Å–µ–∫–∞–µ—Ç—Å—è –ª–∏ —Å —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏
            span_start = span.start_char
            span_end = span.end_char
            
            is_overlapping = False
            for occ_start, occ_end in occupied_ranges:
                if not (span_end <= occ_start or span_start >= occ_end):
                    is_overlapping = True
                    break
            
            if not is_overlapping:
                detection = self._create_simple_abbreviation_detection(span, doc)
                if detection:
                    detections.append(detection)
        
        return detections
    
    def _create_simple_abbreviation_detection(self, span, doc) -> Optional[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é –¥–ª—è –ø—Ä–æ—Å—Ç–æ–π –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã (—Ç–æ–ª—å–∫–æ –∞–Ω–æ–Ω–∏–º–Ω–∞—è —á–∞—Å—Ç—å)"""
        
        abbr_text = span.text.strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç - –µ—Å–ª–∏ –ø–æ—Å–ª–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –µ—Å—Ç—å –µ—â–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        next_token_index = span.end
        if next_token_index < len(doc):
            next_token = doc[next_token_index]
            # –ï—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω - —ç—Ç–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ –∏–∑ –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if (next_token.text and 
                len(next_token.text) <= 10 and 
                next_token.text.isupper() and 
                next_token.text.isalpha()):
                return None
                
            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–ª–µ–¥—É—é—â–∏–π —á–µ—Ä–µ–∑ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω (–¥–ª—è —Å–ª—É—á–∞–µ–≤ "–ï–ò–° –£–§–•–î –ü–ö")  
            if next_token_index + 1 < len(doc):
                token_after_next = doc[next_token_index + 1]
                if (token_after_next.text and 
                    len(token_after_next.text) <= 10 and 
                    token_after_next.text.isupper() and 
                    token_after_next.text.isalpha()):
                    return None
        
        # –ü—Ä–æ—Å—Ç—ã–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å (—Ç–æ–ª—å–∫–æ –∞–Ω–æ–Ω–∏–º–Ω–∞—è —á–∞—Å—Ç—å)
        detection = {
            'category': 'information_system',
            'original_value': abbr_text,
            'confidence': 0.85,
            'position': {'start': span.start_char, 'end': span.end_char},
            'method': 'simple_abbreviation',
            'uuid': 'placeholder',  # –í—Ä–µ–º–µ–Ω–Ω—ã–π placeholder, UUID –±—É–¥–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ –≤ FormatterApplier
            'system_type': 'information_system',
            'core_part': abbr_text,         # –í—Å—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—è - —ç—Ç–æ –∞–Ω–æ–Ω–∏–º–Ω–∞—è —á–∞—Å—Ç—å
            'private_part': '',             # –ù–µ—Ç –ø—Ä–∏–≤–∞—Ç–Ω–æ–π —á–∞—Å—Ç–∏
            'anonymized_text': abbr_text    # –û—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å
        }
        
        print(f"üîß –û–±—â–∞—è —á–∞—Å—Ç—å (–±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏): '{abbr_text}'")
        return detection
    
    def _simple_pattern_search(self, text: str, doc: Doc, existing_detections: List[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≥—Ä–∞–Ω–∏—Ü"""
        detections = []
        
        if existing_detections is None:
            existing_detections = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–∑–∏—Ü–∏–∏ —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–π –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π
        occupied_ranges = []
        for detection in existing_detections:
            pos = detection.get('position', {})
            if 'start' in pos and 'end' in pos:
                occupied_ranges.append((pos['start'], pos['end']))
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ò–° —Å —Ç–æ—á–Ω—ã–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –≥—Ä–∞–Ω–∏—Ü
        patterns = [
            {
                'name': '–µ–¥–∏–Ω–∞—è_–∏—Å_exact', 
                'pattern': r'(?i)(–ï–¥–∏–Ω–∞—è)\s+(–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è)\s+(—Å–∏—Å—Ç–µ–º–∞)',
                'capture_suffix': True
            },
            {
                'name': '–µ–¥–∏–Ω–∞—è_–∏—Å_declined', 
                'pattern': r'(?i)(–µ–¥–∏–Ω–æ[–π—ã–µ–º–∏])\s+(–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ[–π—ã–µ–º–∏])\s+(—Å–∏—Å—Ç–µ–º[–∞—ã–µ—É–æ–π])',
                'capture_suffix': True
            },
            {
                'name': '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è_–∏—Å',
                'pattern': r'(?i)(–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞[—è–æ–µ–π—ã–º–∏]|—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞[—è–æ–µ–π—ã–º–∏])\s+(–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞[—è–æ–µ–π—ã–º–∏])\s+(—Å–∏—Å—Ç–µ–º[–∞—ã–µ—É–æ–π])', 
                'capture_suffix': True
            },
            {
                'name': '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è_–∏—Å',
                'pattern': r'(?i)(–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞[—è–æ–µ–π—ã–º–∏])\s+(–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞[—è–æ–µ–π—ã–º–∏])\s+(—Å–∏—Å—Ç–µ–º[–∞—ã–µ—É–æ–π])',
                'capture_suffix': True
            },
        ]
        
        for pattern_info in patterns:
            pattern = pattern_info['pattern']
            matches = re.finditer(pattern, text, re.IGNORECASE | re.UNICODE)
            
            for match in matches:
                start_char = match.start()
                core_end = match.end()
                core_text = match.group().strip()
                
                # –ò—â–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —á–∞—Å—Ç—å –ø–æ—Å–ª–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                suffix_text = ""
                actual_end = core_end
                
                if pattern_info.get('capture_suffix', False):
                    # –ò—â–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                    remaining_text = text[core_end:]
                    suffix_match = self._extract_system_suffix(remaining_text)
                    if suffix_match:
                        suffix_text = suffix_match
                        # –ü—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –∫–æ–Ω–µ—á–Ω—É—é –ø–æ–∑–∏—Ü–∏—é
                        # –ò—â–µ–º suffix –≤ –æ—Å—Ç–∞–≤—à–µ–º—Å—è —Ç–µ–∫—Å—Ç–µ –∏ –ø—Ä–∏–±–∞–≤–ª—è–µ–º –∫ –ø–æ–∑–∏—Ü–∏–∏ core_end
                        suffix_start_in_remaining = remaining_text.lstrip().find(suffix_match.strip())
                        if suffix_start_in_remaining != -1:
                            # –£—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –Ω–∞—á–∞–ª–µ remaining_text
                            leading_spaces = len(remaining_text) - len(remaining_text.lstrip())
                            actual_end = core_end + leading_spaces + suffix_start_in_remaining + len(suffix_match.strip())
                        else:
                            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç
                            actual_end = core_end + len(suffix_match) + 1
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ, –∏–∑–±–µ–≥–∞—è –¥–≤–æ–π–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                if suffix_text:
                    full_name = f"{core_text} {suffix_text}".strip()
                else:
                    full_name = core_text
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–µ—Ä–µ—Å–µ–∫–∞–µ—Ç—Å—è –ª–∏ —Å —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏
                is_overlapping = False
                for occ_start, occ_end in occupied_ranges:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
                    if not (actual_end <= occ_start or start_char >= occ_end):
                        is_overlapping = True
                        break
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç—É –¥–µ—Ç–µ–∫—Ü–∏—é
                if is_overlapping:
                    continue
                
                # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é
                detection = {
                    'category': 'information_system',
                    'original_value': full_name,
                    'confidence': 0.8,
                    'position': {'start': start_char, 'end': actual_end},
                    'method': 'information_system_regex',
                    'uuid': 'placeholder'  # –í—Ä–µ–º–µ–Ω–Ω—ã–π placeholder, UUID –±—É–¥–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ –≤ FormatterApplier
                }
                
                # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∞—Å—Ç–∏
                partition_result = self._intelligent_partition(core_text, suffix_text)
                if partition_result:
                    detection.update(partition_result)
                
                detections.append(detection)
        
        return detections
    
    def _extract_system_suffix(self, remaining_text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        if not remaining_text:
            return None
        
        # –°—Ç–æ–ø-—Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –∫–æ–Ω–µ—Ü –Ω–∞–∑–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã
        stop_words = [
            '—Å–æ–¥–µ—Ä–∂–∏—Ç', '–≤–∫–ª—é—á–∞–µ—Ç', '–ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞', '–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è', '–ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è',
            '–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç', '—è–≤–ª—è–µ—Ç—Å—è', '–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç', '—Å–ª—É–∂–∏—Ç', '—Å–æ–∑–¥–∞–Ω–∞',
            '—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–∞', '–≤–Ω–µ–¥—Ä–µ–Ω–∞', '—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç', '—Ä–∞–±–æ—Ç–∞–µ—Ç', '–¥–µ–π—Å—Ç–≤—É–µ—Ç',
            '–ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç', '–æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç', '–≤—ã–ø–æ–ª–Ω—è–µ—Ç'
        ]
        
        # –£–±–∏—Ä–∞–µ–º –≤–µ–¥—É—â–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        cleaned_text = remaining_text.lstrip()
        words = cleaned_text.split()
        suffix_words = []
        
        for word in words:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—Ç–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É - —ç—Ç–æ –≤—Å–µ–≥–¥–∞ –∫–æ–Ω–µ—Ü –Ω–∞–∑–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã
            if '(' in word:
                # –ï—Å–ª–∏ —Å–∫–æ–±–∫–∞ –≤ –Ω–∞—á–∞–ª–µ —Å–ª–æ–≤–∞, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                if word.startswith('('):
                    break
                # –ï—Å–ª–∏ —Å–∫–æ–±–∫–∞ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ/–∫–æ–Ω—Ü–µ, –¥–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç—å –¥–æ —Å–∫–æ–±–∫–∏ –∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                else:
                    bracket_pos = word.find('(')
                    word_before_bracket = word[:bracket_pos].strip()
                    if word_before_bracket:
                        suffix_words.append(word_before_bracket)
                    break
            
            # –û—á–∏—â–∞–µ–º –æ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            clean_word = re.sub(r'[^\w\s\-]', '', word).lower()
            
            # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
            if clean_word in stop_words:
                break
            
            # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ —Ç–æ—á–∫—É, –∑–∞–ø—è—Ç—É—é –∏–ª–∏ –¥—Ä—É–≥–æ–π –∑–Ω–∞–∫ –∫–æ–Ω—Ü–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if word.endswith('.') or word.endswith(',') or word.endswith(';') or word.endswith('!') or word.endswith('?'):
                # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–æ –±–µ–∑ –∑–Ω–∞–∫–∞ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
                clean_word_for_suffix = word[:-1]
                if clean_word_for_suffix:
                    suffix_words.append(clean_word_for_suffix)
                break
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ã—á–Ω—ã–µ —Å–ª–æ–≤–∞
            if len(clean_word) > 0:
                suffix_words.append(word)
            else:
                break
                
        return ' '.join(suffix_words).strip() if suffix_words else None
    
    def _intelligent_partition(self, core_text: str, suffix_text: str) -> Optional[Dict[str, Any]]:
        """–£–º–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ò–° –Ω–∞ –æ–±—â—É—é –∏ –ø—Ä–∏–≤–∞—Ç–Ω—É—é —á–∞—Å—Ç–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–∞–¥–µ–∂–∞"""
        
        # –ù–ï –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ–±—â—É—é —á–∞—Å—Ç—å - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞–¥–µ–∂
        core_part = core_text.strip()
        print(f"üîß –û–±—â–∞—è —á–∞—Å—Ç—å (–±–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏): '{core_part}'")
        
        # –ü—Ä–∏–≤–∞—Ç–Ω–∞—è —á–∞—Å—Ç—å - —ç—Ç–æ suffix_text (—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è)
        private_part = suffix_text.strip() if suffix_text else ""
        
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        if private_part and len(private_part.split()) >= 1:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∏–≤–∞—Ç–Ω–∞—è —á–∞—Å—Ç—å, –∑–∞–º–µ–Ω—è–µ–º –µ—ë –Ω–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä
            anonymized_text = f"{core_part} [SYSTEM_ID]"
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø—Ä–∏–≤–∞—Ç–Ω–æ–π —á–∞—Å—Ç–∏, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            anonymized_text = core_part
        
        return {
            'system_type': 'information_system',
            'core_part': core_part, 
            'private_part': private_part,
            'anonymized_text': anonymized_text
        }
    
    def _normalize_system_name(self, core_text: str) -> str:
        """–£–°–¢–ê–†–ï–õ–û: –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã, –ø—Ä–∏–≤–æ–¥—è –∫ –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω–æ–º—É –ø–∞–¥–µ–∂—É
        
        –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å–∏—Å—Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å
        –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞–¥–µ–∂ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏.
        """
        
        # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –±–æ–ª—å—à–µ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        return core_text
    
    def _simple_partition(self, text: str) -> Optional[Dict[str, Any]]:
        """–ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ò–°"""
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Å–∏—Å—Ç–µ–º—ã
        system_keywords = ['—Å–∏—Å—Ç–µ–º–∞', '—Å–∏—Å—Ç–µ–º—ã', '—Å–∏—Å—Ç–µ–º—É', '—Å–∏—Å—Ç–µ–º–µ', '–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞', '–ø–æ—Ä—Ç–∞–ª']
        
        words = text.split()
        system_word_idx = -1
        
        # –ù–∞–π–¥–µ–º –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
        for i, word in enumerate(words):
            if any(kw in word.lower() for kw in system_keywords):
                system_word_idx = i
                break
        
        if system_word_idx == -1:
            return None
        
        # –†–∞–∑–¥–µ–ª—è–µ–º
        core_part = " ".join(words[:system_word_idx + 1])
        private_part = " ".join(words[system_word_idx + 1:]) if system_word_idx + 1 < len(words) else ""
        
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        if private_part and len(private_part.split()) >= 2:
            anonymized_text = f"{core_part} [SYSTEM_ID]"
        else:
            anonymized_text = core_part
            private_part = ""
        
        return {
            'system_type': 'information_system',
            'core_part': core_part.strip(), 
            'private_part': private_part.strip(),
            'anonymized_text': anonymized_text
        }
    
    def _create_detection_from_span(self, span: Span, method: str, doc: Doc) -> Optional[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–∫—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ –∏–∑ spaCy span"""
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –æ–±—â—É—é –∏ –ø—Ä–∏–≤–∞—Ç–Ω—É—é —á–∞—Å—Ç–∏  
        partition_result = self._simple_partition(span.text)
        
        confidence = 0.85 if method == "phrase_matcher" else 0.8
        
        detection = {
            'category': 'information_system',
            'original_value': span.text,
            'confidence': confidence,
            'position': {
                'start': span.start_char,
                'end': span.end_char
            },
            'method': f'information_system_{method}',
            'uuid': 'placeholder'  # –í—Ä–µ–º–µ–Ω–Ω—ã–π placeholder, UUID –±—É–¥–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ –≤ FormatterApplier
        }
        
        if partition_result:
            detection.update(partition_result)
        else:
            detection.update({
                'system_type': 'information_system',
                'core_part': span.text,
                'private_part': '',
                'anonymized_text': span.text
            })
        
        return detection
    
    def combine_results(self, results_by_method: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏ –ò–°
        """
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã spacy_ner, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Ö —á–µ—Ä–µ–∑ –Ω–∞—à—É –¥–µ—Ç–µ–∫—Ü–∏—é
        all_detections = []
        
        for method_name, detections in results_by_method.items():
            for detection in detections:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–µ—Ç –ª–∏ —ç—Ç–æ –±—ã—Ç—å –ò–°
                text = detection.get('original_value', '')
                if self._might_be_information_system(text):
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—à—É –¥–µ—Ç–µ–∫—Ü–∏—é
                    is_detections = self.detect_information_systems_in_text(text)
                    if is_detections:
                        # –ó–∞–º–µ–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –¥–µ—Ç–µ–∫—Ü–∏—é –Ω–∞ –ò–° –¥–µ—Ç–µ–∫—Ü–∏—é
                        for is_det in is_detections:
                            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏–∏
                            orig_start = detection['position']['start']
                            is_det['position']['start'] += orig_start
                            is_det['position']['end'] += orig_start
                            all_detections.append(is_det)
                    else:
                        all_detections.append(detection)
                else:
                    all_detections.append(detection)
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        return self._remove_duplicates(all_detections, threshold=0.6)
    
    def _might_be_information_system(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –º–æ–∂–µ—Ç –ª–∏ —Ç–µ–∫—Å—Ç –±—ã—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –ò–°"""
        text_lower = text.lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ò–°
        is_keywords = [
            '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞', 
            '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–æ—Ä—Ç–∞–ª', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∫–æ–º–ø–ª–µ–∫—Å',
            '–µ–∏—Å', '–∞–∏—Å', '–≥–∏—Å', '–µ–≥–∏—Å', '–µ–ø–≥—É', '–µ—Å–∏–∞'
        ]
        
        return any(keyword in text_lower for keyword in is_keywords)


class ISPartitioner:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –ò–°"""
    
    def __init__(self, nlp, settings: Dict[str, Any]):
        self.nlp = nlp
        self.settings = settings
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –§–ò–û –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞
"""

import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º NLP
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from nlp_adapter import NLPAdapter

def test_fio_detection():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é –§–ò–û –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    print("=" * 80)
    print("–¢–ï–°–¢ –î–ï–¢–ï–ö–¶–ò–ò –§–ò–û –í –ó–ê–ì–û–õ–û–í–ö–ê–• –î–û–ö–£–ú–ï–ù–¢–ê")
    print("=" * 80)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º NLP –∞–¥–∞–ø—Ç–µ—Ä
    print("\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –∞–¥–∞–ø—Ç–µ—Ä–∞...")
    nlp_adapter = NLPAdapter()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    test_cases = [
        {
            'name': '–§–ò–û 1: –ê. –°. –©—É–∫–∏–Ω–∞',
            'text': '\n______________________ –ê. –°. –©—É–∫–∏–Ω–∞'
        },
        {
            'name': '–§–ò–û 2: –ö. –°. –ú—è—Å–Ω–∏–∫–æ–≤ (—Å –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–º –ø—Ä–æ–±–µ–ª–æ–º)',
            'text': '\n______________________ –ö.\xa0–°. –ú—è—Å–Ω–∏–∫–æ–≤'
        },
        {
            'name': '–§–ò–û 3: –í. –í. –ï–≤—Ç—É—à–µ–Ω–∫–æ',
            'text': '\n____________________ –í. –í. –ï–≤—Ç—É—à–µ–Ω–∫–æ'
        },
        {
            'name': '–ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –©—É–∫–∏–Ω–∞',
            'text': '–ù–∞—á–∞–ª—å–Ω–∏–∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è\n______________________ –ê. –°. –©—É–∫–∏–Ω–∞'
        },
        {
            'name': '–ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ú—è—Å–Ω–∏–∫–æ–≤',
            'text': '–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä\n–û–û–û ¬´–ö–ê–ú–ê –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏¬ª\n______________________ –ö.\xa0–°. –ú—è—Å–Ω–∏–∫–æ–≤'
        },
        {
            'name': '–ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ï–≤—Ç—É—à–µ–Ω–∫–æ',
            'text': '–ì–ª–∞–≤–Ω—ã–π —ç–∫—Å–ø–µ—Ä—Ç —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ —Ä–∞–∑–≤–∏—Ç–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –ì–ë–£ –ü–ö ¬´–¶–µ–Ω—Ç—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ\n____________________ –í. –í. –ï–≤—Ç—É—à–µ–Ω–∫–æ'
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'=' * 80}")
        print(f"–¢–ï–°–¢ {i}: {test_case['name']}")
        print(f"{'=' * 80}")
        print(f"\nüìù –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç:")
        print(f"   {repr(test_case['text'])}")
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑:")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é
        detections = nlp_adapter.find_sensitive_data(test_case['text'])
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ –¥–µ—Ç–µ–∫—Ü–∏–π: {len(detections)}")
        
        if detections:
            for j, detection in enumerate(detections, 1):
                print(f"\n   –î–µ—Ç–µ–∫—Ü–∏—è {j}:")
                print(f"      –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {detection['category']}")
                print(f"      –ó–Ω–∞—á–µ–Ω–∏–µ: '{detection['original_value']}'")
                print(f"      –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {detection['confidence']:.2f}")
                print(f"      –ú–µ—Ç–æ–¥: {detection['method']}")
                print(f"      –ü–æ–∑–∏—Ü–∏—è: {detection['position']}")
        else:
            print("   ‚ùå –ù–ï –ù–ê–ô–î–ï–ù–û!")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        print(f"\nüîç –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º spaCy —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
        doc = nlp_adapter.nlp(test_case['text'])
        print(f"   –¢–æ–∫–µ–Ω—ã spaCy:")
        for token in doc:
            if not token.is_space and not token.is_punct:
                print(f"      '{token.text}' ‚Üí POS: {token.pos_}, IS_TITLE: {token.is_title}, IS_UPPER: {token.is_upper}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º spaCy entities
        print(f"\n   –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ spaCy:")
        if doc.ents:
            for ent in doc.ents:
                print(f"      '{ent.text}' ‚Üí Label: {ent.label_}")
        else:
            print(f"      ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—é –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–æ–≤
        text_clean = test_case['text'].replace('_', '').replace('\n', ' ').strip()
        words = text_clean.split()
        
        print(f"\n   –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–≤:")
        for word in words:
            if len(word) <= 3 and '.' in word:  # –ò–Ω–∏—Ü–∏–∞–ª—ã
                print(f"      '{word}' ‚Üí –∏–Ω–∏—Ü–∏–∞–ª (–≤–æ–∑–º–æ–∂–Ω–æ –§–ò–û)")
                if nlp_adapter.morph:
                    parsed = nlp_adapter.morph.parse(word.replace('.', ''))
                    if parsed:
                        print(f"         pymorphy3: {parsed[0].tag}")
        
        print(f"\n   –ü—Ä–æ–≤–µ—Ä–∫–∞ regex –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:")
        import re
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–æ–≤ + —Ñ–∞–º–∏–ª–∏—è
        pattern = r'\b[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å][–∞-—è—ë]+\b'
        matches = list(re.finditer(pattern, test_case['text']))
        if matches:
            for match in matches:
                print(f"      ‚úÖ Regex –Ω–∞—à–µ–ª: '{match.group()}' –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {match.start()}-{match.end()}")
        else:
            print(f"      ‚ùå Regex –Ω–µ –Ω–∞—à–µ–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º: {pattern}")
            # –ü—Ä–æ–≤–µ—Ä–∏–º —Å –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–º –ø—Ä–æ–±–µ–ª–æ–º
            pattern_nbsp = r'\b[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å][–∞-—è—ë]+\b'
            text_normalized = test_case['text'].replace('\xa0', ' ')
            matches_nbsp = list(re.finditer(pattern_nbsp, text_normalized))
            if matches_nbsp:
                print(f"      ‚ö†Ô∏è –ü–æ—Å–ª–µ –∑–∞–º–µ–Ω—ã \\xa0 –Ω–∞ –ø—Ä–æ–±–µ–ª: –Ω–∞–π–¥–µ–Ω–æ {len(matches_nbsp)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
                for match in matches_nbsp:
                    print(f"         '{match.group()}' –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {match.start()}-{match.end()}")

def analyze_nonbreaking_space():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–±–ª–µ–º—É —Å –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–º –ø—Ä–æ–±–µ–ª–æ–º"""
    print("\n" + "=" * 80)
    print("–ê–ù–ê–õ–ò–ó –ù–ï–†–ê–ó–†–´–í–ù–û–ì–û –ü–†–û–ë–ï–õ–ê (\\xa0)")
    print("=" * 80)
    
    text_with_nbsp = "–ö.\xa0–°. –ú—è—Å–Ω–∏–∫–æ–≤"
    text_with_space = "–ö. –°. –ú—è—Å–Ω–∏–∫–æ–≤"
    
    print(f"\n–¢–µ–∫—Å—Ç —Å –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–º –ø—Ä–æ–±–µ–ª–æ–º: {repr(text_with_nbsp)}")
    print(f"–¢–µ–∫—Å—Ç —Å –æ–±—ã—á–Ω—ã–º –ø—Ä–æ–±–µ–ª–æ–º: {repr(text_with_space)}")
    
    import re
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 1: —Ç–æ–ª—å–∫–æ —Å –æ–±—ã—á–Ω—ã–º –ø—Ä–æ–±–µ–ª–æ–º
    pattern1 = r'\b[–ê-–Ø–Å]\.\s[–ê-–Ø–Å]\.\s[–ê-–Ø–Å][–∞-—è—ë]+\b'
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 2: —Å \s (–¥–æ–ª–∂–µ–Ω –ª–æ–≤–∏—Ç—å –ª—é–±—ã–µ whitespace)
    pattern2 = r'\b[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å][–∞-—è—ë]+\b'
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω 3: —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º \xa0
    pattern3 = r'\b[–ê-–Ø–Å]\.[\s\xa0]*[–ê-–Ø–Å]\.[\s\xa0]*[–ê-–Ø–Å][–∞-—è—ë]+\b'
    
    print(f"\n–ü–∞—Ç—Ç–µ—Ä–Ω 1 (—Ç–æ–ª—å–∫–æ –æ–±—ã—á–Ω—ã–π –ø—Ä–æ–±–µ–ª '\\s'):")
    print(f"   –° \\xa0: {bool(re.search(pattern1, text_with_nbsp))}")
    print(f"   –° –ø—Ä–æ–±–µ–ª–æ–º: {bool(re.search(pattern1, text_with_space))}")
    
    print(f"\n–ü–∞—Ç—Ç–µ—Ä–Ω 2 ('\\s*'):")
    print(f"   –° \\xa0: {bool(re.search(pattern2, text_with_nbsp))}")
    print(f"   –° –ø—Ä–æ–±–µ–ª–æ–º: {bool(re.search(pattern2, text_with_space))}")
    
    print(f"\n–ü–∞—Ç—Ç–µ—Ä–Ω 3 ('[\\s\\xa0]*'):")
    print(f"   –° \\xa0: {bool(re.search(pattern3, text_with_nbsp))}")
    print(f"   –° –ø—Ä–æ–±–µ–ª–æ–º: {bool(re.search(pattern3, text_with_space))}")
    
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–≥–æ, —á—Ç–æ —Å—á–∏—Ç–∞–µ—Ç—Å—è whitespace –≤ Python regex:")
    nbsp_char = '\xa0'
    print(f"   '\\xa0'.isspace() = {nbsp_char.isspace()}")
    nbsp_match = re.match(r'\s', nbsp_char)
    print(f"   re.match(r'\\s', '\\xa0') = {bool(nbsp_match)}")

if __name__ == '__main__':
    test_fio_detection()
    analyze_nonbreaking_space()
    
    print("\n" + "=" * 80)
    print("–í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("=" * 80)
    print("""
    –ü–†–û–ë–õ–ï–ú–ê: –§–ò–û —Å –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–º–∏ –ø—Ä–æ–±–µ–ª–∞–º–∏ (\\xa0) –Ω–µ –¥–µ—Ç–µ–∫—Ç–∏—Ä—É—é—Ç—Å—è
    
    –ü–†–ò–ß–ò–ù–´:
    1. Regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã (\\xa0, U+00A0)
    2. spaCy –º–æ–∂–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å \\xa0
    3. –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    
    –†–ï–®–ï–ù–ò–ï:
    1. –î–æ–±–∞–≤–∏—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –Ω–µ—Ä–∞–∑—Ä—ã–≤–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –≤ TextNormalizer
    2. –û–±–Ω–æ–≤–∏—Ç—å regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É—á–µ—Ç–∞ [\\s\\xa0]
    3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É spaCy —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º
    """)

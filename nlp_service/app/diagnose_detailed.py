#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ PhraseMatcher - –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ –∫–∞–∫–∏–µ —Ñ—Ä–∞–∑—ã –∏—â–µ—Ç –∏ –Ω–∞—Ö–æ–¥–∏—Ç
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'patterns'))

import spacy
from spacy.matcher import PhraseMatcher
from government_organizations import GOVERNMENT_ORGANIZATIONS

def diagnose_phrase_matcher_detailed():
    """–î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ PhraseMatcher"""
    
    print("=" * 80)
    print("üî¨ –î–ï–¢–ê–õ–¨–ù–ê–Ø –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê PHRASEMATCHER")
    print("=" * 80)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å
    nlp = spacy.load("ru_core_news_sm")
    
    # –°–æ–∑–¥–∞–µ–º PhraseMatcher
    phrase_matcher = PhraseMatcher(nlp.vocab, attr="LOWER")
    
    print(f"üìö –°–õ–û–í–ê–†–¨ –°–û–î–ï–†–ñ–ò–¢ {len(GOVERNMENT_ORGANIZATIONS)} –û–†–ì–ê–ù–ò–ó–ê–¶–ò–ô:")
    print("-" * 60)
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–∑—ã –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Ö
    patterns = []
    for i, org in enumerate(GOVERNMENT_ORGANIZATIONS, 1):
        pattern_doc = nlp(org.lower())
        patterns.append(pattern_doc)
        word_count = len(org.split())
        print(f"   {i:2d}. '{org}' ({word_count} —Å–ª–æ–≤)")
        if word_count >= 6:
            print(f"       ‚úÖ –î–õ–ò–ù–ù–û–ï –ù–ê–ó–í–ê–ù–ò–ï - –ü–†–ò–û–†–ò–¢–ï–¢")
        elif word_count >= 4:
            print(f"       üü° –°–†–ï–î–ù–ï–ï –ù–ê–ó–í–ê–ù–ò–ï")
        else:
            print(f"       üî∏ –ö–û–†–û–¢–ö–û–ï –ù–ê–ó–í–ê–ù–ò–ï")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ matcher
    phrase_matcher.add("government_org", patterns)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏
    test_text = "–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø –ò –°–í–Ø–ó–ò\n–ü–ï–†–ú–°–ö–û–ì–û –ö–†–ê–Ø"
    normalized_text = "–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø –ò –°–í–Ø–ó–ò –ü–ï–†–ú–°–ö–û–ì–û –ö–†–ê–Ø"
    
    print(f"\nüìù –ò–°–•–û–î–ù–´–ô –¢–ï–ö–°–¢:")
    print(f"   '{test_text}'")
    print(f"\nüìù –ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–´–ô –¢–ï–ö–°–¢:")
    print(f"   '{normalized_text}'")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞
    for text_type, text in [("–ò–°–•–û–î–ù–´–ô", test_text), ("–ù–û–†–ú–ê–õ–ò–ó–û–í–ê–ù–ù–´–ô", normalized_text)]:
        print(f"\nüîç –ê–ù–ê–õ–ò–ó {text_type} –¢–ï–ö–°–¢–ê:")
        print("-" * 60)
        
        doc = nlp(text.lower())
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
        print("üéØ –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø:")
        for i, token in enumerate(doc):
            token_type = "SPACE" if token.is_space else "WORD"
            print(f"   {i}: '{token.text}' ({token_type})")
        
        # –ò—â–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        matches = phrase_matcher(doc)
        
        print(f"\nüéØ –ù–ê–ô–î–ï–ù–ù–´–ï –°–û–í–ü–ê–î–ï–ù–ò–Ø: {len(matches)}")
        
        if matches:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (—Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Å–Ω–∞—á–∞–ª–∞)
            matches_with_info = []
            for match_id, start, end in matches:
                matched_text = doc[start:end].text
                char_start = doc[start].idx
                char_end = doc[end-1].idx + len(doc[end-1].text)
                matches_with_info.append({
                    'text': matched_text,
                    'start': start,
                    'end': end,
                    'char_start': char_start,
                    'char_end': char_end,
                    'length': len(matched_text.split())
                })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (—Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Å–Ω–∞—á–∞–ª–∞)
            matches_with_info.sort(key=lambda x: x['length'], reverse=True)
            
            for i, match_info in enumerate(matches_with_info, 1):
                print(f"   {i}. '{match_info['text']}' (—Ç–æ–∫–µ–Ω—ã {match_info['start']}-{match_info['end']}, {match_info['length']} —Å–ª–æ–≤)")
                print(f"      –ü–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç–µ: {match_info['char_start']}-{match_info['char_end']}")
                
                # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Ñ—Ä–∞–∑—É –≤ —Å–ª–æ–≤–∞—Ä–µ
                for org in GOVERNMENT_ORGANIZATIONS:
                    if org.lower() == match_info['text']:
                        print(f"      ‚úÖ –¢–û–ß–ù–û–ï –°–û–û–¢–í–ï–¢–°–¢–í–ò–ï: '{org}'")
                        break
                else:
                    print(f"      ‚ùì –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∏–ª–∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç—å")
        else:
            print("   ‚ùå –ù–ò –û–î–ù–û–ì–û –°–û–í–ü–ê–î–ï–ù–ò–Ø –ù–ï –ù–ê–ô–î–ï–ù–û")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ —Å–ª–æ–≤–∞—Ä–µ –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
    print(f"\nüéØ –ü–†–û–í–ï–†–ö–ê –°–õ–û–í–ê–†–Ø –ù–ê –ü–û–õ–ù–û–ï –ù–ê–ó–í–ê–ù–ò–ï:")
    print("-" * 60)
    
    full_name = "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ø–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è"
    partial_name = "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è"
    
    full_found = any(org.lower() == full_name for org in GOVERNMENT_ORGANIZATIONS)
    partial_found = any(org.lower() == partial_name for org in GOVERNMENT_ORGANIZATIONS)
    
    print(f"   –ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ '{full_name}': {'‚úÖ –ï–°–¢–¨' if full_found else '‚ùå –ù–ï–¢'}")
    print(f"   –ß–∞—Å—Ç–∏—á–Ω–æ–µ '{partial_name}': {'‚úÖ –ï–°–¢–¨' if partial_found else '‚ùå –ù–ï–¢'}")
    
    if full_found and partial_found:
        print("   ‚ö†Ô∏è –ö–û–ù–§–õ–ò–ö–¢: –ò –ø–æ–ª–Ω–æ–µ, –∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏—è –≤ —Å–ª–æ–≤–∞—Ä–µ!")
        print("   üí° PhraseMatcher –º–æ–∂–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –ø–µ—Ä–≤–æ–µ –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω–æ–µ (–±–æ–ª–µ–µ –∫–æ—Ä–æ—Ç–∫–æ–µ)")
    elif full_found:
        print("   ‚úÖ –•–û–†–û–®–û: –¢–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä–µ")
    elif partial_found:
        print("   ‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –¢–æ–ª—å–∫–æ —á–∞—Å—Ç–∏—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ —Å–ª–æ–≤–∞—Ä–µ")
    else:
        print("   ‚ùå –û–®–ò–ë–ö–ê: –ù–∏ –ø–æ–ª–Ω–æ–≥–æ, –Ω–∏ —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ")


if __name__ == "__main__":
    diagnose_phrase_matcher_detailed()
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç —É–º–Ω–æ–≥–æ PhraseMatcher —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –ø–æ –¥–ª–∏–Ω–µ
"""

import sys
import os
sys.path.append(os.path.dirname(__file__))

from nlp_adapter import NLPAdapter
from smart_phrase_matcher import SmartPhraseMatcher

def test_smart_phrase_matcher():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–º–Ω—ã–π PhraseMatcher —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π –ø–æ –¥–ª–∏–Ω–µ"""
    
    print("=" * 90)
    print("üß† –¢–ï–°–¢ –£–ú–ù–û–ì–û PHRASE MATCHER –° –ü–†–ò–û–†–ò–¢–ò–ó–ê–¶–ò–ï–ô –ü–û –î–õ–ò–ù–ï")
    print("=" * 90)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–¥–∞–ø—Ç–µ—Ä
    adapter = NLPAdapter()
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º–∏
    test_cases = [
        {
            'name': '–ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏',
            'text': """
            –°–æ–≥–ª–∞—Å–Ω–æ –ø–∏—Å—å–º—É –æ—Ç –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø –ò –°–í–Ø–ó–ò
            –ü–ï–†–ú–°–ö–û–ì–û –ö–†–ê–Ø –æ—Ç 15.01.2024 ‚Ññ 123 —Ç—Ä–µ–±—É–µ—Ç—Å—è...
            """,
            'expected_full': ['–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø –ò –°–í–Ø–ó–ò –ü–ï–†–ú–°–ö–û–ì–û –ö–†–ê–Ø'],
            'expected_partial': ['–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø']
        },
        {
            'name': '–î–≤–∞ –ø–æ–ª–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏—è',
            'text': """
            –î–ï–ü–ê–†–¢–ê–ú–ï–ù–¢ –û–ë–†–ê–ó–û–í–ê–ù–ò–Ø –ò –ù–ê–£–ö–ò –ö–ò–†–û–í–°–ö–û–ô –û–ë–õ–ê–°–¢–ò –∏ 
            –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ó–î–†–ê–í–û–û–•–†–ê–ù–ï–ù–ò–Ø –ü–ï–†–ú–°–ö–û–ì–û –ö–†–ê–Ø —Å–æ–≤–º–µ—Å—Ç–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏...
            """,
            'expected_full': [
                '–î–ï–ü–ê–†–¢–ê–ú–ï–ù–¢ –û–ë–†–ê–ó–û–í–ê–ù–ò–Ø –ò –ù–ê–£–ö–ò –ö–ò–†–û–í–°–ö–û–ô –û–ë–õ–ê–°–¢–ò',
                '–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ó–î–†–ê–í–û–û–•–†–ê–ù–ï–ù–ò–Ø –ü–ï–†–ú–°–ö–û–ì–û –ö–†–ê–Ø'
            ],
            'expected_partial': []
        },
        {
            'name': '–°–º–µ—à–∞–Ω–Ω—ã–µ —Å–ª—É—á–∞–∏',
            'text': """
            –£—á–∞—Å—Ç–≤–æ–≤–∞–ª–∏:
            1. –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –§–ò–ù–ê–ù–°–û–í
            2. –î–ï–ü–ê–†–¢–ê–ú–ï–ù–¢ –û–ë–†–ê–ó–û–í–ê–ù–ò–Ø –ò –ù–ê–£–ö–ò –ö–ò–†–û–í–°–ö–û–ô –û–ë–õ–ê–°–¢–ò  
            3. –£–ü–†–ê–í–õ–ï–ù–ò–ï –î–ï–õ–ê–ú–ò –ü–†–ê–í–ò–¢–ï–õ–¨–°–¢–í–ê
            """,
            'expected_full': ['–î–ï–ü–ê–†–¢–ê–ú–ï–ù–¢ –û–ë–†–ê–ó–û–í–ê–ù–ò–Ø –ò –ù–ê–£–ö–ò –ö–ò–†–û–í–°–ö–û–ô –û–ë–õ–ê–°–¢–ò'],
            'expected_partial': ['–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –§–ò–ù–ê–ù–°–û–í', '–£–ü–†–ê–í–õ–ï–ù–ò–ï –î–ï–õ–ê–ú–ò –ü–†–ê–í–ò–¢–ï–õ–¨–°–¢–í–ê']
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*60}")
        print(f"üìã –¢–ï–°–¢ {i}: {test_case['name']}")
        print(f"{'='*60}")
        
        text = test_case['text']
        print(f"üìù –ò–°–•–û–î–ù–´–ô –¢–ï–ö–°–¢:")
        print(repr(text))
        print()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
        print("üîç –û–ë–†–ê–ë–û–¢–ö–ê –¢–ï–ö–°–¢–ê:")
        print("-" * 40)
        results = adapter.find_sensitive_data(text)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        gov_orgs = [r for r in results if r['category'] == 'government_org']
        
        print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π: {len(gov_orgs)}")
        
        full_matches = []
        partial_matches = []
        
        for j, result in enumerate(gov_orgs, 1):
            start = result['position']['start']
            end = result['position']['end']
            found_text = text[start:end]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É–º–Ω–æ–≥–æ –º–∞—Ç—á–µ—Ä–∞
            is_partial = result.get('additional_info', {}).get('is_partial_match', False)
            phrase_length = result.get('additional_info', {}).get('phrase_length', 0)
            smart_match = result.get('additional_info', {}).get('smart_match', False)
            
            print(f"   {j}. '{result['original_value']}'")
            print(f"      üìç –ü–æ–∑–∏—Ü–∏—è: {start}-{end}")
            print(f"      üéØ –ú–µ—Ç–æ–¥: {result['method']}")
            print(f"      üìä Confidence: {result['confidence']:.3f}")
            print(f"      üìè –î–ª–∏–Ω–∞ —Ñ—Ä–∞–∑—ã: {phrase_length} —Å–ª–æ–≤")
            print(f"      üß† –£–º–Ω—ã–π –º–∞—Ç—á–µ—Ä: {smart_match}")
            print(f"      ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ–µ: {is_partial}")
            print(f"      üìÑ –†–µ–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç: '{found_text}'")
            print()
            
            if phrase_length >= 5 or not is_partial:
                full_matches.append(result['original_value'])
            else:
                partial_matches.append(result['original_value'])
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("üìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
        print("-" * 40)
        print(f"üèÜ –ü–æ–ª–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: {len(full_matches)}")
        for match in full_matches:
            print(f"   ‚úÖ '{match}' ({len(match.split())} —Å–ª–æ–≤)")
        
        print(f"‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: {len(partial_matches)}")
        for match in partial_matches:
            print(f"   üî∏ '{match}' ({len(match.split())} —Å–ª–æ–≤)")
        
        # –û—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        expected_full = len(test_case['expected_full'])
        expected_partial = len(test_case['expected_partial'])
        found_full = len(full_matches)
        found_partial = len(partial_matches)
        
        print(f"\nüéØ –û–ñ–ò–î–ê–ï–ú–û–ï vs –ù–ê–ô–î–ï–ù–ù–û–ï:")
        print(f"   –ü–æ–ª–Ω—ã—Ö: –æ–∂–∏–¥–∞–ª–∏ {expected_full}, –Ω–∞—à–ª–∏ {found_full}")
        print(f"   –ß–∞—Å—Ç–∏—á–Ω—ã—Ö: –æ–∂–∏–¥–∞–ª–∏ {expected_partial}, –Ω–∞—à–ª–∏ {found_partial}")
        
        # –û—Ü–µ–Ω–∫–∞
        if found_full >= expected_full and found_partial <= expected_partial:
            print("üéâ –û–¢–õ–ò–ß–ù–û: –ë–æ–ª—å—à–µ –ø–æ–ª–Ω—ã—Ö, –º–µ–Ω—å—à–µ —á–∞—Å—Ç–∏—á–Ω—ã—Ö!")
        elif found_full > found_partial:
            print("‚úÖ –•–û–†–û–®–û: –ü–æ–ª–Ω—ã—Ö –±–æ–ª—å—à–µ —á–µ–º —á–∞—Å—Ç–∏—á–Ω—ã—Ö")
        elif len(gov_orgs) > 0:
            print("‚ö†Ô∏è –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û: –ù–∞–π–¥–µ–Ω—ã –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏")
        else:
            print("‚ùå –ù–ï–£–î–ê–ß–ê: –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # –û–±—â–∏–π –∏—Ç–æ–≥
    print(f"\n{'='*90}")
    print("üéØ –û–ë–©–ò–ô –ò–¢–û–ì –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –£–ú–ù–û–ì–û PHRASE MATCHER")
    print(f"{'='*90}")
    
    print("‚úÖ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ —É–º–Ω–æ–≥–æ –º–∞—Ç—á–µ—Ä–∞:")
    print("   ‚Ä¢ –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –¥–ª–∏–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑ –Ω–∞–¥ –∫–æ—Ä–æ—Ç–∫–∏–º–∏")
    print("   ‚Ä¢ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏—Ö—Å—è —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
    print("   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–∏—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è")
    print("   ‚Ä¢ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫")
    print("   ‚Ä¢ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –∫–∞—á–µ—Å—Ç–≤–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")


def test_direct_smart_matcher():
    """–ü—Ä—è–º–æ–π —Ç–µ—Å—Ç SmartPhraseMatcher"""
    
    print(f"\n{'='*90}")
    print("üîß –ü–†–Ø–ú–û–ô –¢–ï–°–¢ SMART PHRASE MATCHER")
    print(f"{'='*90}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å
    import spacy
    nlp = spacy.load("ru_core_news_sm")
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    test_patterns = {
        'government_org': [
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏ –ü–µ—Ä–º—Å–∫–æ–≥–æ –∫—Ä–∞—è",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –∏ —Å–≤—è–∑–∏",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è",
            "–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏ –ö–∏—Ä–æ–≤—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—É–∫–∏",
            "–î–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"
        ]
    }
    
    # –°–æ–∑–¥–∞–µ–º —É–º–Ω—ã–π –º–∞—Ç—á–µ—Ä
    smart_matcher = SmartPhraseMatcher(
        nlp=nlp,
        patterns_dict=test_patterns,
        category='government_org'
    )
    
    # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
    test_text = "–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø –ò –°–í–Ø–ó–ò –ü–ï–†–ú–°–ö–û–ì–û –ö–†–ê–Ø –Ω–∞–ø—Ä–∞–≤–∏–ª–æ –ø–∏—Å—å–º–æ"
    
    print(f"üìù –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç: '{test_text}'")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
    doc = nlp(test_text)
    matches = smart_matcher.find_matches(doc)
    
    print(f"\nüîç –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len(matches)}")
    
    for i, match in enumerate(matches, 1):
        print(f"   {i}. '{match.text}'")
        print(f"      üìç –ü–æ–∑–∏—Ü–∏—è: {match.start}-{match.end}")
        print(f"      üìè –î–ª–∏–Ω–∞: {match.length} —Å–ª–æ–≤")
        print(f"      üìä Confidence: {match.confidence:.3f}")
        print(f"      ‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ–µ: {match.is_partial}")
        print(f"      üéØ –ü–∞—Ç—Ç–µ—Ä–Ω: '{match.pattern_id}'")


if __name__ == "__main__":
    test_smart_phrase_matcher()
    test_direct_smart_matcher()
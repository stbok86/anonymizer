#!/usr/bin/env python3
"""
NLP Adapter –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
–§–æ–∫—É—Å –Ω–∞ spaCy NER + –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–∞—Ç—á–µ—Ä—ã + –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
"""

import re
import os
import uuid
from typing import Dict, List, Any, Optional, Set
import pandas as pd
import spacy
from spacy.matcher import Matcher, PhraseMatcher
from spacy.tokens import Doc, Token
import pymorphy3

try:
    from nlp_config import NLPConfig
    from detection_strategies import DetectionStrategyFactory
    from detection_factory import DetectionMethodFactory
    from text_normalizer import TextNormalizer
    from smart_phrase_matcher import SmartPhraseMatcher
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(__file__))
    from nlp_config import NLPConfig
    from detection_strategies import DetectionStrategyFactory
    from detection_factory import DetectionMethodFactory
    from text_normalizer import TextNormalizer
    from smart_phrase_matcher import SmartPhraseMatcher


class NLPAdapter:
    """
    –ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
    """
    
    def __init__(self, config_path: Optional[str] = None, patterns_file: Optional[str] = None, confidence_threshold: Optional[float] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –∞–¥–∞–ø—Ç–µ—Ä–∞
        
        Args:
            config_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
            patterns_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            confidence_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏. –ï—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        """
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.config = NLPConfig(config_path)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞
        self.text_normalizer = TextNormalizer()
        
        # –°–ª–æ–≤–∞—Ä—å —É–º–Ω—ã—Ö phrase –º–∞—Ç—á–µ—Ä–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        self.smart_phrase_matchers = {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        self.nlp = None
        self.matcher = None
        self.phrase_matcher = None
        self.morph = None  # pymorphy3 –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        self.patterns = {}
        self.pattern_configs = {}
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        self.confidence_threshold = confidence_threshold or self.config.get_global_confidence_threshold()
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è PhraseMatcher
        self.custom_phrases = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º spaCy –º–æ–¥–µ–ª—å
        self._load_spacy_model()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        if self.config.is_morphology_enabled():
            self._init_morphology()
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns_file_path = patterns_file or self.config.get_patterns_file_path()
        self._load_patterns(patterns_file_path)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –º–∞—Ç—á–µ—Ä—ã
        self._setup_matchers()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –¥–µ—Ç–µ–∫—Ü–∏–∏
        self.detection_factory = DetectionMethodFactory(self.config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ –∫–µ—à–∏—Ä—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º
        self._is_strategy = None
        self._init_information_system_strategy()
    
    def _load_spacy_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä—É—Å—Å–∫—É—é spaCy –º–æ–¥–µ–ª—å —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        preferred_models = self.config.get_spacy_models()
        fallback_error = self.config.get_spacy_fallback_error()
        
        for model_name in preferred_models:
            try:
                self.nlp = spacy.load(model_name)
                if self.config.should_log_model_loading():
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Ä—É—Å—Å–∫–∞—è spaCy –º–æ–¥–µ–ª—å: {model_name}")
                return
            except OSError:
                continue
        
        # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
        raise RuntimeError(fallback_error)
    
    def _init_morphology(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä pymorphy3"""
        try:
            self.morph = pymorphy3.MorphAnalyzer()
            if self.config.should_log_model_loading():
                print("‚úÖ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä pymorphy3 –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å pymorphy3: {e}")
            self.morph = None
    
    def _load_patterns(self, patterns_file: str):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ Excel —Ñ–∞–π–ª–∞
        
        Args:
            patterns_file: –ü—É—Ç—å –∫ Excel —Ñ–∞–π–ª—É —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
        """
        if not os.path.exists(patterns_file):
            raise FileNotFoundError(f"–§–∞–π–ª –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {patterns_file}")
        
        try:
            df = pd.read_excel(patterns_file)
            
            for _, row in df.iterrows():
                category = row['category']
                pattern = row['pattern']
                pattern_type = row['pattern_type']
                confidence = float(row.get('confidence', self.config.get_default_pattern_confidence()))
                context_required = bool(row.get('context_required', True))
                description = row.get('description', '')
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å–∏ —Å –ø—É—Å—Ç—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º (NaN)
                if pd.isna(pattern) and pattern_type == 'regex':
                    continue
                
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
                if category not in self.patterns:
                    self.patterns[category] = []
                    self.pattern_configs[category] = []
                
                self.patterns[category].append(pattern)
                self.pattern_configs[category].append({
                    'pattern': pattern,
                    'pattern_type': pattern_type,
                    'confidence': confidence,
                    'context_required': context_required,
                    'description': description
                })
            
            if self.config.should_log_pattern_loading():
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} NLP –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ {patterns_file}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")
            raise
    
    def _setup_matchers(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç spaCy Matcher –∏ PhraseMatcher —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏"""
        self.matcher = Matcher(self.nlp.vocab)
        self.phrase_matcher = PhraseMatcher(self.nlp.vocab, attr=self.config.get_phrase_matcher_attr())
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –º–∞—Ç—á–µ—Ä—ã
        for category, patterns in self.patterns.items():
            for i, pattern_text in enumerate(patterns):
                config = self.pattern_configs[category][i]
                
                if config['pattern_type'] == 'regex':
                    # –î–ª—è regex –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                    continue
                elif config['pattern_type'] == 'spacy_context':
                    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                    self._add_context_patterns(category, pattern_text)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        self._setup_custom_phrases()
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ Matcher –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        self._setup_custom_matchers()
    
    def _setup_custom_phrases(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç PhraseMatcher –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        
        # –î–æ–ª–∂–Ω–æ—Å—Ç–∏ –∏ —Ä–æ–ª–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫)
        positions = [
            # –†—É–∫–æ–≤–æ–¥—è—â–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "–¥–∏—Ä–µ–∫—Ç–æ—Ä",
            "–∑–∞–º–µ—Å—Ç–∏—Ç–µ–ª—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞", "–∑–∞–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞", "–∑–∞–º. –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞",
            "–Ω–∞—á–∞–ª—å–Ω–∏–∫", "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å", "–∑–∞–≤–µ–¥—É—é—â–∏–π", "—É–ø—Ä–∞–≤–ª—è—é—â–∏–π",
            
            # –ú–µ–Ω–µ–¥–∂–º–µ–Ω—Ç
            "–º–µ–Ω–µ–¥–∂–µ—Ä", "—Å—Ç–∞—Ä—à–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä", "–≤–µ–¥—É—â–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä",
            "–º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º", "–º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–ª—É", "hr –º–µ–Ω–µ–¥–∂–µ—Ä",
            "–ø—Ä–æ–µ–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä", "–ø—Ä–æ–¥–∞–∫—Ç –º–µ–Ω–µ–¥–∂–µ—Ä",
            
            # –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã
            "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç", "–≤–µ–¥—É—â–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç", "—Å—Ç–∞—Ä—à–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
            "–≥–ª–∞–≤–Ω—ã–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç", "–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç", "—ç–∫—Å–ø–µ—Ä—Ç", "–∞–Ω–∞–ª–∏—Ç–∏–∫",
            
            # IT –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç", "—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", "—Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä",
            "—Å–∏—Å–∞–¥–º–∏–Ω", "devops", "—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫", "–∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö",
            "–∞—Ä—Ö–∏—Ç–µ–∫—Ç–æ—Ä", "—Ç–µ—Ö–ª–∏–¥", "team lead",
            
            # –§–∏–Ω–∞–Ω—Å—ã –∏ —É—á–µ—Ç
            "–±—É—Ö–≥–∞–ª—Ç–µ—Ä", "–≥–ª–∞–≤–Ω—ã–π –±—É—Ö–≥–∞–ª—Ç–µ—Ä", "—ç–∫–æ–Ω–æ–º–∏—Å—Ç", "—Ñ–∏–Ω–∞–Ω—Å–∏—Å—Ç",
            "–∫–∞–∑–Ω–∞—á–µ–π", "–∞—É–¥–∏—Ç–æ—Ä", "–∫–æ–Ω—Ç—Ä–æ–ª–µ—Ä",
            
            # –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏
            "—é—Ä–∏—Å—Ç", "–∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–π —é—Ä–∏—Å—Ç", "–ø—Ä–∞–≤–æ–≤–µ–¥", "—é—Ä–∏—Å–∫–æ–Ω—Å—É–ª—å—Ç",
            
            # –ö–∞–¥—Ä—ã –∏ HR
            "–∫–∞–¥—Ä–æ–≤–∏–∫", "hr", "—Ä–µ–∫—Ä—É—Ç–µ—Ä", "—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –∫–∞–¥—Ä–∞–º",
            
            # –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ
            "–∏–Ω–∂–µ–Ω–µ—Ä", "–≥–ª–∞–≤–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä", "—Ç–µ—Ö–Ω–æ–ª–æ–≥", "–º–∞—Å—Ç–µ—Ä",
            "–Ω–∞—á–∞–ª—å–Ω–∏–∫ —Å–º–µ–Ω—ã", "—Å—É–ø–µ—Ä–≤–∞–π–∑–µ—Ä", "–æ–ø–µ—Ä–∞—Ç–æ—Ä",
            
            # –ü—Ä–æ—á–µ–µ
            "—Å–µ–∫—Ä–µ—Ç–∞—Ä—å", "–ø–æ–º–æ—â–Ω–∏–∫", "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä",
            "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä", "–¥–µ–ª–æ–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å"
        ]
        
        # –¢–∏–ø—ã –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
        organization_types = [
            "–æ–±—â–µ—Å—Ç–≤–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å—é", "–æ–æ–æ",
            "–∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ", "–∞–æ", "–ø–∞–æ", "–∑–∞–æ",
            "–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "–∏–ø",
            "–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ —É–Ω–∏—Ç–∞—Ä–Ω–æ–µ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ", "–≥—É–ø",
            "–º—É–Ω–∏—Ü–∏–ø–∞–ª—å–Ω–æ–µ —É–Ω–∏—Ç–∞—Ä–Ω–æ–µ –ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ", "–º—É–ø",
            "–Ω–µ–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è", "–Ω–∫–æ",
            "–∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è –Ω–µ–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è", "–∞–Ω–æ",
            "—É—á—Ä–µ–∂–¥–µ–Ω–∏–µ", "–±—é–¥–∂–µ—Ç–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ", "–∫–∞–∑–µ–Ω–Ω–æ–µ —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ"
        ]
        
        # –ü–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
        departments = [
            "–æ—Ç–¥–µ–ª", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç", "—Å–ª—É–∂–±–∞", "—Å–µ–∫—Ç–æ—Ä",
            "–≥—Ä—É–ø–ø–∞", "–∫–æ–º–∞–Ω–¥–∞", "–ø–æ–¥—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ", "—Ñ–∏–ª–∏–∞–ª", "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ",
            "–æ—Ç–¥–µ–ª –∫–∞–¥—Ä–æ–≤", "–±—É—Ö–≥–∞–ª—Ç–µ—Ä–∏—è", "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –æ—Ç–¥–µ–ª",
            "it –æ—Ç–¥–µ–ª", "–æ—Ç–¥–µ–ª –ø—Ä–æ–¥–∞–∂", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "pr –æ—Ç–¥–µ–ª"
        ]
        
        # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        financial_terms = [
            "–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞", "–∑–∞—Ä–ø–ª–∞—Ç–∞", "–æ–∫–ª–∞–¥", "–¥–æ—Ö–æ–¥", "–∑–∞—Ä–∞–±–æ—Ç–æ–∫",
            "–≤—ã–ø–ª–∞—Ç–∞", "–ø—Ä–µ–º–∏—è", "–±–æ–Ω—É—Å", "–Ω–∞–¥–±–∞–≤–∫–∞", "–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏—è",
            "—Å—Ç–∏–ø–µ–Ω–¥–∏—è", "–ø–µ–Ω—Å–∏—è", "–ø–æ—Å–æ–±–∏–µ", "—Å—É–±—Å–∏–¥–∏—è"
        ]
        
        # –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        medical_terms = [
            "–¥–∏–∞–≥–Ω–æ–∑", "–∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ", "–±–æ–ª–µ–∑–Ω—å", "–ª–µ—á–µ–Ω–∏–µ", "—Ç–µ—Ä–∞–ø–∏—è",
            "–æ–ø–µ—Ä–∞—Ü–∏—è", "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞", "–∞–Ω–∞–ª–∏–∑", "–æ–±—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
            "–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –∫–∞—Ä—Ç–∞", "–∏—Å—Ç–æ—Ä–∏—è –±–æ–ª–µ–∑–Ω–∏", "—Ä–µ—Ü–µ–ø—Ç"
        ]
        
        # –ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        confidential_terms = [
            "–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∞—è —Ç–∞–π–Ω–∞", "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ", "—Å–µ–∫—Ä–µ—Ç–Ω–æ",
            "–Ω–æ—É-—Ö–∞—É", "—Å–ª—É–∂–µ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", "—á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"
        ]
        
        # –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ - –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
        government_orgs = self._load_government_organizations()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–∑—ã –≤ PhraseMatcher
        phrase_categories = {
            'position': positions,
            'organization': organization_types,
            'department': departments,
            'salary': financial_terms,
            'health_info': medical_terms,
            'trade_secret': confidential_terms,
            'government_org': government_orgs  # –ù–æ–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª—è –≥–æ—Å–æ—Ä–≥–∞–Ω–æ–≤
        }
        
        for category, phrases in phrase_categories.items():
            if phrases:
                # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Ñ—Ä–∞–∑
                phrase_docs = [self.nlp(phrase) for phrase in phrases]
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ matcher
                self.phrase_matcher.add(f"{category}_phrases", phrase_docs)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
                self.custom_phrases[category] = phrases
        
        print(f"‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω–æ {len(phrase_categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ñ—Ä–∞–∑")
        
        # –°–æ–∑–¥–∞–µ–º —É–º–Ω—ã–µ phrase –º–∞—Ç—á–µ—Ä—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –±–æ–ª—å—à–∏–º–∏ —Å–ª–æ–≤–∞—Ä—è–º–∏
        self._setup_smart_phrase_matchers()
    
    def _setup_smart_phrase_matchers(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç —É–º–Ω—ã–µ phrase –º–∞—Ç—á–µ—Ä—ã –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        try:
            # –°–æ–∑–¥–∞–µ–º —É–º–Ω—ã–π –º–∞—Ç—á–µ—Ä –¥–ª—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
            if 'government_org' in self.custom_phrases:
                gov_org_patterns = {'government_org': self.custom_phrases['government_org']}
                
                self.smart_phrase_matchers['government_org'] = SmartPhraseMatcher(
                    nlp=self.nlp,
                    patterns_dict=gov_org_patterns,
                    category='government_org'
                )
                
                print(f"‚úÖ –°–æ–∑–¥–∞–Ω —É–º–Ω—ã–π –º–∞—Ç—á–µ—Ä –¥–ª—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π ({len(self.custom_phrases['government_org'])} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤)")
            
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —É–º–Ω—ã–µ –º–∞—Ç—á–µ—Ä—ã –¥–ª—è –¥—Ä—É–≥–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —É–º–Ω—ã—Ö –º–∞—Ç—á–µ—Ä–æ–≤: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –±–µ–∑ —É–º–Ω—ã—Ö –º–∞—Ç—á–µ—Ä–æ–≤
    
    def _load_government_organizations(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–∞ government_organizations.py
            import sys
            patterns_dir = os.path.join(os.path.dirname(__file__), '..', 'patterns')
            sys.path.insert(0, patterns_dir)
            
            from government_organizations import GOVERNMENT_ORGANIZATIONS
            
            if self.config.should_log_pattern_loading():
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(GOVERNMENT_ORGANIZATIONS)} –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π")
            
            return GOVERNMENT_ORGANIZATIONS
            
        except ImportError:
            # Fallback - –±–∞–∑–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            fallback_orgs = [
                "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ", "–¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç", "—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "—Å–ª—É–∂–±–∞", "–∫–æ–º–∏—Ç–µ—Ç",
                "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü–∏—è", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–¥—É–º–∞", "—Å–æ–≤–µ—Ç",
                "—Ñ–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Å–ª—É–∂–±–∞", "–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –¥—É–º–∞", 
                "—Å–æ–≤–µ—Ç —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏"
            ]
            
            if self.config.should_log_pattern_loading():
                print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback —Å–ø–∏—Å–æ–∫ ({len(fallback_orgs)} –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π)")
            
            return fallback_orgs
    
    def _setup_custom_matchers(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ Matcher –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –§–ò–û: PROPN PROPN PROPN (–§–∞–º–∏–ª–∏—è –ò–º—è –û—Ç—á–µ—Å—Ç–≤–æ)
        fio_pattern = [
            {"POS": "PROPN", "IS_TITLE": True},
            {"POS": "PROPN", "IS_TITLE": True},
            {"POS": "PROPN", "IS_TITLE": True}
        ]
        self.matcher.add("full_name", [fio_pattern])
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–Ω—ã—Ö –∏–º–µ–Ω: PROPN "." PROPN "." (–ò.–û.)
        initials_pattern = [
            {"POS": "PROPN", "LENGTH": 1},
            {"TEXT": "."},
            {"POS": "PROPN", "LENGTH": 1},
            {"TEXT": "."},
            {"POS": "PROPN", "IS_TITLE": True}
        ]
        self.matcher.add("initials_lastname", [initials_pattern])
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π —Å —é—Ä.—Ñ–æ—Ä–º–æ–π
        org_pattern = [
            {"LOWER": {"IN": ["–æ–æ–æ", "–∞–æ", "–ø–∞–æ", "–∑–∞–æ", "–∏–ø", "–≥—É–ø", "–º—É–ø"]}},
            {"TEXT": {"IN": ["\"", "¬´", "'"]}},
            {"POS": {"IN": ["NOUN", "PROPN"]}, "OP": "+"},
            {"TEXT": {"IN": ["\"", "¬ª", "'"]}},
        ]
        self.matcher.add("quoted_organization", [org_pattern])
        
        print("‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω—ã –∫–∞—Å—Ç–æ–º–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ Matcher")
    
    def _add_context_patterns(self, category: str, pattern_text: str):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –º–∞—Ç—á–µ—Ä—ã"""
        # –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ —Ç–µ–ø–µ—Ä—å —É–ø—Ä–æ—â–µ–Ω, —Ç–∞–∫ –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –≤ _setup_custom_phrases
        pass
    
    def find_sensitive_data(self, text: str) -> List[Dict[str, Any]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ç–µ–∫—Å—Ç–µ –∏—Å–ø–æ–ª—å–∑—É—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –ª–æ–≥–∏–∫—É
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        """
        if not text or not isinstance(text, str):
            print(f"üö´ –ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç: {repr(text)}")
            return []

        print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–æ–π {len(text)} —Å–∏–º–≤–æ–ª–æ–≤: '{text[:50]}...'")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è mapping –ø–æ–∑–∏—Ü–∏–π
        original_text = text
        
        # –î–õ–Ø –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–• –°–ò–°–¢–ï–ú –ù–ï –ò–°–ü–û–õ–¨–ó–£–ï–ú –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Æ
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –º–æ–∂–µ—Ç –Ω–∞—Ä—É—à–∏—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –∑–∞–º–µ–Ω—ã
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞–ø—Ä—è–º—É—é
        processing_text = original_text
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ spaCy –æ–¥–∏–Ω —Ä–∞–∑ –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
        doc = self.nlp(processing_text)
        print(f"üìù spaCy –æ–±—Ä–∞–±–æ—Ç–∞–ª {len(doc)} —Ç–æ–∫–µ–Ω–æ–≤")
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        available_categories = self.config.get_available_categories()
        print(f"üéØ –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {available_categories}")
        
        all_detections = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –æ—Ç–¥–µ–ª—å–Ω–æ —Å –µ—ë –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        for category in available_categories:
            print(f"üîé –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category}")
            category_detections = self._detect_for_category(category, processing_text, doc)
            
            # –ü–æ–∑–∏—Ü–∏–∏ —É–∂–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            
            if category_detections:
                print(f"‚úÖ –ö–∞—Ç–µ–≥–æ—Ä–∏—è {category}: –Ω–∞–π–¥–µ–Ω–æ {len(category_detections)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            else:
                print(f"‚ùå –ö–∞—Ç–µ–≥–æ—Ä–∏—è {category}: –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            all_detections.extend(category_detections)
        
        print(f"üìä –í—Å–µ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–π –¥–æ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(all_detections)}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        final_detections = self._global_deduplicate(all_detections)
        print(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(final_detections)}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É confidence threshold
        filtered_detections = [
            detection for detection in final_detections
            if detection.get('confidence', 0) >= self.confidence_threshold
        ]
        print(f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (threshold {self.confidence_threshold}): {len(filtered_detections)}")
        
        return filtered_detections
    
    def _detect_for_category(self, category: str, text: str, doc: Doc) -> List[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—è –µ—ë –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        
        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            doc: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π spaCy –¥–æ–∫—É–º–µ–Ω—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        """
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        enabled_methods = self.config.get_enabled_methods_for_category(category)
        strategy_name = self.config.get_detection_strategy_name(category)
        max_results = self.config.get_max_results_for_category(category)
        
        if not enabled_methods:
            return []
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ç–æ–¥–∞–º —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        results_by_method = {}
        priority_order = self.config.get_method_priority_order(category)
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–¥–æ–≤ —Å –∏—Ö –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        methods_with_priority = list(zip(enabled_methods, priority_order)) if priority_order else [(m, 1) for m in enabled_methods]
        methods_with_priority.sort(key=lambda x: x[1])  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        
        for method, priority in methods_with_priority:
            method_results = self._execute_detection_method(method, category, text, doc)
            
            if method_results:
                results_by_method[method] = method_results
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º early exit
                if self._should_early_exit(category, method, method_results):
                    if self.config.should_log_detection_stats():
                        print(f"Early exit for '{category}' after method '{method}' with {len(method_results)} results")
                    break
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        strategy_settings = self.config.get_detection_strategy_settings(strategy_name)
        
        # –î–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        if strategy_name == 'information_system' and self._is_strategy is not None:
            strategy = self._is_strategy
        else:
            strategy = DetectionStrategyFactory.create_strategy(strategy_name, strategy_settings)
        
        combined_results = strategy.combine_results(results_by_method)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if len(combined_results) > max_results:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ confidence –∏ –±–µ—Ä–µ–º –ª—É—á—à–∏–µ
            combined_results.sort(key=lambda x: x.get('confidence', 0), reverse=True)
            combined_results = combined_results[:max_results]
        
        return combined_results
    
    def _execute_detection_method(self, method: str, category: str, text: str, doc: Doc) -> List[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        
        Args:
            method: –ù–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç–æ–¥–∞
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç  
            doc: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π spaCy –¥–æ–∫—É–º–µ–Ω—Ç
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ—Ç–æ–¥–∞
        """
        method_settings = self.config.get_method_settings(category, method)
        min_confidence = self.config.get_min_confidence_for_method(category, method)
        
        results = []
        
        try:
            if method == 'spacy_ner':
                results = self._extract_spacy_entities_for_category(doc, category)
            elif method == 'regex':
                results = self._extract_regex_patterns_for_category(text, category)
            elif method == 'morphological':
                results = self._extract_morphological_names_for_category(doc, category)
            elif method == 'custom_matcher':
                results = self._extract_custom_matches_for_category(doc, category)
            elif method == 'phrase_matcher':
                results = self._extract_context_matches_for_category(doc, category)
            elif method == 'context_matcher':
                results = self._extract_context_matches_for_category(doc, category)
            else:
                if self.config.should_log_detection_stats():
                    print(f"Unknown detection method: {method}")
                return []
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π confidence –¥–ª—è –º–µ—Ç–æ–¥–∞
            filtered_results = [r for r in results if r.get('confidence', 0) >= min_confidence]
            
            return filtered_results
            
        except Exception as e:
            if self.config.should_log_detection_stats():
                print(f"Error in method {method} for category {category}: {str(e)}")
            return []
    
    def _should_early_exit(self, category: str, method: str, results: List[Dict[str, Any]]) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –¥–µ–ª–∞—Ç—å early exit –ø–æ—Å–ª–µ –¥–∞–Ω–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
        
        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è
            method: –ú–µ—Ç–æ–¥
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ—Ç–æ–¥–∞
            
        Returns:
            True –µ—Å–ª–∏ –Ω—É–∂–µ–Ω early exit
        """
        # –û—Ç–∫–ª—é—á–∞–µ–º early exit –¥–ª—è government_org, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        if category == 'government_org':
            return False
            
        if not results:
            return False
        
        early_exit_threshold = self.config.get_early_exit_threshold(category, method)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –≤—ã—Å–æ–∫–æ–π confidence
        for result in results:
            if result.get('confidence', 0) >= early_exit_threshold:
                return True
        
        return False
    
    def _global_deduplicate(self, all_detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –ì–ª–æ–±–∞–ª—å–Ω–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É –≤—Å–µ–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        
        Args:
            all_detections: –í—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏
            
        Returns:
            –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
        """
        if not all_detections:
            return []
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–º—Å—è –ø–æ–∑–∏—Ü–∏—è–º
        deduplicated = []
        used_indices = set()
        
        for i, detection in enumerate(all_detections):
            if i in used_indices:
                continue
            
            # –ò—â–µ–º –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –¥–µ—Ç–µ–∫—Ü–∏–∏
            overlapping_group = [detection]
            used_indices.add(i)
            
            for j, other_detection in enumerate(all_detections[i+1:], i+1):
                if j in used_indices:
                    continue
                
                if self._detections_overlap(detection, other_detection):
                    overlapping_group.append(other_detection)
                    used_indices.add(j)
            
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –¥–µ—Ç–µ–∫—Ü–∏—é –∏–∑ –≥—Ä—É–ø–ø—ã
            best_detection = max(overlapping_group, key=lambda x: x.get('confidence', 0))
            deduplicated.append(best_detection)
        
        return deduplicated
    
    def _detections_overlap(self, det1: Dict[str, Any], det2: Dict[str, Any], threshold: float = 0.5) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—Ç—Å—è –ª–∏ –¥–≤–µ –¥–µ—Ç–µ–∫—Ü–∏–∏
        
        Args:
            det1, det2: –î–µ—Ç–µ–∫—Ü–∏–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            threshold: –ü–æ—Ä–æ–≥ –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            
        Returns:
            True –µ—Å–ª–∏ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—Ç—Å—è
        """
        pos1 = det1.get('position', {})
        pos2 = det2.get('position', {})
        
        start1, end1 = pos1.get('start', 0), pos1.get('end', 0)
        start2, end2 = pos2.get('start', 0), pos2.get('end', 0)
        
        overlap_start = max(start1, start2)
        overlap_end = min(end1, end2)
        
        if overlap_start >= overlap_end:
            return False
        
        overlap_length = overlap_end - overlap_start
        min_length = min(end1 - start1, end2 - start2)
        
        return (overlap_length / min_length) >= threshold if min_length > 0 else False
    
    def _extract_spacy_entities(self, doc: Doc) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ spaCy NER"""
        entities = []
        
        # –ú–∞–ø–ø–∏–Ω–≥ spaCy –º–µ—Ç–æ–∫ –Ω–∞ –Ω–∞—à–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        category_map = self.config.get_spacy_entity_mapping()
        
        for ent in doc.ents:
            if ent.label_ in category_map:
                category = category_map[ent.label_]
                
                # –î–µ—Ç–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç–æ–¥ spaCy NER
                method_detail = f"spacy_ner_{ent.label_.lower()}"
                
                detection = {
                    'category': category,
                    'original_value': ent.text,
                    'confidence': self.config.get_spacy_ner_confidence(),
                    'position': {
                        'start': ent.start_char,
                        'end': ent.end_char
                    },
                    'method': method_detail,
                    'spacy_label': ent.label_,  # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–µ—Ç–∫—É spaCy
                    'uuid': str(uuid.uuid4())
                }
                entities.append(detection)
        
        return entities
    
    def _extract_spacy_entities_for_category(self, doc: Doc, category: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç spaCy NER —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        entities = []
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º
        if category == 'information_system':
            return self._extract_information_systems(doc)
        
        # –ú–∞–ø–ø–∏–Ω–≥ spaCy –º–µ—Ç–æ–∫ –Ω–∞ –Ω–∞—à–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        category_map = self.config.get_spacy_entity_mapping()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        relevant_labels = [label for label, cat in category_map.items() if cat == category]
        
        for ent in doc.ents:
            if ent.label_ in relevant_labels:
                detection = self.detection_factory.create_detection(
                    method=f"spacy_ner_{ent.label_.lower()}",
                    category=category,
                    original_value=ent.text,
                    position=(ent.start_char, ent.end_char),
                    additional_info={
                        'spacy_confidence': getattr(ent, 'confidence', None),
                        'spacy_label': ent.label_
                    }
                )
                entities.append(detection)
        
        return entities
    
    def _init_information_system_strategy(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –∫–µ—à–∏—Ä—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ"""
        try:
            from information_system_strategy import InformationSystemStrategy
            strategy_settings = self.config.get_detection_strategy_settings('information_system')
            # –ü–µ—Ä–µ–¥–∞–µ–º —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é spaCy –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            self._is_strategy = InformationSystemStrategy(strategy_settings, self.nlp)
            print("üîÑ –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–π spaCy –º–æ–¥–µ–ª—å—é")
        except ImportError as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å InformationSystemStrategy: {e}")
            self._is_strategy = None
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ò–°: {e}")
            self._is_strategy = None
    
    def _extract_information_systems(self, doc: Doc) -> List[Dict[str, Any]]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
            if self._is_strategy is None:
                return []
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
            detections = self._is_strategy.detect_information_systems_in_text(doc.text, doc)
            
            return detections
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: {e}")
            return []
    
    def _extract_regex_patterns_for_category(self, text: str, category: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        detections = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω—É–∂–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if category not in self.pattern_configs:
            return []
        
        category_pattern_configs = self.pattern_configs[category]
        
        for pattern_config in category_pattern_configs:
            pattern = pattern_config['pattern']
            pattern_type = pattern_config['pattern_type']
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º non-regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–ª–∏ –ø—É—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            if pattern_type != 'regex' or pd.isna(pattern):
                continue
            
            flags = self.config.get_regex_flags_for_category(category)
            
            try:
                matches = re.finditer(pattern, text, flags)
                
                for match in matches:
                    if self._validate_context(text, match, category):
                        detection = self.detection_factory.create_detection(
                            method='regex',
                            category=category,
                            original_value=match.group(),
                            position=(match.start(), match.end()),
                            additional_info={
                                'pattern_type': pattern_type,
                                'pattern_complexity': len(pattern) / 100.0,  # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                                'has_context': True
                            }
                        )
                        detections.append(detection)
                        
            except re.error as e:
                if self.config.should_log_pattern_loading():
                    print(f"Regex error in pattern for {category}: {e}")
        
        return detections
    
    def _extract_morphological_names_for_category(self, doc: Doc, category: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–∞ —á–µ—Ä–µ–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        if category != 'person_name' or not self.morph:
            return []
        
        detections = []
        
        # –ò—â–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –ø–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        for token in doc:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–æ–∫–µ–Ω—ã
            if token.is_stop or token.is_punct or len(token.text) < 2:
                continue
            
            # –ò—â–µ–º —Å–ª–æ–≤–∞ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞)
            if token.text[0].isupper() and token.pos_ == 'PROPN':
                if self._is_likely_person_name_morph(token.text):
                    detection = self.detection_factory.create_detection(
                        method='morphological_enhanced',
                        category=category,
                        original_value=token.text,
                        position=(token.idx, token.idx + len(token.text)),
                        additional_info={
                            'morphological_tags': ['enhanced'],
                            'pos_tag': token.pos_
                        }
                    )
                    detections.append(detection)
                elif self._is_likely_person_name(token, doc):
                    detection = self.detection_factory.create_detection(
                        method='morphological',
                        category=category,
                        original_value=token.text,
                        position=(token.idx, token.idx + len(token.text)),
                        additional_info={
                            'morphological_tags': ['basic'],
                            'pos_tag': token.pos_
                        }
                    )
                    detections.append(detection)
        
        return detections
    
    def _extract_custom_matches_for_category(self, doc: Doc, category: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–∞—Ç—á–µ—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        detections = []
        
        if not self.matcher:
            return []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Matcher –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        matches = self.matcher(doc)
        
        for match_id, start, end in matches:
            span = doc[start:end]
            label = self.nlp.vocab.strings[match_id]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –º–µ—Ç–∫–µ –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
            if label == "full_name" or label == "initials_lastname":
                detected_category = "person_name"
            elif label == "quoted_organization":
                detected_category = "organization"
            else:
                detected_category = "unknown"
            
            if detected_category == category:
                detection = self.detection_factory.create_detection(
                    method='custom_matcher',
                    category=category,
                    original_value=span.text,
                    position=(span.start_char, span.end_char),
                    additional_info={
                        'matcher_label': label,
                        'is_structured': True,
                        'match_accuracy': 0.9  # –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
                    }
                )
                detections.append(detection)
        
        return detections
    
    def _extract_context_matches_for_category(self, doc: Doc, category: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –º–∞—Ç—á–µ—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        detections = []
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —É–º–Ω—ã–π –º–∞—Ç—á–µ—Ä, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        if category in self.smart_phrase_matchers:
            try:
                smart_matcher = self.smart_phrase_matchers[category]
                smart_matches = smart_matcher.find_matches(doc)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —É–º–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤ –¥–µ—Ç–µ–∫—Ü–∏–∏
                smart_detections = smart_matcher.convert_to_detections(
                    matches=smart_matches,
                    category=category,
                    method='smart_phrase_matcher'
                )
                
                detections.extend(smart_detections)
                
                # –ï—Å–ª–∏ —É–º–Ω—ã–π –º–∞—Ç—á–µ—Ä –Ω–∞—à–µ–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∏—Ö
                if smart_detections:
                    print(f"üéØ –£–º–Ω—ã–π –º–∞—Ç—á–µ—Ä –¥–ª—è {category}: –Ω–∞–π–¥–µ–Ω–æ {len(smart_detections)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
                    return detections
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —É–º–Ω–æ–º –º–∞—Ç—á–µ—Ä–µ –¥–ª—è {category}: {e}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –æ–±—ã—á–Ω—ã–º –º–∞—Ç—á–µ—Ä–æ–º
        
        # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É phrase matcher
        if not self.phrase_matcher:
            return detections
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º phrase matcher
        matches = self.phrase_matcher(doc)
        
        for match_id, start, end in matches:
            span = doc[start:end]
            label = self.nlp.vocab.strings[match_id]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ phrase patterns
            detected_category = self._get_phrase_category(label)
            
            if detected_category == category:
                detection = self.detection_factory.create_detection(
                    method='phrase_matcher',
                    category=category,
                    original_value=span.text,
                    position=(span.start_char, span.end_char),
                    additional_info={
                        'phrase_label': label,
                        'match_accuracy': 0.8
                    }
                )
                detections.append(detection)
        
        return detections
    
    def _get_phrase_category(self, label: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –º–µ—Ç–∫–µ phrase matcher"""
        # –ú–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–æ–∫ –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        label_to_category = {
            'position_phrases': 'position',
            'organization_phrases': 'organization', 
            'department_phrases': 'organization',
            'salary_phrases': 'financial_amount',
            'health_info_phrases': 'medical',
            'trade_secret_phrases': 'confidential',
            'government_org_phrases': 'government_org'  # –ù–æ–≤–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        }
        
        return label_to_category.get(label, 'unknown')
    
    def _extract_regex_patterns(self, text: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã"""
        detections = []
        
        for category, configs in self.pattern_configs.items():
            for config in configs:
                if config['pattern_type'] != 'regex' or not config['pattern']:
                    continue
                
                try:
                    pattern = config['pattern']
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ñ–ª–∞–≥–∏ regex –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
                    regex_flags = self.config.get_regex_flags_for_category(category)
                    matches = re.finditer(pattern, text, regex_flags)
                    
                    for match in matches:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
                        if config['context_required']:
                            if not self._validate_context(text, match, category):
                                continue
                        
                        detection = {
                            'category': category,
                            'original_value': match.group(),
                            'confidence': config['confidence'],
                            'position': {
                                'start': match.start(),
                                'end': match.end()
                            },
                            'method': 'regex',
                            'uuid': str(uuid.uuid4())
                        }
                        detections.append(detection)
                
                except re.error as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ regex –ø–∞—Ç—Ç–µ—Ä–Ω–∞ {category}: {e}")
                    continue
        
        return detections
    
    def _extract_context_matches(self, doc: Doc) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –º–∞—Ç—á–µ—Ä—ã"""
        detections = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º phrase matcher
        matches = self.phrase_matcher(doc)
        
        for match_id, start, end in matches:
            span = doc[start:end]
            label = self.nlp.vocab.strings[match_id]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ –º–µ—Ç–∫–∏
            category = label.split('_')[0] if '_' in label else label
            
            detection = {
                'category': category,
                'original_value': span.text,
                'confidence': 0.7,
                'position': {
                    'start': span.start_char,
                    'end': span.end_char
                },
                'method': 'phrase_matcher',
                'uuid': str(uuid.uuid4())
            }
            detections.append(detection)
        
        return detections
    
    def _extract_morphological_names(self, doc: Doc) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º–µ–Ω–∞ —á–µ—Ä–µ–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å pymorphy3"""
        detections = []
        
        # –ò—â–µ–º –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –ø–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
        for token in doc:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–æ–∫–µ–Ω—ã
            if token.is_stop or token.is_punct or len(token.text) < 2:
                continue
            
            # –ò—â–µ–º —Å–ª–æ–≤–∞ —Å –±–æ–ª—å—à–æ–π –±—É–∫–≤—ã (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞)
            if token.text[0].isupper() and token.pos_ == 'PROPN':
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º pymorphy3 –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                if self.morph and self._is_likely_person_name_morph(token.text):
                    detection = {
                        'category': 'person_name',
                        'original_value': token.text,
                        'confidence': self.config.get_morphological_enhanced_confidence(),
                        'position': {
                            'start': token.idx,
                            'end': token.idx + len(token.text)
                        },
                        'method': 'morphological_enhanced',
                        'uuid': str(uuid.uuid4())
                    }
                    detections.append(detection)
                elif self._is_likely_person_name(token, doc):
                    # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
                    detection = {
                        'category': 'person_name',
                        'original_value': token.text,
                        'confidence': self.config.get_morphological_fallback_confidence(),
                        'position': {
                            'start': token.idx,
                            'end': token.idx + len(token.text)
                        },
                        'method': 'morphological',
                        'uuid': str(uuid.uuid4())
                    }
                    detections.append(detection)
        
        return detections
    
    def _is_likely_person_name_morph(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–ª–æ–≤–æ –Ω–∞ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç—å –∫ –∏–º–µ–Ω–∞–º —Å –ø–æ–º–æ—â—å—é pymorphy3"""
        if not self.morph:
            return False
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ª–æ–≤–æ
            parsed = self.morph.parse(word)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–≥–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            person_name_tags = self.config.get_morphological_person_name_tags()
            animated_noun_tags = self.config.get_morphological_animated_noun_tags()
            
            for parse in parsed:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–≥–∏ –¥–ª—è –∏–º–µ–Ω
                for tag in person_name_tags:
                    if tag in parse.tag:
                        return True
                
                # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –æ–¥—É—à–µ–≤–ª–µ–Ω–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
                if all(tag in parse.tag for tag in animated_noun_tags):
                    return True
            
            return False
            
        except Exception:
            return False
    
    def _extract_custom_matches(self, doc: Doc) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ –∫–∞—Å—Ç–æ–º–Ω—ã–µ spaCy –º–∞—Ç—á–µ—Ä—ã"""
        detections = []
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Matcher –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        matches = self.matcher(doc)
        
        for match_id, start, end in matches:
            span = doc[start:end]
            label = self.nlp.vocab.strings[match_id]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –º–µ—Ç–∫–µ
            if label == "full_name" or label == "initials_lastname":
                category = "person_name"
                confidence = self.config.get_custom_matcher_confidence('person_name')
            elif label == "quoted_organization":
                category = "organization"
                confidence = self.config.get_custom_matcher_confidence('organization')
            else:
                category = "unknown"
                confidence = self.config.get_custom_matcher_confidence('unknown')
            
            detection = {
                'category': category,
                'original_value': span.text,
                'confidence': confidence,
                'position': {
                    'start': span.start_char,
                    'end': span.end_char
                },
                'method': 'custom_matcher',
                'uuid': str(uuid.uuid4())
            }
            detections.append(detection)
        
        return detections
    
    def _validate_context(self, text: str, match: re.Match, category: str) -> bool:
        """–í–∞–ª–∏–¥–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è"""
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        context_size = self.config.get_context_window_size()
        start = max(0, match.start() - context_size)
        end = min(len(text), match.end() + context_size)
        context = text[start:end].lower()
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        context_keywords = self.config.get_context_keywords_for_category(category)
        
        if context_keywords:
            return any(keyword in context for keyword in context_keywords)
        
        return True  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞–∑—Ä–µ—à–∞–µ–º
    
    def _is_likely_person_name(self, token: Token, doc: Doc) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ–∫–µ–Ω –≤–µ—Ä–æ—è—Ç–Ω—ã–º –∏–º–µ–Ω–µ–º —á–µ–ª–æ–≤–µ–∫–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: 
        # 1. –ë–æ–ª—å—à–∞—è –±—É–∫–≤–∞ –≤ –Ω–∞—á–∞–ª–µ
        # 2. –ù–µ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–≤–µ—Å—Ç–Ω—ã–º —Å–ª–æ–≤–æ–º (–Ω–µ –≤ —Å–ª–æ–≤–∞—Ä–µ)
        # 3. –û–∫—Ä—É–∂–µ–Ω –¥—Ä—É–≥–∏–º–∏ –∏–º–µ–Ω–∞–º–∏ –∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        
        if not token.text[0].isupper():
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ —Ä—è–¥–æ–º
        context_words = self.config.get_context_keywords_for_category('person_name')
        
        # –°–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–æ—Å–µ–¥–Ω–∏–µ —Ç–æ–∫–µ–Ω—ã
        start_idx = max(0, token.i - 2)
        end_idx = min(len(doc), token.i + 3)
        
        nearby_text = ' '.join([t.text.lower() for t in doc[start_idx:end_idx]])
        
        return any(word in nearby_text for word in context_words)
    
    def _deduplicate_detections(self, detections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–π"""
        unique_detections = []
        seen_positions = set()
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        detections.sort(key=lambda x: x['position']['start'])
        
        for detection in detections:
            pos_key = (detection['position']['start'], detection['position']['end'])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Å —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏
            overlaps = False
            for seen_start, seen_end in seen_positions:
                if (detection['position']['start'] < seen_end and 
                    detection['position']['end'] > seen_start):
                    overlaps = True
                    break
            
            if not overlaps:
                unique_detections.append(detection)
                seen_positions.add(pos_key)
        
        return unique_detections
    
    def _map_positions_to_original(self, detections: List[Dict[str, Any]], 
                                  original_text: str, normalized_text: str) -> List[Dict[str, Any]]:
        """
        –ú–∞–ø–ø–∏—Ç –ø–æ–∑–∏—Ü–∏–∏ –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –æ–±—Ä–∞—Ç–Ω–æ –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É
        
        Args:
            detections: –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π —Å –ø–æ–∑–∏—Ü–∏—è–º–∏ –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
            original_text: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            normalized_text: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–µ—Ç–µ–∫—Ü–∏–π —Å –ø–æ–∑–∏—Ü–∏—è–º–∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
        """
        if original_text == normalized_text:
            return detections
        
        mapped_detections = []
        
        for detection in detections:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
                start_norm = detection['position']['start']
                end_norm = detection['position']['end']
                found_text = normalized_text[start_norm:end_norm]
                
                # –ò—â–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –ø–æ–∑–∏—Ü–∏—é –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
                original_start = self._find_text_position_in_original(
                    found_text, original_text, start_norm, normalized_text
                )
                
                if original_start is not None:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –¥–µ—Ç–µ–∫—Ü–∏—é —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–æ–∑–∏—Ü–∏—è–º–∏
                    mapped_detection = detection.copy()
                    mapped_detection['position'] = {
                        'start': original_start,
                        'end': original_start + len(found_text)
                    }
                    # –û–±–Ω–æ–≤–ª—è–µ–º original_value –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
                    mapped_detection['original_value'] = original_text[
                        original_start:original_start + len(found_text)
                    ]
                    mapped_detections.append(mapped_detection)
                else:
                    # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –Ω–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ '{found_text}' –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –º–∞–ø–ø–∏–Ω–≥–µ –ø–æ–∑–∏—Ü–∏–∏: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –¥–µ—Ç–µ–∫—Ü–∏—é
                mapped_detections.append(detection)
        
        return mapped_detections
    
    def _find_text_position_in_original(self, found_text: str, original_text: str, 
                                       approximate_pos: int, normalized_text: str) -> Optional[int]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –ø–æ–∑–∏—Ü–∏—é —Ç–µ–∫—Å—Ç–∞ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
        
        Args:
            found_text: –ù–∞–π–¥–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            original_text: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
            approximate_pos: –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
            normalized_text: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –ü–æ–∑–∏—Ü–∏—è –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        """
        # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        position = original_text.find(found_text)
        if position != -1:
            return position
        
        # –ò—â–µ–º –±–µ–∑ —É—á–µ—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞
        position = original_text.lower().find(found_text.lower())
        if position != -1:
            return position
        
        # –ò—â–µ–º —Å —É—á–µ—Ç–æ–º –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫
        # –ó–∞–º–µ–Ω—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ –≤ –Ω–∞–π–¥–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã –∏ –∏—â–µ–º —Å–Ω–æ–≤–∞
        text_with_spaces = found_text.replace('\n', ' ').replace('\r', ' ')
        text_variants = [text_with_spaces, text_with_spaces.strip()]
        
        for variant in text_variants:
            position = original_text.find(variant)
            if position != -1:
                return position
            
            position = original_text.lower().find(variant.lower())
            if position != -1:
                return position
        
        return None
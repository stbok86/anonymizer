"""
ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² - ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÐµÑÑŒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
"""

import os
import uuid
import json
import requests
import pandas as pd
from typing import List, Dict, Any, Optional
from docx import Document

from block_builder import BlockBuilder
from rule_adapter import RuleEngineAdapter
from formatter_applier import FormatterApplier


class FullAnonymizer:
    def __init__(self, patterns_path: str = None, nlp_service_url: str = "http://localhost:8006"):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°
        """
        self.patterns_path = patterns_path or "patterns/sensitive_patterns.xlsx"
        self.nlp_service_url = nlp_service_url
        self.block_builder = BlockBuilder()
        self.rule_engine = RuleEngineAdapter(self.patterns_path)
        self.formatter = FormatterApplier(highlight_replacements=True)
        
    def _call_nlp_service(self, text: str) -> List[Dict[str, Any]]:
        """
        Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ NLP Service Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        """
        try:
            payload = {
                "blocks": [
                    {
                        "content": text,
                        "block_id": "doc_block_1",
                        "block_type": "text"
                    }
                ],
                "options": {}
            }
            
            response = requests.post(
                f"{self.nlp_service_url}/analyze",
                json=payload,
                timeout=30
            )
            if response.status_code == 200:
                result = response.json()
                return result.get('detections', [])
            else:
                print(f"âš ï¸  NLP Service error: {response.status_code}")
                print(f"âš ï¸  Response: {response.text}")
                return []
        except Exception as e:
            print(f"âš ï¸  NLP Service unavailable: {str(e)}")
            return []
        
    def anonymize_document(self, 
                          input_path: str, 
                          output_path: str,
                          excel_report_path: Optional[str] = None,
                          json_ledger_path: Optional[str] = None,
                          replacements_table: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """
        ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²
        
        Args:
            input_path: ÐŸÑƒÑ‚ÑŒ Ðº Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñƒ
            output_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            excel_report_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            json_ledger_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ JSON Ð¶ÑƒÑ€Ð½Ð°Ð»Ð° (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            replacements_table: ÐŸÑ€ÐµÐ´Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð·Ð°Ð¼ÐµÐ½ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            
        Returns:
            Dict Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        """
        try:
            # Ð­Ð¢ÐÐŸ 1: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            doc = Document(input_path)
            
            # Ð­Ð¢ÐÐŸ 2: Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð±Ð»Ð¾ÐºÐ¾Ð²
            blocks = self.block_builder.build_blocks(doc)
            
            # Ð­Ð¢ÐÐŸ 3: ÐŸÐ¾Ð¸ÑÐº Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (ÐµÑÐ»Ð¸ Ð½Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð·Ð°Ð¼ÐµÐ½)
            if replacements_table is None:
                # 3.1: ÐŸÐ¾Ð¸ÑÐº Ñ‡ÐµÑ€ÐµÐ· Rule Engine (ÑÑ‚Ð°Ñ€Ñ‹Ðµ regex Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹)
                processed_blocks = self.rule_engine.apply_rules_to_blocks(blocks)
                rule_engine_matches = []
                
                for block in processed_blocks:
                    if 'sensitive_patterns' in block:
                        for pattern in block['sensitive_patterns']:
                            match = {
                                'block_id': block['block_id'],
                                'original_value': pattern['original_value'],
                                'position': pattern['position'],
                                'element': block.get('element'),
                                'category': pattern['category'],
                                'confidence': pattern.get('confidence', 1.0),
                                'source': 'rule_engine',
                                'method': 'regex'
                            }
                            rule_engine_matches.append(match)
                
                # 3.2: ÐŸÐ¾Ð¸ÑÐº Ñ‡ÐµÑ€ÐµÐ· NLP Service (Ð½Ð¾Ð²Ð°Ñ Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð°)
                nlp_matches = []
                
                # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð²ÐµÑÑŒ Ñ‚ÐµÐºÑÑ‚ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
                full_text = ""
                block_offsets = []  # Ð”Ð»Ñ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³Ð° Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¹ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð½Ð° Ð±Ð»Ð¾ÐºÐ¸
                
                for block in blocks:
                    block_text = block.get('text', block.get('content', ''))
                    if block_text.strip():
                        block_start = len(full_text)
                        full_text += block_text + "\n"
                        block_end = len(full_text) - 1
                        
                        block_offsets.append({
                            'block_id': block['block_id'],
                            'start': block_start,
                            'end': block_end,
                            'element': block.get('element'),
                            'original_text': block_text
                        })
                
                # Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ NLP Service
                if full_text.strip():
                    print(f"ðŸ¤– Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ NLP Service Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ñ‚ÐµÐºÑÑ‚Ð° ({len(full_text)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²)")
                    nlp_detections = self._call_nlp_service(full_text)
                    
                    print(f"ðŸŽ¯ NLP Service Ð½Ð°ÑˆÐµÐ» {len(nlp_detections)} Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¹")
                    
                    # ÐœÐ°Ð¿Ð¿Ð¸Ð¼ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸ NLP Service Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾ Ð½Ð° Ð±Ð»Ð¾ÐºÐ¸
                    for detection in nlp_detections:
                        detection_start = detection['position']['start']
                        detection_end = detection['position']['end']
                        
                        # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ð±Ð»Ð¾Ðº, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¼Ñƒ Ð¿Ñ€Ð¸Ð½Ð°Ð´Ð»ÐµÐ¶Ð¸Ñ‚ ÑÑ‚Ð° Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ñ
                        for block_info in block_offsets:
                            if (detection_start >= block_info['start'] and 
                                detection_end <= block_info['end']):
                                
                                # ÐŸÐµÑ€ÐµÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸ÑŽ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð±Ð»Ð¾ÐºÐ°
                                relative_start = detection_start - block_info['start']
                                relative_end = detection_end - block_info['start']
                                
                                match = {
                                    'block_id': block_info['block_id'],
                                    'original_value': detection['original_value'],
                                    'position': {
                                        'start': relative_start,
                                        'end': relative_end,
                                        'global_start': detection_start,
                                        'global_end': detection_end
                                    },
                                    'element': block_info['element'],
                                    'category': detection['category'],
                                    'confidence': detection['confidence'],
                                    'source': 'nlp_service',
                                    'method': detection['method']
                                }
                                nlp_matches.append(match)
                                break
                
                # 3.3: ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ (Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ NLP Service)
                print(f"ðŸ“Š ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹: Rule Engine={len(rule_engine_matches)}, NLP Service={len(nlp_matches)}")
                
                # ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ñ NLP Service (Ð±Ð¾Ð»ÐµÐµ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ)
                all_matches = nlp_matches.copy()
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Rule Engine Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ðµ Ð¿ÐµÑ€ÐµÑÐµÐºÐ°ÑŽÑ‚ÑÑ Ñ NLP
                for re_match in rule_engine_matches:
                    is_duplicate = False
                    for nlp_match in nlp_matches:
                        if (re_match['block_id'] == nlp_match['block_id'] and
                            self._positions_overlap(re_match['position'], nlp_match['position'])):
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        all_matches.append(re_match)
                
                print(f"âœ… Ð˜Ñ‚Ð¾Ð³Ð¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹: {len(all_matches)}")
                
                # --- Ð”ÐžÐ‘ÐÐ’Ð›Ð¯Ð•Ðœ ÐÐÐÐ›Ð˜Ð— Ð˜ ÐÐÐžÐÐ˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð® ÐœÐ•Ð¢ÐÐ”ÐÐÐÐ«Ð¥ ---
                from docx_metadata_handler import DocxMetadataHandler
                metadata_handler = DocxMetadataHandler(input_path)
                metadata = metadata_handler.extract_metadata()
                # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
                metadata_matches = []
                for section_name, section in metadata.items():
                    if isinstance(section, dict):
                        for value in section.values():
                            if value:
                                patterns = self.rule_engine.find_patterns_in_text(value)
                                for pattern in patterns:
                                    already_found = any(m['original_value'] == value for m in all_matches)
                                    if not already_found:
                                        metadata_matches.append({
                                            'block_id': f'metadata_{pattern["category"]}',
                                            'original_value': value,
                                            'position': {'start': 0, 'end': len(value)},
                                            'element': None,
                                            'category': pattern['category'],
                                            'confidence': pattern.get('confidence', 1.0),
                                            'source': 'metadata',
                                            'method': 'regex',
                                            'metadata_section': section_name  # <--- ÐšÐ»ÑŽÑ‡ÐµÐ²Ð¾Ð¹ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚!
                                        })
                if metadata_matches:
                    print(f"ðŸ” [METADATA] ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…: {len(metadata_matches)}")
                all_matches.extend(metadata_matches)
                # --- ÐšÐžÐÐ•Ð¦ Ð”ÐžÐ‘ÐÐ’Ð›Ð•ÐÐ˜Ð¯ ÐÐÐÐ›Ð˜Ð—Ð ÐœÐ•Ð¢ÐÐ”ÐÐÐÐ«Ð¥ ---
                
            else:
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ð·Ð°Ð¼ÐµÐ½
                all_matches = replacements_table
                processed_blocks = blocks
            
            # Ð­Ð¢ÐÐŸ 4: ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¼ÐµÐ½ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
            replacement_stats = self.formatter.apply_replacements_to_document(doc, all_matches)
            # Ð­Ð¢ÐÐŸ 5: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° (Ñ‚ÐµÐºÑÑ‚)
            doc.save(output_path)
            # Ð­Ð¢ÐÐŸ 6: ÐÐ½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… (ÐµÑÐ»Ð¸ Ð±Ñ‹Ð»Ð¸ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹)
            if metadata_matches:
                # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ UUID Ð´Ð»Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹
                for m in metadata_matches:
                    from uuid_mapper import UUIDMapper
                    uuid_mapper = self.formatter.uuid_mapper if hasattr(self.formatter, 'uuid_mapper') else UUIDMapper()
                    m['uuid'] = uuid_mapper.get_uuid_for_text(m['original_value'], m['category'])
                # ÐÐ½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² docx
                metadata_handler.anonymize_metadata_in_docx(output_path, output_path, metadata_matches)
            # Ð­Ð¢ÐÐŸ 7: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²
            results = {
                'status': 'success',
                'message': 'Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½',
                'statistics': replacement_stats,
                'total_blocks': len(blocks),
                'matches_count': len(all_matches),
                'anonymized_document_path': output_path
            }
            # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°
            if excel_report_path:
                excel_data = self._generate_excel_report(processed_blocks, all_matches)
                results['excel_report_path'] = excel_report_path
                results['excel_report_generated'] = True
            # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ JSON Ð¶ÑƒÑ€Ð½Ð°Ð»Ð°
            if json_ledger_path:
                ledger_data = self._generate_json_ledger(all_matches, replacement_stats)
                with open(json_ledger_path, 'w', encoding='utf-8') as f:
                    json.dump(ledger_data, f, ensure_ascii=False, indent=2)
                results['json_ledger_path'] = json_ledger_path
                results['json_ledger_generated'] = True
            return results
            
        except Exception as e:
            return {
                'status': 'error',
                'error_message': f'ÐžÑˆÐ¸Ð±ÐºÐ° Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {str(e)}',
                'error_type': type(e).__name__
            }

    def anonymize_selected_items(self, 
                                input_path: str, 
                                output_path: str,
                                selected_items: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ÐÐ½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
        
        Args:
            input_path: ÐŸÑƒÑ‚ÑŒ Ðº Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñƒ
            output_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            selected_items: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
            
        Returns:
            Dict Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ ÑÐµÐ»ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        """
        try:
            print(f"ðŸ”§ [FULL_ANONYMIZER] ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð·Ð°Ð¼ÐµÐ½Ñ‹: {len(selected_items)}")
            for i, item in enumerate(selected_items[:5]):  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 5
                print(f"ðŸ”§ [FULL_ANONYMIZER] Ð­Ð»ÐµÐ¼ÐµÐ½Ñ‚ {i+1}: '{item.get('original_value', 'N/A')}' Ð² Ð±Ð»Ð¾ÐºÐµ {item.get('block_id', 'N/A')}")
            if len(selected_items) > 5:
                print(f"ðŸ”§ [FULL_ANONYMIZER] ... Ð¸ ÐµÑ‰Ðµ {len(selected_items) - 5} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
            
            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚
            doc = Document(input_path)
            
            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð±Ð»Ð¾ÐºÐ¸ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            blocks = self.block_builder.build_blocks(doc)
            
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ Ð±Ð»Ð¾ÐºÐ¾Ð² Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°
            blocks_map = {block['block_id']: block for block in blocks}
            print(f"ðŸ—‚ï¸  [FULL_ANONYMIZER] Ð¡Ð¾Ð·Ð´Ð°Ð½Ð° ÐºÐ°Ñ€Ñ‚Ð° Ð±Ð»Ð¾ÐºÐ¾Ð²: {list(blocks_map.keys())}")
            
            # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð·Ð°Ð¼ÐµÐ½Ñ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
            replacements_for_formatting = []
            skipped_items = []
            seen_replacements = set()  # Ð”Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
            
            for item in selected_items:
                block_id = item.get('block_id')
                original_value = item.get('original_value', '')
                position = item.get('position', {})
                uuid_val = item.get('uuid', '')

                # Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ñ… uuid
                if not uuid_val or str(uuid_val).strip().lower() == 'placeholder':
                    print(f"ðŸš¨ [BUG] ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ uuid Ð´Ð»Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ '{original_value}' (block_id={block_id}): '{uuid_val}'")

                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡ Ð´Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
                dedup_key = (block_id, original_value, position.get('start'), position.get('end'))

                if dedup_key in seen_replacements:
                    print(f"ðŸ”„ [FULL_ANONYMIZER] ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚: '{original_value}' Ð² {block_id}")
                    continue

                seen_replacements.add(dedup_key)

                if block_id in blocks_map:
                    block = blocks_map[block_id]
                    replacement = {
                        'block_id': block_id,
                        'original_value': original_value,
                        'uuid': uuid_val,
                        'position': position,
                        'element': block.get('element'),
                        'category': item['category']
                    }
                    replacements_for_formatting.append(replacement)
                else:
                    skipped_items.append(item)
                    print(f"âš ï¸  [FULL_ANONYMIZER] ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ - Ð±Ð»Ð¾Ðº '{block_id}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: '{original_value}'")
            
            print(f"ðŸ”§ [FULL_ANONYMIZER] ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð·Ð°Ð¼ÐµÐ½ Ð´Ð»Ñ FormatterApplier: {len(replacements_for_formatting)}")
            print(f"âš ï¸  [FULL_ANONYMIZER] ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(skipped_items)}")
            if skipped_items:
                print(f"âš ï¸  [FULL_ANONYMIZER] Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ block_id: {list(blocks_map.keys())}")

            # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð·Ð°Ð¼ÐµÐ½Ñ‹
            replacement_stats = self.formatter.apply_replacements_to_document(doc, replacements_for_formatting)
            doc.save(output_path)

            # --- Ð¡ÐšÐ’ÐžÐ—ÐÐÐ¯ ÐÐÐžÐÐ˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯ Ð”Ð›Ð¯ HEADER ---
            # Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð±Ð»Ð¾ÐºÐ° Ñ‚Ð¸Ð¿Ð° header Ð´ÐµÐ»Ð°ÐµÐ¼ Ð·Ð°Ð¼ÐµÐ½Ñƒ Ð¸ Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…
            header_items = [item for item in selected_items if 'header' in (item.get('block_id') or '').lower()]
            if header_items:
                print(f"ðŸ”§ [FULL_ANONYMIZER] ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ header-ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ ÑÐºÐ²Ð¾Ð·Ð½Ð¾Ð¹ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {len(header_items)}")
                # Ð“Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð»Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…: Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ header original_value Ð¸ uuid (ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ - Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼)
                from uuid_mapper import UUIDMapper
                uuid_mapper = self.formatter.uuid_mapper if hasattr(self.formatter, 'uuid_mapper') else UUIDMapper()
                metadata_items = []
                for h in header_items:
                    uuid_val = h.get('uuid')
                    if not uuid_val or str(uuid_val).strip().lower() == 'placeholder':
                        uuid_val = uuid_mapper.get_uuid_for_text(h['original_value'], h['category'])
                    for section in ['core', 'app', 'custom']:
                        metadata_items.append({
                            'original_value': h['original_value'],
                            'uuid': uuid_val,
                            'category': h['category'],
                            'metadata_section': section,
                        })
                from docx_metadata_handler import DocxMetadataHandler
                metadata_handler = DocxMetadataHandler(output_path)
                metadata_handler.anonymize_metadata_in_docx(output_path, output_path, metadata_items)

            return {
                'status': 'success',
                'message': f'Ð¡ÐµÐ»ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°. ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(selected_items)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð².',
                'statistics': replacement_stats,
                'selected_items_count': len(selected_items),
                'replacements_applied': replacement_stats.get('total_replacements', 0),
                'anonymized_document_path': output_path
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'error_message': f'ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐµÐ»ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {str(e)}',
                'error_type': type(e).__name__
            }

    def _generate_excel_report(self, processed_blocks: List[Dict], matches: List[Dict]) -> str:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° Ð¾ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…"""
        try:
            report_data = []
            for match in matches:
                report_data.append({
                    'Ð‘Ð»Ð¾Ðº ID': match.get('block_id', ''),
                    'ÐšÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ñ': match.get('category', ''),
                    'ÐžÑ€Ð¸Ð³Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ': match.get('original_value', ''),
                    'UUID Ð·Ð°Ð¼ÐµÐ½Ñ‹': 'Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ÑÑ Ð¿Ñ€Ð¸ Ð·Ð°Ð¼ÐµÐ½Ðµ',
                    'ÐŸÐ¾Ð·Ð¸Ñ†Ð¸Ñ Ð½Ð°Ñ‡Ð°Ð»Ð°': match.get('position', {}).get('start', ''),
                    'ÐŸÐ¾Ð·Ð¸Ñ†Ð¸Ñ ÐºÐ¾Ð½Ñ†Ð°': match.get('position', {}).get('end', ''),
                    'Ð£Ð²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ': match.get('confidence', '')
                })
            
            df = pd.DataFrame(report_data)
            # Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² Ð²Ð¸Ð´Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸, Ñ‚.Ðº. Ð¿ÑƒÑ‚ÑŒ Ð´Ð»Ñ Excel Ð¿Ð¾ÐºÐ° Ð½Ðµ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½
            return df.to_string()
            
        except Exception as e:
            return f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°: {str(e)}"

    def _generate_json_ledger(self, matches: List[Dict], stats: Dict) -> Dict:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ JSON Ð¶ÑƒÑ€Ð½Ð°Ð»Ð° Ð·Ð°Ð¼ÐµÐ½"""
        return {
            'timestamp': pd.Timestamp.now().isoformat(),
            'total_matches': len(matches),
            'replacement_statistics': stats,
            'replacements': [
                {
                    'uuid': match.get('uuid', '[UUID_WILL_BE_GENERATED]'),
                    'category': match.get('category', ''),
                    'original_value': match.get('original_value', ''),
                    'block_id': match.get('block_id', ''),
                    'position': match.get('position', {}),
                    'confidence': match.get('confidence', 1.0)
                }
                for match in matches
            ]
        }
    
    def _positions_overlap(self, pos1: Dict, pos2: Dict, threshold: float = 0.5) -> bool:
        """
        ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‚ÑÑ Ð»Ð¸ Ð´Ð²Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸
        
        Args:
            pos1, pos2: ÐŸÐ¾Ð·Ð¸Ñ†Ð¸Ð¸ Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ 'start' Ð¸ 'end'
            threshold: ÐŸÐ¾Ñ€Ð¾Ð³ Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ñ‚Ð¸Ñ Ð´Ð»Ñ ÑÑ‡Ð¸Ñ‚Ð°Ð½Ð¸Ñ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð¼
            
        Returns:
            True ÐµÑÐ»Ð¸ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‚ÑÑ
        """
        start1 = pos1.get('start', 0)
        end1 = pos1.get('end', 0)
        start2 = pos2.get('start', 0)  
        end2 = pos2.get('end', 0)
        
        overlap_start = max(start1, start2)
        overlap_end = min(end1, end2)
        
        if overlap_start >= overlap_end:
            return False
        
        overlap_length = overlap_end - overlap_start
        min_length = min(end1 - start1, end2 - start2)
        
        return (overlap_length / min_length) >= threshold if min_length > 0 else False
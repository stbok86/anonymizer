"""
ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² - ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÐµÑÑŒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
"""

import os
import uuid
import json
import requests
import pandas as pd
from typing import List, Dict, Any, Optional
from docx import Document

from block_builder import BlockBuilder
from rule_adapter import RuleEngineAdapter
from formatter_applier import FormatterApplier


class FullAnonymizer:
    def __init__(self, patterns_path: str = None, nlp_service_url: str = "http://localhost:8006"):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°
        """
        self.patterns_path = patterns_path or "patterns/sensitive_patterns.xlsx"
        self.nlp_service_url = nlp_service_url
        self.block_builder = BlockBuilder()
        self.rule_engine = RuleEngineAdapter(self.patterns_path)
        self.formatter = FormatterApplier(highlight_replacements=True)
        
    def _call_nlp_service(self, text: str) -> List[Dict[str, Any]]:
        """
        Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ NLP Service Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        Ð£Ð¡Ð¢ÐÐ Ð•Ð›Ðž: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ _process_blocks_optimized Ð´Ð»Ñ Ð±Ð°Ñ‚Ñ‡Ð¸Ð½Ð³Ð°
        """
        try:
            payload = {
                "blocks": [
                    {
                        "content": text,
                        "block_id": "doc_block_1",
                        "block_type": "text"
                    }

            # ...existing code...
                ],
                "options": {}
            }
            
            response = requests.post(
                f"{self.nlp_service_url}/analyze",
                json=payload,
                timeout=30
            )
            if response.status_code == 200:
                result = response.json()
                return result.get('detections', [])
            else:
                print(f"[WARNING] NLP Service error: {response.status_code}")
                print(f"[WARNING] Response: {response.text}")
                return []
        except Exception as e:
            print(f"[WARNING] NLP Service unavailable: {str(e)}")
            return []
    
    def _deduplicate_blocks(self, blocks: List[Dict]) -> tuple:
        """
        ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯ 1: Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÑ‚ Ð±Ð»Ð¾ÐºÐ¸ Ð¿Ð¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ð¾Ð¼Ñƒ Ñ‚ÐµÐºÑÑ‚Ñƒ Ð´Ð»Ñ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
        
        Args:
            blocks: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð±Ð»Ð¾ÐºÐ¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
            
        Returns:
            tuple: (unique_blocks, text_to_blocks_mapping)
        """
        text_to_blocks = {}
        
        for block in blocks:
            text = block.get('text', '').strip()
            if text:
                if text not in text_to_blocks:
                    text_to_blocks[text] = []
                text_to_blocks[text].append(block)
        
        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÑÐ¿Ð¸ÑÐ¾Ðº ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð±Ð»Ð¾ÐºÐ¾Ð² (Ð±ÐµÑ€Ñ‘Ð¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ð¸Ð· ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð³Ñ€ÑƒÐ¿Ð¿Ñ‹)
        unique_blocks = [block_list[0] for block_list in text_to_blocks.values()]
        
        return unique_blocks, text_to_blocks
    
    def _process_blocks_batch(self, blocks: List[Dict], batch_size: int = 50) -> List[Dict]:
        """
        ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯ 2: ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð±Ð»Ð¾ÐºÐ¸ Ð±Ð°Ñ‚Ñ‡Ð°Ð¼Ð¸ Ñ‡ÐµÑ€ÐµÐ· NLP Service
        
        Args:
            blocks: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð±Ð»Ð¾ÐºÐ¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
            batch_size: Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð° (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ 50)
            
        Returns:
            Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¹ Ñ Ð¿Ñ€Ð¸Ð²ÑÐ·ÐºÐ¾Ð¹ Ðº Ð±Ð»Ð¾ÐºÐ°Ð¼
        """
        all_matches = []
        
        # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð° Ð±Ð°Ñ‚Ñ‡Ð¸
        for i in range(0, len(blocks), batch_size):
            batch_blocks = blocks[i:i + batch_size]
            
            # Ð“Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ payload Ð´Ð»Ñ NLP Service
            nlp_payload = {
                "blocks": [
                    {
                        "content": block.get('text', ''),
                        "block_id": block['block_id'],
                        "block_type": block.get('type', 'text')
                    }
                    for block in batch_blocks
                    if block.get('text', '').strip()
                ],
                "options": {}
            }
            
            if not nlp_payload["blocks"]:
                continue
            
            try:
                # ÐžÐ”Ð˜Ð HTTP Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð½Ð° Ð²ÐµÑÑŒ Ð±Ð°Ñ‚Ñ‡
                response = requests.post(
                    f"{self.nlp_service_url}/analyze",
                    json=nlp_payload,
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    batch_detections = result.get('detections', [])
                    
                    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ mapping block_id -> block Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°
                    blocks_map = {b['block_id']: b for b in batch_blocks}
                    
                    # ÐŸÑ€Ð¸Ð²ÑÐ·Ñ‹Ð²Ð°ÐµÐ¼ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸ Ðº Ð±Ð»Ð¾ÐºÐ°Ð¼
                    for detection in batch_detections:
                        block_id = detection.get('block_id')
                        if block_id in blocks_map:
                            source_block = blocks_map[block_id]
                            all_matches.append({
                                'block_id': block_id,
                                'original_value': detection['original_value'],
                                'position': detection['position'],
                                'element': source_block.get('element'),
                                'category': detection['category'],
                                'confidence': detection['confidence'],
                                'source': 'nlp_service',
                                'method': detection['method']
                            })
                else:
                    print(f"[WARNING] NLP Service error for batch: {response.status_code}")
            except Exception as e:
                print(f"[WARNING] Error processing batch: {str(e)}")
                continue
        
        return all_matches
    
    def _process_blocks_optimized(self, blocks: List[Dict], batch_size: int = 50) -> List[Dict]:
        """
        ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯ ÐšÐžÐœÐ‘Ðž: ÐšÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² + Ð‘Ð°Ñ‚Ñ‡Ð¸Ð½Ð³ HTTP Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
        
        Args:
            blocks: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… Ð±Ð»Ð¾ÐºÐ¾Ð² Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
            batch_size: Ð Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ñ‚Ñ‡Ð° Ð´Ð»Ñ HTTP Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
            
        Returns:
            Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²ÑÐµÑ… Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¹ Ð´Ð»Ñ Ð²ÑÐµÑ… Ð±Ð»Ð¾ÐºÐ¾Ð² (Ð²ÐºÐ»ÑŽÑ‡Ð°Ñ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹)
        """
        import time
        start_time = time.time()
        
        # Ð¨ÐÐ“ 1: Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ - Ð½Ð°Ñ…Ð¾Ð´Ð¸Ð¼ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚ÐµÐºÑÑ‚Ñ‹
        unique_blocks, text_to_blocks = self._deduplicate_blocks(blocks)
        
        blocks_with_text = [b for b in blocks if b.get('text', '').strip()]
        dedup_ratio = (1 - len(unique_blocks) / len(blocks_with_text)) * 100 if blocks_with_text else 0
        
        print(f"\n[OPTIMIZATION] Ð”ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ñ:")
        print(f"   Ð’ÑÐµÐ³Ð¾ Ð±Ð»Ð¾ÐºÐ¾Ð²: {len(blocks)}")
        print(f"   Ð¡ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼: {len(blocks_with_text)}")
        print(f"   Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ…: {len(unique_blocks)}")
        print(f"   Ð”ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²: {len(blocks_with_text) - len(unique_blocks)} ({dedup_ratio:.1f}%)")
        
        # Ð¨ÐÐ“ 2: Ð‘Ð°Ñ‚Ñ‡Ð¸Ð½Ð³ - Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð±Ð»Ð¾ÐºÐ¸
        num_batches = (len(unique_blocks) + batch_size - 1) // batch_size
        print(f"\n[OPTIMIZATION] Ð‘Ð°Ñ‚Ñ‡Ð¸Ð½Ð³:")
        print(f"   Batch size: {batch_size}")
        print(f"   ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð±Ð°Ñ‚Ñ‡ÐµÐ¹: {num_batches}")
        print(f"   Ð‘Ñ‹Ð»Ð¾ Ð±Ñ‹ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð‘Ð•Ð— Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {len(blocks_with_text)}")
        print(f"   Ð‘ÑƒÐ´ÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¡ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹: {num_batches}")
        print(f"   Ð­ÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ HTTP Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²: {len(blocks_with_text) - num_batches} ({100*(1 - num_batches/len(blocks_with_text)):.1f}%)\n")
        
        unique_matches = self._process_blocks_batch(unique_blocks, batch_size)
        
        # Ð¨ÐÐ“ 3: Ð ÐµÐ¿Ð»Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ - ÐºÐ¾Ð¿Ð¸Ñ€ÑƒÐµÐ¼ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸ Ð½Ð° Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹
        all_matches = []
        
        for match in unique_matches:
            # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ð¸ÑÑ…Ð¾Ð´Ð½Ñ‹Ð¹ Ð±Ð»Ð¾Ðº Ð¸ ÐµÐ³Ð¾ Ñ‚ÐµÐºÑÑ‚
            source_block_id = match['block_id']
            source_block = next((b for b in unique_blocks if b['block_id'] == source_block_id), None)
            
            if source_block:
                source_text = source_block.get('text', '').strip()
                
                # Ð ÐµÐ¿Ð»Ð¸Ñ†Ð¸Ñ€ÑƒÐµÐ¼ match Ð´Ð»Ñ Ð²ÑÐµÑ… Ð±Ð»Ð¾ÐºÐ¾Ð² Ñ Ñ‚Ð°ÐºÐ¸Ð¼ Ð¶Ðµ Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼
                for duplicate_block in text_to_blocks.get(source_text, []):
                    match_copy = {
                        'block_id': duplicate_block['block_id'],      # Ð£ÐÐ˜ÐšÐÐ›Ð¬ÐÐ«Ð™ ID
                        'original_value': match['original_value'],
                        'position': match['position'],                # ÐžÐ”Ð˜ÐÐÐšÐžÐ’ÐÐ¯ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ñ
                        'element': duplicate_block.get('element'),    # Ð ÐÐ—ÐÐÐ¯ ÑÑÑ‹Ð»ÐºÐ° Ð½Ð° Ð¾Ð±ÑŠÐµÐºÑ‚
                        'category': match['category'],
                        'confidence': match['confidence'],
                        'source': match['source'],
                        'method': match['method']
                    }
                    all_matches.append(match_copy)
        
        elapsed_time = time.time() - start_time
        print(f"[OPTIMIZATION] ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° Ð·Ð° {elapsed_time:.2f}Ñ")
        print(f"   Ð”ÐµÑ‚ÐµÐºÑ†Ð¸Ð¹ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾: {len(all_matches)}\n")
        
        return all_matches
        
    def anonymize_document(self, 
                          input_path: str, 
                          output_path: str,
                          excel_report_path: Optional[str] = None,
                          json_ledger_path: Optional[str] = None,
                          replacements_table: Optional[List[Dict]] = None,
                          selected_items: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²
        
        Args:
            input_path: ÐŸÑƒÑ‚ÑŒ Ðº Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñƒ
            output_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            excel_report_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            json_ledger_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ JSON Ð¶ÑƒÑ€Ð½Ð°Ð»Ð° (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            replacements_table: ÐŸÑ€ÐµÐ´Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð·Ð°Ð¼ÐµÐ½ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            selected_items: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² (Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÐµÐ½ Ð² Ñ€Ð°Ð±Ð¾Ñ‡ÐµÐ¼ Ñ€ÐµÐ¶Ð¸Ð¼Ðµ)
            
        Returns:
            Dict Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        """
        try:
            # Ð­Ð¢ÐÐŸ 1: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            doc = Document(input_path)
            
            # Ð­Ð¢ÐÐŸ 2: Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð±Ð»Ð¾ÐºÐ¾Ð²
            blocks = self.block_builder.build_blocks(doc)
            
            # Ð­Ð¢ÐÐŸ 3: ÐŸÐ¾Ð¸ÑÐº Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (ÐµÑÐ»Ð¸ Ð½Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð·Ð°Ð¼ÐµÐ½)
            if replacements_table is None:
                # 3.1: ÐŸÐ¾Ð¸ÑÐº Ñ‡ÐµÑ€ÐµÐ· Rule Engine (ÑÑ‚Ð°Ñ€Ñ‹Ðµ regex Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹)
                processed_blocks = self.rule_engine.apply_rules_to_blocks(blocks)
                rule_engine_matches = []
                
                for block in processed_blocks:
                    if 'sensitive_patterns' in block:
                        for pattern in block['sensitive_patterns']:
                            match = {
                                'block_id': block['block_id'],
                                'original_value': pattern['original_value'],
                                'position': pattern['position'],
                                'element': block.get('element'),
                                'category': pattern['category'],
                                'confidence': pattern.get('confidence', 1.0),
                                'source': 'rule_engine',
                                'method': 'regex'
                            }
                            rule_engine_matches.append(match)
                
                # 3.2: ÐŸÐ¾Ð¸ÑÐº Ñ‡ÐµÑ€ÐµÐ· NLP Service (ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—Ð˜Ð ÐžÐ’ÐÐÐÐÐ¯ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°)
                print(f"\n[NLP SERVICE] ÐÐ½Ð°Ð»Ð¸Ð· {len(blocks)} Ð±Ð»Ð¾ÐºÐ¾Ð² Ñ Ð¾Ð¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÐµÐ¹...")
                
                import time
                nlp_start = time.time()
                
                # ÐžÐŸÐ¢Ð˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² + Ð±Ð°Ñ‚Ñ‡Ð¸Ð½Ð³
                nlp_matches = self._process_blocks_optimized(blocks, batch_size=50)
                
                nlp_elapsed = time.time() - nlp_start
                print(f"[NLP SERVICE] Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾ Ð·Ð° {nlp_elapsed:.2f}Ñ ({len(nlp_matches)} Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¹)\n")
                
                
                # 3.3: ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ (Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ NLP Service)
                # print(f"ðŸ“Š ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹: Rule Engine={len(rule_engine_matches)}, NLP Service={len(nlp_matches)}")
                
                # ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ñ NLP Service (Ð±Ð¾Ð»ÐµÐµ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ)
                all_matches = nlp_matches.copy()
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Rule Engine Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ðµ Ð¿ÐµÑ€ÐµÑÐµÐºÐ°ÑŽÑ‚ÑÑ Ñ NLP
                for re_match in rule_engine_matches:
                    is_duplicate = False
                    for nlp_match in nlp_matches:
                        if (re_match['block_id'] == nlp_match['block_id'] and
                            self._positions_overlap(re_match['position'], nlp_match['position'])):
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        all_matches.append(re_match)
                
                # print(f"âœ… Ð˜Ñ‚Ð¾Ð³Ð¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹: {len(all_matches)}")
                
                # --- Ð”ÐžÐ‘ÐÐ’Ð›Ð¯Ð•Ðœ ÐÐÐÐ›Ð˜Ð— Ð˜ ÐÐÐžÐÐ˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð® ÐœÐ•Ð¢ÐÐ”ÐÐÐÐ«Ð¥ ---
                # Ð’Ð Ð•ÐœÐ•ÐÐÐž ÐžÐ¢ÐšÐ›Ð®Ð§Ð•ÐÐž Ð¸Ð·-Ð·Ð° Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð° find_patterns_in_text
                metadata_matches = []
                # from docx_metadata_handler import DocxMetadataHandler
                # metadata_handler = DocxMetadataHandler(input_path)
                # metadata = metadata_handler.extract_metadata()
                # # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
                # for section_name, section in metadata.items():
                #     if isinstance(section, dict):
                #         for value in section.values():
                #             if value:
                #                 # TODO: ÐÑƒÐ¶Ð½Ð¾ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð¸ÑÐº Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· NLP Service
                #                 pass
                # --- ÐšÐžÐÐ•Ð¦ Ð”ÐžÐ‘ÐÐ’Ð›Ð•ÐÐ˜Ð¯ ÐÐÐÐ›Ð˜Ð—Ð ÐœÐ•Ð¢ÐÐ”ÐÐÐÐ«Ð¥ ---
                
            else:
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ð·Ð°Ð¼ÐµÐ½
                all_matches = replacements_table
                processed_blocks = blocks
            
            # Ð­Ð¢ÐÐŸ 3.5: Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ð¼ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð¼
            if selected_items:
                # print(f"ðŸŽ¯ [USER_SELECTION] ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð²Ñ‹Ð±Ð¾Ñ€ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ: {len(selected_items)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
                
                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ Ð±Ð»Ð¾ÐºÐ¾Ð² Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°
                blocks_map = {block['block_id']: block for block in blocks}
                
                # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¾Ñ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð·Ð°Ð¼ÐµÐ½
                filtered_matches = []
                skipped_items = []
                seen_replacements = set()  # Ð”Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
                
                for item in selected_items:
                    block_id = item.get('block_id')
                    original_value = item.get('original_value', '')
                    position = item.get('position', {})
                    uuid_val = item.get('uuid', '')
                    
                    # Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ñ… uuid
                    # if not uuid_val or str(uuid_val).strip().lower() == 'placeholder':
                    #     print(f"ðŸš¨ [BUG] ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ uuid Ð´Ð»Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ '{original_value}' (block_id={block_id}): '{uuid_val}'")
                    
                    # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡ Ð´Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
                    dedup_key = (block_id, original_value, position.get('start'), position.get('end'))
                    
                    if dedup_key in seen_replacements:
                        # print(f"ðŸ”„ [USER_SELECTION] ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚: '{original_value}' Ð² {block_id}")
                        continue
                    
                    seen_replacements.add(dedup_key)
                    
                    if block_id in blocks_map:
                        block = blocks_map[block_id]
                        replacement = {
                            'block_id': block_id,
                            'original_value': original_value,
                            'uuid': uuid_val,
                            'position': position,
                            'element': block.get('element'),
                            'category': item.get('category', 'unknown')
                        }
                        filtered_matches.append(replacement)
                    else:
                        skipped_items.append(item)
                
                # print(f"ðŸŽ¯ [USER_SELECTION] ÐžÑ‚Ð¾Ð±Ñ€Ð°Ð½Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼: {len(filtered_matches)} Ð¸Ð· {len(all_matches)} Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ…")
                
                # Ð—Ð°Ð¼ÐµÐ½ÑÐµÐ¼ all_matches Ð½Ð° Ð¾Ñ‚Ñ„Ð¸Ð»ÑŒÑ‚Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº
                all_matches = filtered_matches
            
            # Ð­Ð¢ÐÐŸ 4: ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¼ÐµÐ½ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
            replacement_stats = self.formatter.apply_replacements_to_document(doc, all_matches)
            
            # ðŸŽ¯ Ð’ÐÐ–ÐÐž: ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð°Ð¼ÐµÐ½Ñ‹ Ñ UUID Ð´Ð»Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²
            normalized_matches = replacement_stats.get('normalized_replacements', all_matches)
            
            # Ð­Ð¢ÐÐŸ 5: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° (Ñ‚ÐµÐºÑÑ‚)
            doc.save(output_path)
            
            # Ð­Ð¢ÐÐŸ 5.5: Ð¡ÐºÐ²Ð¾Ð·Ð½Ð°Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ header ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² (Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼)
            if selected_items:
                header_items = [item for item in selected_items if 'header' in (item.get('block_id') or '').lower()]
                if header_items:
                    
                    from uuid_mapper import UUIDMapper
                    uuid_mapper = self.formatter.uuid_mapper if hasattr(self.formatter, 'uuid_mapper') else UUIDMapper()
                    
                    metadata_items = []
                    for h in header_items:
                        uuid_val = h.get('uuid')
                        if not uuid_val or str(uuid_val).strip().lower() == 'placeholder':
                            uuid_val = uuid_mapper.get_uuid_for_text(h['original_value'], h.get('category', 'unknown'))
                        for section in ['core', 'app', 'custom']:
                            metadata_items.append({
                                'original_value': h['original_value'],
                                'uuid': uuid_val,
                                'category': h.get('category', 'unknown'),
                                'metadata_section': section,
                            })
                    
                    from docx_metadata_handler import DocxMetadataHandler
                    metadata_handler = DocxMetadataHandler(output_path)
                    metadata_handler.anonymize_metadata_in_docx(output_path, output_path, metadata_items)
            
            # Ð­Ð¢ÐÐŸ 6: ÐÐ½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² docProps/core.xml
            try:
                from docx_metadata_handler import DocxMetadataHandler
                from uuid_mapper import UUIDMapper
                
                uuid_mapper = self.formatter.uuid_mapper if hasattr(self.formatter, 'uuid_mapper') else UUIDMapper()
                metadata_handler = DocxMetadataHandler(output_path)
                
                # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
                metadata_handler.extract_metadata()
                
                # Ð˜Ñ‰ÐµÐ¼ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ñ Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ normalized_matches (Ñ UUID)
                sensitive_metadata = metadata_handler.find_sensitive_metadata(normalized_matches)
                
                if sensitive_metadata:
                    # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ UUID Ð´Ð»Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… ÐµÑÐ»Ð¸ Ð¸Ñ… Ð½ÐµÑ‚
                    for i, m in enumerate(sensitive_metadata):
                        existing_uuid = m.get('uuid')
                        if not existing_uuid:
                            m['uuid'] = uuid_mapper.get_uuid_for_text(m['original_value'], m.get('category', 'unknown'))
                        # else: pass
                    
                    # ÐÐ½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² docx
                    metadata_handler.anonymize_metadata_in_docx(output_path, output_path, sensitive_metadata)
                    
                    # ðŸŽ¯ Ð’ÐÐ–ÐÐž: Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² ÑÐ¿Ð¸ÑÐ¾Ðº Ð·Ð°Ð¼ÐµÐ½ Ð´Ð»Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°
                    # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€Ð°Ñ†Ð¸Ñ: Ð½Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐµÑÐ»Ð¸ Ð²ÑÐµ partial_matches ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ
                    
                    doc_values = set(m.get('original_value', '') for m in normalized_matches)
                    
                    for meta in sensitive_metadata:
                        partial_matches = meta.get('partial_matches', [])
                        
                        if partial_matches:
                            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼: ÐµÑÑ‚ÑŒ Ð»Ð¸ Ñ…Ð¾Ñ‚Ñ Ð±Ñ‹ Ð¾Ð´Ð¸Ð½ partial_match, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ ÐÐ•Ð¢ Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ
                            has_new_value = any(pm.get('partial_match', '') not in doc_values for pm in partial_matches)
                            
                            if has_new_value:
                                # Ð•ÑÑ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ â€” Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…
                                normalized_matches.append(meta)
                            # else:
                                # Ð’ÑÐµ partial_matches ÑƒÐ¶Ðµ Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ â€” Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼
                        else:
                            # Ð”Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹ â€” Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ðµ
                            if meta.get('original_value', '') not in doc_values:
                                normalized_matches.append(meta)
                            # else: pass
                # else: pass
                    
            except Exception as e:
                import traceback
                traceback.print_exc()
            
            # Ð­Ð¢ÐÐŸ 7: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²
            results = {
                'status': 'success',
                'message': 'Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½',
                'replacement_stats': replacement_stats,
                'statistics': replacement_stats,  # Ð”Ð»Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸
                'total_blocks': len(blocks),
                'matches_count': len(all_matches),
                'detections_found': normalized_matches,  # Ð”Ð»Ñ Ñ‚ÐµÑÑ‚Ð¾Ð² Ð¸ UI
                'anonymized_document_path': output_path
            }
            # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°
            if excel_report_path:
                excel_generated = self._generate_excel_report(processed_blocks, normalized_matches, excel_report_path)
                results['excel_report_path'] = excel_report_path
                results['excel_report_generated'] = excel_generated
            # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ JSON Ð¶ÑƒÑ€Ð½Ð°Ð»Ð°
            if json_ledger_path:
                ledger_data = self._generate_json_ledger(normalized_matches, replacement_stats)
                with open(json_ledger_path, 'w', encoding='utf-8') as f:
                    json.dump(ledger_data, f, ensure_ascii=False, indent=2)
                results['json_ledger_path'] = json_ledger_path
                results['json_ledger_generated'] = True
            return results
            
        except Exception as e:
            return {
                'status': 'error',
                'error_message': f'ÐžÑˆÐ¸Ð±ÐºÐ° Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {str(e)}',
                'error_type': type(e).__name__
            }

    def _generate_excel_report(self, processed_blocks: List[Dict], matches: List[Dict], excel_path: str) -> bool:
        """
        Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° Ñ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ UUID
        
        Args:
            processed_blocks: ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ Ð±Ð»Ð¾ÐºÐ¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            matches: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹
            excel_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Excel Ñ„Ð°Ð¹Ð»Ð°
            
        Returns:
            True ÐµÑÐ»Ð¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾, False Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ
        """
        try:
            report_data = []
            
            # print(f"ðŸ“ [EXCEL_REPORT] Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° Ð´Ð»Ñ {len(matches)} Ð·Ð°Ð¼ÐµÐ½")
            # print(f"ðŸ“ [EXCEL_REPORT] ÐŸÐµÑ€Ð²Ñ‹Ðµ 3 Ð·Ð°Ð¼ÐµÐ½Ñ‹:")
            # for i, match in enumerate(matches[:3], 1):
            #     print(f"  {i}. original_value: '{match.get('original_value', 'N/A')[:50]}'")
            #     print(f"     uuid: '{match.get('uuid', 'N/A')}'")
            #     print(f"     category: '{match.get('category', 'N/A')}'")
            #     print(f"     source: '{match.get('source', 'N/A')}'")
            #     print(f"     block_id (top level): '{match.get('block_id', 'N/A')}'")
            #     position = match.get('position', {})
            #     block_id = position.get('block_id', 'N/A') if isinstance(position, dict) else 'N/A'
            #     print(f"     position.block_id: '{block_id}'")
            
            for i, match in enumerate(matches, 1):
                original_value = match.get('original_value', '')
                category = match.get('category', 'unknown')
                
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ UUID ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ¶Ðµ Ð±Ñ‹Ð» ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð² formatter_applier
                # Ð•ÑÐ»Ð¸ UUID Ð½ÐµÑ‚ Ð² match, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ (Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚)
                uuid_for_replacement = match.get('uuid')
                if not uuid_for_replacement:
                    uuid_for_replacement = self.formatter.uuid_mapper.get_uuid_for_text(original_value, category)
                
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ block_id - ÑÐ½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð²ÐµÑ€Ñ…Ð½Ð¸Ð¹ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ, Ð¿Ð¾Ñ‚Ð¾Ð¼ position
                block_id = match.get('block_id', '')
                if not block_id:
                    position = match.get('position', {})
                    block_id = position.get('block_id', '') if isinstance(position, dict) else ''
                
                report_data.append({
                    'â„–': i,
                    'Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ': original_value,
                    'Ð—Ð°Ð¼ÐµÐ½Ð° (Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€)': uuid_for_replacement,
                    'ID Ð±Ð»Ð¾ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°': block_id
                })
            
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataFrame Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ°Ð¼Ð¸
            df = pd.DataFrame(report_data)

            # Ð£Ð´Ð°Ð»ÐµÐ½Ð° Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ° ÑÑ‚Ñ€Ð¾Ðº Ð¸ Ð¿ÐµÑ€ÐµÐºÐ¾Ð´Ð¸Ñ€Ð¾Ð²ÐºÐ°: ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ð² Unicode Ð´Ð»Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ ÐºÐ¸Ñ€Ð¸Ð»Ð»Ð¸Ñ†Ñ‹ Ð¸ Ð´Ñ€ÑƒÐ³Ð¸Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
            # df = df.applymap(clean_excel_string)

            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² Excel
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='Ð—Ð°Ð¼ÐµÐ½Ñ‹', index=False)
                # ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
                worksheet = writer.sheets['Ð—Ð°Ð¼ÐµÐ½Ñ‹']
                worksheet.column_dimensions['A'].width = 5      # â„–
                worksheet.column_dimensions['B'].width = 60     # Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ð±Ñ‹Ð»Ð¾ 40, ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¾ Ð² 1.5 Ñ€Ð°Ð·Ð°)
                worksheet.column_dimensions['C'].width = 45     # Ð—Ð°Ð¼ÐµÐ½Ð° (Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€)
                worksheet.column_dimensions['D'].width = 30     # ID Ð±Ð»Ð¾ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° (Ð±Ñ‹Ð»Ð¾ 20, ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¾ Ð² 1.5 Ñ€Ð°Ð·Ð°)
            # print(f"âœ… Excel Ð¾Ñ‚Ñ‡ÐµÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½: {excel_path} ({len(report_data)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹)")
            return True
            
        except Exception as e:
            # print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def _generate_json_ledger(self, matches: List[Dict], stats: Dict) -> Dict:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ JSON Ð¶ÑƒÑ€Ð½Ð°Ð»Ð° Ð·Ð°Ð¼ÐµÐ½ Ñ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ UUID"""
        replacements_list = []
        
        for match in matches:
            original_value = match.get('original_value', '')
            category = match.get('category', 'unknown')
            
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ UUID ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ¶Ðµ Ð±Ñ‹Ð» ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð² formatter_applier
            # Ð•ÑÐ»Ð¸ UUID Ð½ÐµÑ‚ Ð² match, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ (Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚)
            uuid_for_replacement = match.get('uuid')
            if not uuid_for_replacement:
                uuid_for_replacement = self.formatter.uuid_mapper.get_uuid_for_text(original_value, category)
            
            replacements_list.append({
                'uuid': uuid_for_replacement,
                'category': category,
                'original_value': original_value,
                'block_id': match.get('block_id', ''),
                'position': match.get('position', {}),
                'confidence': match.get('confidence', 1.0)
            })
        
        return {
            'timestamp': pd.Timestamp.now().isoformat(),
            'total_matches': len(matches),
            'replacement_statistics': stats,
            'replacements': replacements_list
        }
    
    def _positions_overlap(self, pos1: Dict, pos2: Dict, threshold: float = 0.5) -> bool:
        """
        ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‚ÑÑ Ð»Ð¸ Ð´Ð²Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸
        
        Args:
            pos1, pos2: ÐŸÐ¾Ð·Ð¸Ñ†Ð¸Ð¸ Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ 'start' Ð¸ 'end'
            threshold: ÐŸÐ¾Ñ€Ð¾Ð³ Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ñ‚Ð¸Ñ Ð´Ð»Ñ ÑÑ‡Ð¸Ñ‚Ð°Ð½Ð¸Ñ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð¼
            
        Returns:
            True ÐµÑÐ»Ð¸ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‚ÑÑ
        """
        start1 = pos1.get('start', 0)
        end1 = pos1.get('end', 0)
        start2 = pos2.get('start', 0)  
        end2 = pos2.get('end', 0)
        
        overlap_start = max(start1, start2)
        overlap_end = min(end1, end2)
        
        if overlap_start >= overlap_end:
            return False
        
        overlap_length = overlap_end - overlap_start
        min_length = min(end1 - start1, end2 - start2)
        
        return (overlap_length / min_length) >= threshold if min_length > 0 else False
"""
ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² - ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÐµÑÑŒ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸
"""

import os
import uuid
import json
import requests
import pandas as pd
from typing import List, Dict, Any, Optional
from docx import Document

from block_builder import BlockBuilder
from rule_adapter import RuleEngineAdapter
from formatter_applier import FormatterApplier


class FullAnonymizer:
    def __init__(self, patterns_path: str = None, nlp_service_url: str = "http://localhost:8006"):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ‚Ð¾Ñ€Ð°
        """
        self.patterns_path = patterns_path or "patterns/sensitive_patterns.xlsx"
        self.nlp_service_url = nlp_service_url
        self.block_builder = BlockBuilder()
        self.rule_engine = RuleEngineAdapter(self.patterns_path)
        self.formatter = FormatterApplier(highlight_replacements=True)
        
    def _call_nlp_service(self, text: str) -> List[Dict[str, Any]]:
        """
        Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÑ‚ NLP Service Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ° Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        """
        try:
            payload = {
                "blocks": [
                    {
                        "content": text,
                        "block_id": "doc_block_1",
                        "block_type": "text"
                    }
                ],
                "options": {}
            }
            
            response = requests.post(
                f"{self.nlp_service_url}/analyze",
                json=payload,
                timeout=30
            )
            if response.status_code == 200:
                result = response.json()
                return result.get('detections', [])
            else:
                print(f"âš ï¸  NLP Service error: {response.status_code}")
                print(f"âš ï¸  Response: {response.text}")
                return []
        except Exception as e:
            print(f"âš ï¸  NLP Service unavailable: {str(e)}")
            return []
        
    def anonymize_document(self, 
                          input_path: str, 
                          output_path: str,
                          excel_report_path: Optional[str] = None,
                          json_ledger_path: Optional[str] = None,
                          replacements_table: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """
        ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ†Ð¸ÐºÐ» Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²
        
        Args:
            input_path: ÐŸÑƒÑ‚ÑŒ Ðº Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñƒ
            output_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            excel_report_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            json_ledger_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ JSON Ð¶ÑƒÑ€Ð½Ð°Ð»Ð° (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            replacements_table: ÐŸÑ€ÐµÐ´Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð½Ð°Ñ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð·Ð°Ð¼ÐµÐ½ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            
        Returns:
            Dict Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        """
        try:
            # Ð­Ð¢ÐÐŸ 1: Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            doc = Document(input_path)
            
            # Ð­Ð¢ÐÐŸ 2: Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð±Ð»Ð¾ÐºÐ¾Ð²
            blocks = self.block_builder.build_blocks(doc)
            
            # Ð­Ð¢ÐÐŸ 3: ÐŸÐ¾Ð¸ÑÐº Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… (ÐµÑÐ»Ð¸ Ð½Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð° Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° Ð·Ð°Ð¼ÐµÐ½)
            if replacements_table is None:
                # 3.1: ÐŸÐ¾Ð¸ÑÐº Ñ‡ÐµÑ€ÐµÐ· Rule Engine (ÑÑ‚Ð°Ñ€Ñ‹Ðµ regex Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹)
                processed_blocks = self.rule_engine.apply_rules_to_blocks(blocks)
                rule_engine_matches = []
                
                for block in processed_blocks:
                    if 'sensitive_patterns' in block:
                        for pattern in block['sensitive_patterns']:
                            match = {
                                'block_id': block['block_id'],
                                'original_value': pattern['original_value'],
                                'position': pattern['position'],
                                'element': block.get('element'),
                                'category': pattern['category'],
                                'confidence': pattern.get('confidence', 1.0),
                                'source': 'rule_engine',
                                'method': 'regex'
                            }
                            rule_engine_matches.append(match)
                
                # 3.2: ÐŸÐ¾Ð¸ÑÐº Ñ‡ÐµÑ€ÐµÐ· NLP Service (Ð¿Ð¾Ð±Ð»Ð¾Ñ‡Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°)
                nlp_matches = []
                
                print(f"ðŸ¤– Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ NLP Service Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° {len(blocks)} Ð±Ð»Ð¾ÐºÐ¾Ð²")
                
                # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð±Ð»Ð¾Ðº Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾ Ñ‡ÐµÑ€ÐµÐ· NLP Service
                for block in blocks:
                    block_text = block.get('text', block.get('content', ''))
                    if not block_text.strip():
                        continue
                    
                    # Ð’Ñ‹Ð·Ñ‹Ð²Ð°ÐµÐ¼ NLP Service Ð´Ð»Ñ ÐžÐ”ÐÐžÐ“Ðž Ð±Ð»Ð¾ÐºÐ°
                    block_detections = self._call_nlp_service(block_text)
                    
                    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸ Ñ Ð¿Ñ€Ð¸Ð²ÑÐ·ÐºÐ¾Ð¹ Ðº Ð±Ð»Ð¾ÐºÑƒ
                    for detection in block_detections:
                        match = {
                            'block_id': block['block_id'],
                            'original_value': detection['original_value'],
                            'position': detection['position'],  # Ð£Ð¶Ðµ Ð¾Ñ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð±Ð»Ð¾ÐºÐ°
                            'element': block.get('element'),
                            'category': detection['category'],
                            'confidence': detection['confidence'],
                            'source': 'nlp_service',
                            'method': detection['method']
                        }
                        nlp_matches.append(match)
                
                print(f"ðŸŽ¯ NLP Service Ð½Ð°ÑˆÐµÐ» {len(nlp_matches)} Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¹ Ð²Ð¾ Ð²ÑÐµÑ… Ð±Ð»Ð¾ÐºÐ°Ñ…")
                
                # 3.3: ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ (Ð¿Ñ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚ NLP Service)
                print(f"ðŸ“Š ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹: Rule Engine={len(rule_engine_matches)}, NLP Service={len(nlp_matches)}")
                
                # ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ñ NLP Service (Ð±Ð¾Ð»ÐµÐµ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ)
                all_matches = nlp_matches.copy()
                
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Rule Engine Ð´ÐµÑ‚ÐµÐºÑ†Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½Ðµ Ð¿ÐµÑ€ÐµÑÐµÐºÐ°ÑŽÑ‚ÑÑ Ñ NLP
                for re_match in rule_engine_matches:
                    is_duplicate = False
                    for nlp_match in nlp_matches:
                        if (re_match['block_id'] == nlp_match['block_id'] and
                            self._positions_overlap(re_match['position'], nlp_match['position'])):
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        all_matches.append(re_match)
                
                print(f"âœ… Ð˜Ñ‚Ð¾Ð³Ð¾ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹: {len(all_matches)}")
                
                # --- Ð”ÐžÐ‘ÐÐ’Ð›Ð¯Ð•Ðœ ÐÐÐÐ›Ð˜Ð— Ð˜ ÐÐÐžÐÐ˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð® ÐœÐ•Ð¢ÐÐ”ÐÐÐÐ«Ð¥ ---
                # Ð’Ð Ð•ÐœÐ•ÐÐÐž ÐžÐ¢ÐšÐ›Ð®Ð§Ð•ÐÐž Ð¸Ð·-Ð·Ð° Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ñ Ð¼ÐµÑ‚Ð¾Ð´Ð° find_patterns_in_text
                metadata_matches = []
                # from docx_metadata_handler import DocxMetadataHandler
                # metadata_handler = DocxMetadataHandler(input_path)
                # metadata = metadata_handler.extract_metadata()
                # # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð²ÑÐµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
                # for section_name, section in metadata.items():
                #     if isinstance(section, dict):
                #         for value in section.values():
                #             if value:
                #                 # TODO: ÐÑƒÐ¶Ð½Ð¾ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð¸ÑÐº Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð¾Ð² Ñ‡ÐµÑ€ÐµÐ· NLP Service
                #                 pass
                # --- ÐšÐžÐÐ•Ð¦ Ð”ÐžÐ‘ÐÐ’Ð›Ð•ÐÐ˜Ð¯ ÐÐÐÐ›Ð˜Ð—Ð ÐœÐ•Ð¢ÐÐ”ÐÐÐÐ«Ð¥ ---
                
            else:
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ð·Ð°Ð¼ÐµÐ½
                all_matches = replacements_table
                processed_blocks = blocks
            
            # Ð­Ð¢ÐÐŸ 4: ÐŸÑ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¼ÐµÐ½ Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸ÐµÐ¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
            replacement_stats = self.formatter.apply_replacements_to_document(doc, all_matches)
            
            # ðŸŽ¯ Ð’ÐÐ–ÐÐž: ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð°Ð¼ÐµÐ½Ñ‹ Ñ UUID Ð´Ð»Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²
            normalized_matches = replacement_stats.get('normalized_replacements', all_matches)
            
            # Ð­Ð¢ÐÐŸ 5: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° (Ñ‚ÐµÐºÑÑ‚)
            doc.save(output_path)
            
            # Ð­Ð¢ÐÐŸ 6: ÐÐ½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² docProps/core.xml
            print(f"\nðŸ”§ [METADATA] ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸ÑŽ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
            try:
                from docx_metadata_handler import DocxMetadataHandler
                from uuid_mapper import UUIDMapper
                
                uuid_mapper = self.formatter.uuid_mapper if hasattr(self.formatter, 'uuid_mapper') else UUIDMapper()
                metadata_handler = DocxMetadataHandler(output_path)
                
                # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¸Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
                metadata_handler.extract_metadata()
                
                # Ð˜Ñ‰ÐµÐ¼ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ñ Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ normalized_matches (Ñ UUID)
                sensitive_metadata = metadata_handler.find_sensitive_metadata(normalized_matches)
                
                if sensitive_metadata:
                    print(f"ðŸ”§ [METADATA] ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ‡ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…: {len(sensitive_metadata)}")
                    # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ UUID Ð´Ð»Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… ÐµÑÐ»Ð¸ Ð¸Ñ… Ð½ÐµÑ‚
                    for i, m in enumerate(sensitive_metadata):
                        existing_uuid = m.get('uuid')
                        if not existing_uuid:
                            print(f"âš ï¸ [METADATA] UUID Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð´Ð»Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… #{i}: '{m.get('original_value', 'N/A')[:50]}', partial_match: '{m.get('partial_match', 'N/A')}'")
                            m['uuid'] = uuid_mapper.get_uuid_for_text(m['original_value'], m.get('category', 'unknown'))
                        else:
                            print(f"âœ… [METADATA] UUID ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ Ð´Ð»Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… #{i}: '{m.get('original_value', 'N/A')[:30]}...' â†’ '{existing_uuid}'")
                    
                    # ÐÐ½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² docx
                    metadata_handler.anonymize_metadata_in_docx(output_path, output_path, sensitive_metadata)
                    print(f"ðŸ”§ [METADATA] âœ… ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹")
                    
                    # ðŸŽ¯ Ð’ÐÐ–ÐÐž: Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð² ÑÐ¿Ð¸ÑÐ¾Ðº Ð·Ð°Ð¼ÐµÐ½ Ð´Ð»Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°
                    # ÐÐ¾ ÑÐ½Ð°Ñ‡Ð°Ð»Ð° ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹ Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
                    print(f"ðŸ”§ [DEDUP] Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð² Ð¿ÐµÑ€ÐµÐ´ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸ÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…...")
                    print(f"ðŸ”§ [DEDUP] Ð”Ð¾: {len(normalized_matches)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ Ð² normalized_matches")
                    
                    # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð’Ð¡Ð• Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð¸Ð· Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… (Ð¸ partial_match, Ð¸ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ðµ ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ñ)
                    metadata_values = set()
                    for m in sensitive_metadata:
                        # Ð”Ð»Ñ Ñ‡Ð°ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ñ… ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹ Ð±ÐµÑ€ÐµÐ¼ partial_match
                        partial = m.get('partial_match')
                        if partial:
                            metadata_values.add(partial)
                        # Ð”Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹ Ð±ÐµÑ€ÐµÐ¼ original_value
                        else:
                            metadata_values.add(m.get('original_value', ''))
                    
                    print(f"ðŸ”§ [DEDUP] ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {len(metadata_values)} ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…")
                    
                    # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð¸Ð· normalized_matches Ð·Ð°Ð¿Ð¸ÑÐ¸, Ñƒ ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ñ… original_value ÐµÑÑ‚ÑŒ Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…
                    # Ð˜ source ÐÐ• metadata_* (Ñ‚.Ðµ. Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°)
                    filtered_matches = []
                    removed_count = 0
                    for match in normalized_matches:
                        orig_val = match.get('original_value', '')
                        source = match.get('source', '')
                        
                        # Ð•ÑÐ»Ð¸ ÑÑ‚Ð¾ Ð·Ð°Ð¿Ð¸ÑÑŒ Ð¸Ð· Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° Ð˜ ÐµÑ‘ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ ÐµÑÑ‚ÑŒ Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… â€” ÑƒÐ´Ð°Ð»ÑÐµÐ¼
                        if not source.startswith('metadata_') and orig_val in metadata_values:
                            print(f"ðŸ”§ [DEDUP] âŒ Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚: '{orig_val}' (source: {source})")
                            removed_count += 1
                        else:
                            filtered_matches.append(match)
                    
                    normalized_matches = filtered_matches
                    print(f"ðŸ”§ [DEDUP] ÐŸÐ¾ÑÐ»Ðµ ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²: {len(normalized_matches)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹ (ÑƒÐ´Ð°Ð»ÐµÐ½Ð¾ {removed_count})")
                    
                    # Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
                    normalized_matches.extend(sensitive_metadata)
                    print(f"ðŸ”§ [DEDUP] ÐŸÐ¾ÑÐ»Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…: {len(normalized_matches)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹")
                else:
                    print(f"ðŸ”§ [METADATA] â„¹ï¸ Ð§ÑƒÐ²ÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾")
                    
            except Exception as e:
                print(f"ðŸ”§ [METADATA] âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…: {str(e)}")
                import traceback
                traceback.print_exc()
            
            # Ð­Ð¢ÐÐŸ 7: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð¾Ð²
            results = {
                'status': 'success',
                'message': 'Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½',
                'statistics': replacement_stats,
                'total_blocks': len(blocks),
                'matches_count': len(all_matches),
                'anonymized_document_path': output_path
            }
            # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°
            if excel_report_path:
                excel_generated = self._generate_excel_report(processed_blocks, normalized_matches, excel_report_path)
                results['excel_report_path'] = excel_report_path
                results['excel_report_generated'] = excel_generated
            # Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ JSON Ð¶ÑƒÑ€Ð½Ð°Ð»Ð°
            if json_ledger_path:
                ledger_data = self._generate_json_ledger(normalized_matches, replacement_stats)
                with open(json_ledger_path, 'w', encoding='utf-8') as f:
                    json.dump(ledger_data, f, ensure_ascii=False, indent=2)
                results['json_ledger_path'] = json_ledger_path
                results['json_ledger_generated'] = True
            return results
            
        except Exception as e:
            return {
                'status': 'error',
                'error_message': f'ÐžÑˆÐ¸Ð±ÐºÐ° Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {str(e)}',
                'error_type': type(e).__name__
            }

    def anonymize_selected_items(self, 
                                input_path: str, 
                                output_path: str,
                                selected_items: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ÐÐ½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
        
        Args:
            input_path: ÐŸÑƒÑ‚ÑŒ Ðº Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð¼Ñƒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñƒ
            output_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            selected_items: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… Ð´Ð»Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
            
        Returns:
            Dict Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ð¼Ð¸ ÑÐµÐ»ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        """
        try:
            print(f"ðŸ”§ [FULL_ANONYMIZER] ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð·Ð°Ð¼ÐµÐ½Ñ‹: {len(selected_items)}")
            for i, item in enumerate(selected_items[:5]):  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 5
                print(f"ðŸ”§ [FULL_ANONYMIZER] Ð­Ð»ÐµÐ¼ÐµÐ½Ñ‚ {i+1}: '{item.get('original_value', 'N/A')}' Ð² Ð±Ð»Ð¾ÐºÐµ {item.get('block_id', 'N/A')}")
            if len(selected_items) > 5:
                print(f"ðŸ”§ [FULL_ANONYMIZER] ... Ð¸ ÐµÑ‰Ðµ {len(selected_items) - 5} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
            
            # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚
            doc = Document(input_path)
            
            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð±Ð»Ð¾ÐºÐ¸ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            blocks = self.block_builder.build_blocks(doc)
            
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ°Ñ€Ñ‚Ñƒ Ð±Ð»Ð¾ÐºÐ¾Ð² Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°
            blocks_map = {block['block_id']: block for block in blocks}
            print(f"ðŸ—‚ï¸  [FULL_ANONYMIZER] Ð¡Ð¾Ð·Ð´Ð°Ð½Ð° ÐºÐ°Ñ€Ñ‚Ð° Ð±Ð»Ð¾ÐºÐ¾Ð²: {list(blocks_map.keys())}")
            
            # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð·Ð°Ð¼ÐµÐ½Ñ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ñ‹Ñ… ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²
            replacements_for_formatting = []
            skipped_items = []
            seen_replacements = set()  # Ð”Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
            
            for item in selected_items:
                block_id = item.get('block_id')
                original_value = item.get('original_value', '')
                position = item.get('position', {})
                uuid_val = item.get('uuid', '')

                # Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° Ð½ÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ñ… uuid
                if not uuid_val or str(uuid_val).strip().lower() == 'placeholder':
                    print(f"ðŸš¨ [BUG] ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ uuid Ð´Ð»Ñ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ '{original_value}' (block_id={block_id}): '{uuid_val}'")

                # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»ÑŽÑ‡ Ð´Ð»Ñ Ð´ÐµÐ´ÑƒÐ¿Ð»Ð¸ÐºÐ°Ñ†Ð¸Ð¸
                dedup_key = (block_id, original_value, position.get('start'), position.get('end'))

                if dedup_key in seen_replacements:
                    print(f"ðŸ”„ [FULL_ANONYMIZER] ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚: '{original_value}' Ð² {block_id}")
                    continue

                seen_replacements.add(dedup_key)

                if block_id in blocks_map:
                    block = blocks_map[block_id]
                    replacement = {
                        'block_id': block_id,
                        'original_value': original_value,
                        'uuid': uuid_val,
                        'position': position,
                        'element': block.get('element'),
                        'category': item['category']
                    }
                    replacements_for_formatting.append(replacement)
                else:
                    skipped_items.append(item)
                    print(f"âš ï¸  [FULL_ANONYMIZER] ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚ - Ð±Ð»Ð¾Ðº '{block_id}' Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½: '{original_value}'")
            
            print(f"ðŸ”§ [FULL_ANONYMIZER] ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð»ÐµÐ½Ð¾ Ð·Ð°Ð¼ÐµÐ½ Ð´Ð»Ñ FormatterApplier: {len(replacements_for_formatting)}")
            print(f"âš ï¸  [FULL_ANONYMIZER] ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(skipped_items)}")
            if skipped_items:
                print(f"âš ï¸  [FULL_ANONYMIZER] Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ block_id: {list(blocks_map.keys())}")

            # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ð·Ð°Ð¼ÐµÐ½Ñ‹
            replacement_stats = self.formatter.apply_replacements_to_document(doc, replacements_for_formatting)
            doc.save(output_path)

            # --- Ð¡ÐšÐ’ÐžÐ—ÐÐÐ¯ ÐÐÐžÐÐ˜ÐœÐ˜Ð—ÐÐ¦Ð˜Ð¯ Ð”Ð›Ð¯ HEADER ---
            # Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð±Ð»Ð¾ÐºÐ° Ñ‚Ð¸Ð¿Ð° header Ð´ÐµÐ»Ð°ÐµÐ¼ Ð·Ð°Ð¼ÐµÐ½Ñƒ Ð¸ Ð² Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…
            header_items = [item for item in selected_items if 'header' in (item.get('block_id') or '').lower()]
            if header_items:
                print(f"ðŸ”§ [FULL_ANONYMIZER] ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ header-ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð´Ð»Ñ ÑÐºÐ²Ð¾Ð·Ð½Ð¾Ð¹ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {len(header_items)}")
                # Ð“Ð¾Ñ‚Ð¾Ð²Ð¸Ð¼ ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð»Ñ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ…: Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ header original_value Ð¸ uuid (ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ - Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼)
                from uuid_mapper import UUIDMapper
                uuid_mapper = self.formatter.uuid_mapper if hasattr(self.formatter, 'uuid_mapper') else UUIDMapper()
                metadata_items = []
                for h in header_items:
                    uuid_val = h.get('uuid')
                    if not uuid_val or str(uuid_val).strip().lower() == 'placeholder':
                        uuid_val = uuid_mapper.get_uuid_for_text(h['original_value'], h['category'])
                    for section in ['core', 'app', 'custom']:
                        metadata_items.append({
                            'original_value': h['original_value'],
                            'uuid': uuid_val,
                            'category': h['category'],
                            'metadata_section': section,
                        })
                from docx_metadata_handler import DocxMetadataHandler
                metadata_handler = DocxMetadataHandler(output_path)
                metadata_handler.anonymize_metadata_in_docx(output_path, output_path, metadata_items)

            return {
                'status': 'success',
                'message': f'Ð¡ÐµÐ»ÐµÐºÑ‚Ð¸Ð²Ð½Ð°Ñ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°. ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(selected_items)} ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð¾Ð².',
                'statistics': replacement_stats,
                'selected_items_count': len(selected_items),
                'replacements_applied': replacement_stats.get('total_replacements', 0),
                'anonymized_document_path': output_path
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'error_message': f'ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐµÐ»ÐµÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ Ð°Ð½Ð¾Ð½Ð¸Ð¼Ð¸Ð·Ð°Ñ†Ð¸Ð¸: {str(e)}',
                'error_type': type(e).__name__
            }

    def _generate_excel_report(self, processed_blocks: List[Dict], matches: List[Dict], excel_path: str) -> bool:
        """
        Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° Ñ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ UUID
        
        Args:
            processed_blocks: ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ðµ Ð±Ð»Ð¾ÐºÐ¸ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°
            matches: Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… ÑÐ¾Ð²Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¹
            excel_path: ÐŸÑƒÑ‚ÑŒ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Excel Ñ„Ð°Ð¹Ð»Ð°
            
        Returns:
            True ÐµÑÐ»Ð¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾, False Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ
        """
        try:
            report_data = []
            
            print(f"ðŸ“ [EXCEL_REPORT] Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ñ‡ÐµÑ‚Ð° Ð´Ð»Ñ {len(matches)} Ð·Ð°Ð¼ÐµÐ½")
            print(f"ðŸ“ [EXCEL_REPORT] ÐŸÐµÑ€Ð²Ñ‹Ðµ 3 Ð·Ð°Ð¼ÐµÐ½Ñ‹:")
            for i, match in enumerate(matches[:3], 1):
                print(f"  {i}. original_value: '{match.get('original_value', 'N/A')[:50]}'")
                print(f"     uuid: '{match.get('uuid', 'N/A')}'")
                print(f"     category: '{match.get('category', 'N/A')}'")
            
            for i, match in enumerate(matches, 1):
                original_value = match.get('original_value', '')
                category = match.get('category', 'unknown')
                
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ UUID ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ¶Ðµ Ð±Ñ‹Ð» ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð² formatter_applier
                # Ð•ÑÐ»Ð¸ UUID Ð½ÐµÑ‚ Ð² match, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ (Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚)
                uuid_for_replacement = match.get('uuid')
                if not uuid_for_replacement:
                    print(f"âš ï¸ [EXCEL_REPORT] UUID Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð´Ð»Ñ '{original_value[:50]}', Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹")
                    uuid_for_replacement = self.formatter.uuid_mapper.get_uuid_for_text(original_value, category)
                
                # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ñ€Ð°ÑÐ¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¼ÐµÐ½Ñ‹
                # Ð•ÑÐ»Ð¸ source Ð½Ð°Ñ‡Ð¸Ð½Ð°ÐµÑ‚ÑÑ Ñ metadata_ â€” ÑÑ‚Ð¾ Ð¼ÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ
                source = match.get('source', '')
                location = 'ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ' if source.startswith('metadata_') else 'Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚'
                
                report_data.append({
                    'â„–': i,
                    'Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ': original_value,
                    'Ð—Ð°Ð¼ÐµÐ½Ð° (Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€)': uuid_for_replacement,
                    'Ð Ð°ÑÐ¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ': location
                })
            
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ DataFrame Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ ÐºÐ¾Ð»Ð¾Ð½ÐºÐ°Ð¼Ð¸
            df = pd.DataFrame(report_data)
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² Excel
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='Ð—Ð°Ð¼ÐµÐ½Ñ‹', index=False)
                
                # ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº
                worksheet = writer.sheets['Ð—Ð°Ð¼ÐµÐ½Ñ‹']
                worksheet.column_dimensions['A'].width = 5
                worksheet.column_dimensions['B'].width = 40
                worksheet.column_dimensions['C'].width = 45
                worksheet.column_dimensions['D'].width = 15  # ÐšÐ¾Ð»Ð¾Ð½ÐºÐ° "Ð Ð°ÑÐ¿Ð¾Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ"
            
            print(f"âœ… Excel Ð¾Ñ‚Ñ‡ÐµÑ‚ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½: {excel_path} ({len(report_data)} Ð·Ð°Ð¿Ð¸ÑÐµÐ¹)")
            return True
            
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Excel Ð¾Ñ‚Ñ‡ÐµÑ‚Ð°: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def _generate_json_ledger(self, matches: List[Dict], stats: Dict) -> Dict:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ JSON Ð¶ÑƒÑ€Ð½Ð°Ð»Ð° Ð·Ð°Ð¼ÐµÐ½ Ñ Ð´ÐµÑ‚ÐµÑ€Ð¼Ð¸Ð½Ð¸ÑÑ‚Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ UUID"""
        replacements_list = []
        
        for match in matches:
            original_value = match.get('original_value', '')
            category = match.get('category', 'unknown')
            
            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ UUID ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹ ÑƒÐ¶Ðµ Ð±Ñ‹Ð» ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð² formatter_applier
            # Ð•ÑÐ»Ð¸ UUID Ð½ÐµÑ‚ Ð² match, Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ð¹ (Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ð¹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚)
            uuid_for_replacement = match.get('uuid')
            if not uuid_for_replacement:
                uuid_for_replacement = self.formatter.uuid_mapper.get_uuid_for_text(original_value, category)
            
            replacements_list.append({
                'uuid': uuid_for_replacement,
                'category': category,
                'original_value': original_value,
                'block_id': match.get('block_id', ''),
                'position': match.get('position', {}),
                'confidence': match.get('confidence', 1.0)
            })
        
        return {
            'timestamp': pd.Timestamp.now().isoformat(),
            'total_matches': len(matches),
            'replacement_statistics': stats,
            'replacements': replacements_list
        }
    
    def _positions_overlap(self, pos1: Dict, pos2: Dict, threshold: float = 0.5) -> bool:
        """
        ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚, Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‚ÑÑ Ð»Ð¸ Ð´Ð²Ðµ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸
        
        Args:
            pos1, pos2: ÐŸÐ¾Ð·Ð¸Ñ†Ð¸Ð¸ Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ 'start' Ð¸ 'end'
            threshold: ÐŸÐ¾Ñ€Ð¾Ð³ Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ñ‚Ð¸Ñ Ð´Ð»Ñ ÑÑ‡Ð¸Ñ‚Ð°Ð½Ð¸Ñ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð¼
            
        Returns:
            True ÐµÑÐ»Ð¸ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¸ Ð¿ÐµÑ€ÐµÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‚ÑÑ
        """
        start1 = pos1.get('start', 0)
        end1 = pos1.get('end', 0)
        start2 = pos2.get('start', 0)  
        end2 = pos2.get('end', 0)
        
        overlap_start = max(start1, start2)
        overlap_end = min(end1, end2)
        
        if overlap_start >= overlap_end:
            return False
        
        overlap_length = overlap_end - overlap_start
        min_length = min(end1 - start1, end2 - start2)
        
        return (overlap_length / min_length) >= threshold if min_length > 0 else False
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–û—Ç–ª–∞–¥–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º
"""

import sys
import os
import requests
from docx import Document

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from block_builder import BlockBuilder

def analyze_is_detection():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º"""
    
    input_path = r'C:\Projects\Anonymizer\unified_document_service\test_docs\test_01_1_4_SD2.docx'
    
    print("=" * 120)
    print("–ê–ù–ê–õ–ò–ó –î–ï–¢–ï–ö–¶–ò–ò –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–• –°–ò–°–¢–ï–ú")
    print("=" * 120)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
    doc = Document(input_path)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ BlockBuilder
    bb = BlockBuilder()
    blocks = bb.build_blocks(doc)
    
    # –°–æ–±–∏—Ä–∞–µ–º full_text –∫–∞–∫ –≤ FullAnonymizer
    full_text = ""
    block_positions = []
    
    for block in blocks:
        block_start = len(full_text)
        block_text = block.get('text', '')
        full_text += block_text + "\n"
        block_end = len(full_text) - 1  # –ù–µ –≤–∫–ª—é—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π \n
        
        block_positions.append({
            'id': block.get('id'),
            'type': block.get('type'),
            'start': block_start,
            'end': block_end,
            'text': block_text[:100]
        })
    
    print(f"\nüìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(blocks)} –±–ª–æ–∫–æ–≤, –≤—Å–µ–≥–æ {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
    is_keywords = [
        "–ï–î–ò–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê",
        "–ï–¥–∏–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã",
        "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è",
        "–ø–æ–¥—Å–∏—Å—Ç–µ–º—ã ¬´–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–º—É—â–µ—Å—Ç–≤–æ–º¬ª",
        "–§–ò–ù–ê–ù–°–û–í–û-–•–û–ó–Ø–ô–°–¢–í–ï–ù–ù–û–ô –î–ï–Ø–¢–ï–õ–¨–ù–û–°–¢–¨–Æ",
        "–ï–ò–°",
    ]
    
    print(f"\nüîç –ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤ —Ç–µ–∫—Å—Ç–µ:")
    for keyword in is_keywords:
        if keyword in full_text or keyword.upper() in full_text.upper():
            # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏–∏
            text_to_search = full_text if keyword == keyword.upper() else full_text
            pos = text_to_search.find(keyword)
            if pos == -1:
                # –ü–æ–ø—Ä–æ–±—É–µ–º case-insensitive
                pos = full_text.upper().find(keyword.upper())
            
            if pos != -1:
                context_start = max(0, pos - 50)
                context_end = min(len(full_text), pos + len(keyword) + 50)
                context = full_text[context_start:context_end]
                print(f"\n  ‚úÖ –ù–∞–π–¥–µ–Ω–æ '{keyword}' –Ω–∞ –ø–æ–∑–∏—Ü–∏–∏ {pos}")
                print(f"     –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{context}...")
    
    # –í—ã–∑—ã–≤–∞–µ–º NLP Service
    print(f"\n\nüìû –í—ã–∑—ã–≤–∞–µ–º NLP Service –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞...")
    nlp_url = "http://localhost:8006/analyze"
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –±–ª–æ–∫–∏ –∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç NLP Service
    request_blocks = []
    for i, block in enumerate(blocks[:10]):  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 –±–ª–æ–∫–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
        block_id = block.get('id') or f"block_{i}"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –µ—Å–ª–∏ id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
        request_blocks.append({
            "block_id": block_id,
            "content": block.get('text', '')
        })
    
    try:
        response = requests.post(
            nlp_url,
            json={"blocks": request_blocks},
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            detections = result.get('detections', [])
            
            print(f"‚úÖ NLP Service –≤–µ—Ä–Ω—É–ª {len(detections)} –¥–µ—Ç–µ–∫—Ü–∏–π\n")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã
            is_detections = [d for d in detections if d.get('entity_type') == 'information_system']
            
            print(f"üñ•Ô∏è  –î–µ—Ç–µ–∫—Ü–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: {len(is_detections)}")
            
            if is_detections:
                print(f"\n–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã:")
                for i, det in enumerate(is_detections, 1):
                    text = det.get('text', '')
                    start = det.get('start', 0)
                    end = det.get('end', 0)
                    confidence = det.get('confidence', 0)
                    strategy = det.get('detection_strategy', 'unknown')
                    
                    print(f"\n  {i}. –¢–µ–∫—Å—Ç: '{text}'")
                    print(f"     –ü–æ–∑–∏—Ü–∏—è: {start}-{end}")
                    print(f"     –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}")
                    print(f"     –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}")
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    context_start = max(0, start - 30)
                    context_end = min(len(full_text), end + 30)
                    context = full_text[context_start:context_end]
                    print(f"     –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{context}...")
            else:
                print("\n‚ùå NLP Service –ù–ï –ù–ê–®–ï–õ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º!")
                print("\nüìã –í—Å–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏):")
                for i, det in enumerate(detections[:10], 1):
                    print(f"  {i}. {det.get('entity_type')}: '{det.get('text', '')[:80]}'")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é NLP Service
                print(f"\n\nüîß –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é NLP Service...")
                config_response = requests.get("http://localhost:8006/config")
                if config_response.status_code == 200:
                    config = config_response.json()
                    print(f"\nüìù –¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è NLP Service:")
                    print(f"   - –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–∫–ª—é—á–µ–Ω—ã: {config.get('enabled_strategies', [])}")
                    print(f"   - information_system –≤–∫–ª—é—á–µ–Ω–∞: {'information_system' in config.get('enabled_strategies', [])}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ NLP Service: {response.status_code}")
            print(f"   –û—Ç–≤–µ—Ç: {response.text[:200]}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ NLP Service: {e}")
        import traceback
        traceback.print_exc()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º
    print(f"\n\nüîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –≤ NLP Service...")
    try:
        # –ß–∏—Ç–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ NLP
        nlp_config_path = r'C:\Projects\Anonymizer\nlp_service\config\nlp_config.json'
        if os.path.exists(nlp_config_path):
            import json
            with open(nlp_config_path, 'r', encoding='utf-8') as f:
                nlp_config_data = json.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ nlp_service_config
            nlp_service_config = nlp_config_data.get('nlp_service_config', {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º detection_methods –¥–ª—è information_system
            detection_methods = nlp_service_config.get('detection_methods', {})
            is_method_config = detection_methods.get('information_system', {})
            
            print(f"\nüìù –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–µ—Ç–æ–¥–∞ information_system:")
            print(f"   - –í–∫–ª—é—á–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã: {is_method_config.get('enabled_methods', [])}")
            print(f"   - –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {is_method_config.get('strategy', 'unknown')}")
            print(f"   - Max results: {is_method_config.get('max_results', 0)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            detection_strategies = nlp_service_config.get('detection_strategies', {})
            is_strategy = detection_strategies.get('information_system', {})
            print(f"\nüìù –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ information_system:")
            print(f"   - Core keywords: {is_strategy.get('core_keywords', [])[:5]}")
            print(f"   - Known abbreviations: {is_strategy.get('known_abbreviations', [])[:10]}")
            print(f"   - Min confidence: {is_strategy.get('confidence_modifiers', {}).get('min_confidence', 0)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            patterns_path = r'C:\Projects\Anonymizer\nlp_service\patterns\nlp_patterns.json'
            if os.path.exists(patterns_path):
                with open(patterns_path, 'r', encoding='utf-8') as f:
                    patterns_data = json.load(f)
                
                all_patterns = patterns_data.get('patterns', [])
                is_patterns = [p for p in all_patterns if p.get('category') == 'information_system']
                print(f"\n   üìã –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º: {len(is_patterns)}")
                if is_patterns:
                    print(f"   –ü—Ä–∏–º–µ—Ä—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤:")
                    for p in is_patterns[:5]:
                        print(f"      - {p.get('pattern', '')[:100]}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")

if __name__ == "__main__":
    analyze_is_detection()

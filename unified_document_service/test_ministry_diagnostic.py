"""
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: –ø–æ—á–µ–º—É "–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø –ò –°–í–Ø–ó–ò" –Ω–µ –∑–∞–º–µ–Ω—è–µ—Ç—Å—è
"""
import os
import sys
import requests
import base64
from docx import Document
import json

sys.stdout.reconfigure(encoding='utf-8')
os.environ['PYTHONIOENCODING'] = 'utf-8'

doc_path = r'C:\Projects\Anonymizer\unified_document_service\test_docs\test_01_1_4_SD33.docx'

print("=" * 80)
print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø –ò –°–í–Ø–ó–ò")
print("=" * 80)
print()

# –®–∞–≥ 1: –ß–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
doc = Document(doc_path)

# –ò—â–µ–º —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
target_text = "–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ì–û –†–ê–ó–í–ò–¢–ò–Ø –ò –°–í–Ø–ó–ò"
found_in_doc = False

print("üìÑ –®–∞–≥ 1: –ü–æ–∏—Å–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ...")
print()

for i, para in enumerate(doc.paragraphs[:20]):
    if target_text in para.text.upper():
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ #{i}: '{para.text}'")
        found_in_doc = True
        break

if not found_in_doc:
    print(f"‚ùå '{target_text}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –ø–µ—Ä–≤—ã—Ö 20 –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞—Ö")
    print("\nüîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –¥—Ä—É–≥–∏–µ —á–∞—Å—Ç–∏...")
    for section in doc.sections:
        header = section.header
        for para in header.paragraphs:
            if target_text in para.text.upper():
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ –ó–ê–ì–û–õ–û–í–ö–ï: '{para.text}'")
                found_in_doc = True
                break

print()

# –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ NLP Service
print("ü§ñ –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ NLP Service...")
print()

# –ß–∏—Ç–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
full_text = '\n'.join([p.text for p in doc.paragraphs])

# –í—ã–∑—ã–≤–∞–µ–º NLP Service –Ω–∞–ø—Ä—è–º—É—é
nlp_url = "http://localhost:8006/analyze"

blocks = [{"block_id": "test_block", "content": full_text[:5000]}]  # –ü–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤

response = requests.post(nlp_url, json={"blocks": blocks})

if response.status_code == 200:
    result = response.json()
    detections = result.get('detections', [])
    
    # –ò—â–µ–º –Ω–∞—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    ministry_detections = [d for d in detections if '–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û' in d.get('text', '').upper() or '–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û' in d.get('original_value', '').upper()]
    
    print(f"NLP Service –Ω–∞—à–µ–ª {len(detections)} –¥–µ—Ç–µ–∫—Ü–∏–π")
    print(f"–ò–∑ –Ω–∏—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û–ú: {len(ministry_detections)}")
    print()
    
    if ministry_detections:
        print("üéØ –î–µ—Ç–µ–∫—Ü–∏–∏ –ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û:")
        for d in ministry_detections:
            print(f"   - –¢–µ–∫—Å—Ç: '{d.get('text', d.get('original_value', 'N/A'))}'")
            print(f"     –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {d.get('category', 'N/A')}")
            print(f"     –ú–µ—Ç–æ–¥: {d.get('method', 'N/A')}")
            print(f"     Confidence: {d.get('confidence', 'N/A')}")
            print()
else:
    print(f"‚ùå –û—à–∏–±–∫–∞ NLP Service: {response.status_code}")

print()

# –®–∞–≥ 3: –ü–æ–ª–Ω–∞—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è
print("üîÑ –®–∞–≥ 3: –ü–æ–ª–Ω–∞—è –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞...")
print()

with open(doc_path, 'rb') as f:
    files = {'file': ('test.docx', f, 'application/vnd.openxmlformats-officedocument.wordprocessingml.document')}
    data = {
        'patterns_file': 'patterns/sensitive_patterns.xlsx',
        'generate_excel_report': 'true'
    }
    
    response = requests.post('http://localhost:8009/anonymize_full', files=files, data=data, timeout=120)

if response.status_code == 200:
    result = response.json()
    
    print(f"‚úÖ –ê–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
    print(f"   –í—Å–µ–≥–æ –∑–∞–º–µ–Ω: {result.get('statistics', {}).get('total_replacements', 0)}")
    print()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    doc_data = base64.b64decode(result['files_base64']['anonymized_document_base64'])
    anon_path = 'test_anonymized_diagnostic.docx'
    with open(anon_path, 'wb') as f:
        f.write(doc_data)
    
    # –ß–∏—Ç–∞–µ–º –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
    anon_doc = Document(anon_path)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ç–µ–∫—Å—Ç
    print("üîç –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–º–µ–Ω—ã –≤ –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ...")
    print()
    
    found_in_anon = False
    for i, para in enumerate(anon_doc.paragraphs[:20]):
        if target_text in para.text.upper():
            print(f"‚ùå –ü–†–û–ë–õ–ï–ú–ê: –¢–µ–∫—Å—Ç –ù–ï –ó–ê–ú–ï–ù–ï–ù –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ #{i}")
            print(f"   –û—Ä–∏–≥–∏–Ω–∞–ª: '{para.text}'")
            found_in_anon = True
            break
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
    for section in anon_doc.sections:
        header = section.header
        for para in header.paragraphs:
            if target_text in para.text.upper():
                print(f"‚ùå –ü–†–û–ë–õ–ï–ú–ê: –¢–µ–∫—Å—Ç –ù–ï –ó–ê–ú–ï–ù–ï–ù –≤ –ó–ê–ì–û–õ–û–í–ö–ï")
                print(f"   –¢–µ–∫—Å—Ç: '{para.text}'")
                found_in_anon = True
                break
    
    if not found_in_anon:
        print("‚úÖ –¢–µ–∫—Å—Ç –±—ã–ª –∑–∞–º–µ–Ω–µ–Ω (–Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∞–Ω–æ–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ)")
    
    print()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∞–±–ª–∏—Ü—É –∑–∞–º–µ–Ω
    excel_data = base64.b64decode(result['files_base64']['excel_report_base64'])
    excel_path = 'test_replacements_diagnostic.xlsx'
    with open(excel_path, 'wb') as f:
        f.write(excel_data)
    
    import pandas as pd
    df = pd.read_excel(excel_path)
    
    print("üìä –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∞–±–ª–∏—Ü—ã –∑–∞–º–µ–Ω...")
    print()
    
    ministry_in_table = df[df['–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ'].str.contains('–ú–ò–ù–ò–°–¢–ï–†–°–¢–í–û', case=False, na=False)]
    
    if len(ministry_in_table) > 0:
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü–µ –∑–∞–º–µ–Ω: {len(ministry_in_table)} –∑–∞–ø–∏—Å–µ–π")
        for idx, row in ministry_in_table.iterrows():
            print(f"   - '{row['–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ']}'")
            print(f"     UUID: {row['–ó–∞–º–µ–Ω–∞ (–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä)']}")
        print()
    else:
        print("‚ùå –ù–ï –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–∞–±–ª–∏—Ü–µ –∑–∞–º–µ–Ω!")
        print()
        print("üîç –í—Å–µ –∑–∞–ø–∏—Å–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ:")
        for idx, row in df.head(10).iterrows():
            print(f"   {idx+1}. '{row['–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ']}'")
        print()

else:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–æ–Ω–∏–º–∏–∑–∞—Ü–∏–∏: {response.status_code}")

print()
print("=" * 80)
print("üéØ –î–ò–ê–ì–ù–û–ó")
print("=" * 80)
